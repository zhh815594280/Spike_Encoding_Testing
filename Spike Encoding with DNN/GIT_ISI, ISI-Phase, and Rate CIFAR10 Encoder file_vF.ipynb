{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1ONMJQoVfrTuUhJPqjLXBTWweQ2uKALKe","timestamp":1770904522177},{"file_id":"12ebtNRiccUhxG-Ujg-cV9dKEumFzkJa0","timestamp":1762568797204}],"authorship_tag":"ABX9TyMYIAtvvj+S4BFq585FmygZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"s8DWiw4fb8vb"},"outputs":[],"source":["import sys\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","rateencoderspikecount = 6 #was 6\n","isiencoderspikecount = 6 #was 6\n","my_epoch=11\n","encoder_type = \"Rate\"\n","\n","\n","if __name__ == \"__main__\":\n","    if len(sys.argv) > 4:\n","        rateencoderspikecount = sys.argv[1]\n","        isiencoderspikecount = sys.argv[2]\n","        my_epoch = sys.argv[3]\n","        encoder_type = sys.argv[4]\n","\n","        print(\"Parameters: rateencoderspikecount, isiencoderspikecount, my_epoch, encoder_type\")\n","        print(\"Encoder types: Rate, TTFS, ISI, TTFS-Phase, ISI-Phase\")\n","        print(rateencoderspikecount, isiencoderspikecount, my_epoch, encoder_type)\n","    else:\n","        print(\"Usage: ipython 'filename.py' <argument1> <argument2>\")\n","\n","rateencoderspikecount = int(rateencoderspikecount)\n","isiencoderspikecount = int(isiencoderspikecount)\n","my_epoch = int(my_epoch)\n","\n","#!/usr/bin/env python\n","# coding: utf-8\n","\n","# This notebook uses CIFAR-10 and CNN to verify the efficiency of encoding schems.\n","\n","# Block of importing libraries.\n","\n","# In[1]:\n","\n","\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns; sns.set()\n","from keras.datasets import cifar10\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.metrics import confusion_matrix\n","from keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout\n","from keras import layers\n","from keras.models import Sequential, load_model\n","from keras.callbacks import EarlyStopping\n","from tensorflow.keras import regularizers\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import time\n","import scipy.io\n","import matplotlib.pyplot as plt\n","import keras\n","from keras.models import Sequential\n","from keras.datasets import mnist\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.optimizers import Adam\n","#because of multiclass datasets\n","from tensorflow.keras.utils import to_categorical\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","import torchvision.transforms as transforms\n","import tensorflow as tf\n","\n","\n","# Normalization function.\n","\n","# In[2]:\n","\n","\n","def feature_normalize(X):\n","\n","    # feature_normalize: Normalizes the features in X\n","    # feature_normalize(X): returns a normalized version of X where the mean value of each feature is 0 and the standard deviation is 1.\n","    # This is often a good preprocessing step to do when working with learning algorithms.\n","\n","    mu     = 0 # mean of dataset\n","    sigma  = 0 #std dev\n","    mu     = np.mean(X)\n","    sigma  = np.std(X)\n","    X      = (X - mu) / sigma #normalization func\n","    X_norm = X\n","\n","    return X_norm\n","\n","\n","# TTFS encoder.\n","\n","# In[3]:\n","\n","\n","#Input is any any-d array (ideally 1d) that contains input currents\n","#Returns outputs a 1d array with each output being a different encoding window's time-to-spike.\n","def TTFS_Encoder(input, tau = 20, thr = 0.2, tmax = 290.2, epsilon = 1e-7):\n","  idx = input < thr #taking all input values below threshold...\n","  input = np.clip(input, thr + epsilon, 1e9) #then \"squeezing\" the inputs to a range between threshold and 1*10^9. Below and upper values become the min or max of the new value range\n","  output = tau * np.log(input / (input - thr)) # This equation shows that the output (time) for the spike to fire is caused by both the time it takes for the capacitor to fully charge (tau) and the distance/how long it takes to get to the threshold voltage given an input current (log(expression))\n","  output[idx] = tmax # set all inputs that are below threshold to a fixed time value: 290.2 to basically separate the inputs above thresholds from those below it\n","  return output\n","\n","\n","# Test the encoder with simple input.\n","\n","# In[4]:\n","\n","\n","TestResult = TTFS_Encoder(np.array([[1, 2, 4, 8]]))\n","TestResult\n","\n","\n","# Visualize the function of TTFS encoder.\n","\n","# In[5]:\n","\n","\n","#Plotting the TTFS encoder\n","input_x = np.linspace(1.0, 3.0, num=20) #array generation function: start, end, num. of in-betweens\n","plt.plot(input_x, TTFS_Encoder(np.array([input_x]))[0], 'ro')\n","plt.xlabel('Input', fontsize = 22)\n","plt.ylabel('Output', fontsize = 22)\n","plt.rc('xtick',labelsize=18) #rc is default setting config of graph properties\n","plt.rc('ytick',labelsize=18) #rc is default setting\n","#plt.show()\n","\n","\n","# It works well. Move to the ISI encoder. This encoder is using the parallel structure which is easier to be implemented is circuit level and is what I have implemented in circuit.\n","\n","# In[6]:\n","\n","\n","#Sets output to length param. N; for each N value in the output array, switch the value to the output of tau*log(expr) of TTFS func with diff. threshold values for each.\n","\n","#input is 2d array with 3 columns; each row is a separate encoding window with three different input currents\n","#output is a 2d array with 3 columns modified by threshold; each row is three different time-to-spikes relative to the time of when the encoding begun.\n","def ISI_Encoder(input, N = isiencoderspikecount):\n","  output = np.zeros(N)\n","  for j in range(N):\n","    output[j] = TTFS_Encoder(np.array([input]), thr = 0.2 + 0.05 * j) #Each row of the 2d array is input of TTFS func. -> 1d array as input for func.\n","  return output #outputs N (3 for this example) time-to-spikes for each encoding window. Output is probably a 2d array.\n","\n","\n","# Test the ISI encoder with simple input.\n","\n","# In[7]:\n","\n","\n","ISITestResult = ISI_Encoder(3)\n","ISITestResult\n","\n","\n","# Visualize the ISI encoder.\n","\n","# In[8]:\n","\n","\n","input_x = np.linspace(1.0, isiencoderspikecount, num=10)\n","ISI_spike_plot = np.zeros((10, isiencoderspikecount))\n","for i in range(10):\n","  ISI_spike_plot[i] = ISI_Encoder(input_x[i])\n","#print(ISI_spike_plot)\n","#Plot the ISI using a spike-like graph\n","plt.eventplot(ISI_spike_plot, colors=['C{}'.format(i) for i in range(ISI_spike_plot.shape[0])])\n","plt.xlabel('Input', fontsize = 22)\n","plt.ylabel('Output', fontsize = 22)\n","plt.rc('xtick',labelsize=18)\n","plt.rc('ytick',labelsize=18)\n","#plt.show()\n","\n","\n","# In[9]:\n","\n","\n","#Creates a reference table to save time by pre-creating spike values at corresponding input currents\n","ISI_SpikesList = np.zeros((256, isiencoderspikecount)) #!!!Revert back to 3\n","for i in range(256):\n","  ISI_SpikesList[i] = ISI_Encoder(i/32) #i/32 just is auto-scaler to fit image dataset settings\n","#print(ISI_SpikesList)\n","\n","\n","# It works well. Move to Latency-phase encoder.\n","\n","# In[10]:\n","\n","\n","def TTFS_Phase_Encoder(input, SMO_freq = 5000, TMAX = 295):\n","  #1. Create the maximums/peaks of the sinuodal function based off of the period/interval magnitude and the maximum time.\n","\n","  #print(SMO_freq)\n","  SMO_interval = 1000 / SMO_freq\n","  #print(SMO_interval)\n","  SMO_max = np.arange(0, TMAX - 0.1, SMO_interval)\n","  #print(SMO_max)\n","\n","  #2. Then, process the input in a TTFS encoder\n","\n","  TT = TTFS_Encoder(input)\n","\n","  #3. Align the TTFS output to be at the next local maximums of the sinuoidal function\n","  for i in range(SMO_max.shape[0]):\n","    if SMO_max[i] >= TT:\n","      TT = SMO_max[i]\n","      break\n","  return TT # Return the newly aligned output times to first spikes\n","\n","\n","# Test the TTFS_Phase_Encoder with simple input.\n","\n","# In[11]:\n","\n","\n","TTFS_PhaseTestResult = TTFS_Phase_Encoder(np.array([2]))\n","TTFS_PhaseTestResult\n","\n","\n","# visualize the TTFS-phase encoder.\n","\n","# In[12]:\n","\n","\n","input_x = np.linspace(1.0, 6.0, num=20)\n","output_x = np.zeros(20)\n","#graph the TTFS phase aligned outputs.\n","for i in range(20):\n","  output_x[i] = TTFS_Phase_Encoder(np.array([input_x[i]]))\n","plt.plot(input_x, output_x, 'ro')\n","plt.xlabel('Input', fontsize = 22)\n","plt.ylabel('Output', fontsize = 22)\n","plt.rc('xtick',labelsize=18)\n","plt.rc('ytick',labelsize=18)\n","#plt.show()\n","\n","\n","# In[13]:\n","\n","\n","TTFS_Phase_SpikesList = np.zeros(256)\n","for i in range(256):\n","  TTFS_Phase_SpikesList[i] = TTFS_Phase_Encoder(np.array([i]))\n","\n","\n","# It works well. Move to ISI_Phase Encoder.\n","\n","\n","# In[14]:\n","\n","\n","#Same as TTFS encoder. 1. create SMO intervals based on SMO frequency. Also define the local maximums within the time range.\n","#2. Input a 1d or 2d array, with each row being a separate encoding window, into the ISI encoder. 3. gamma align each output to the next local max\n","def ISI_Phase_Encoder(input, SMO_freq = 5000, TMAX = 295):\n","  SMO_interval = 1000 / SMO_freq\n","  SMO_max = np.arange(0, TMAX - 0.1, SMO_interval)\n","  TT = ISI_Encoder(input) ##TT is just a 2d array (encoding windows (row) by each time-to-spike (column))\n","  for i in range(TT.shape[0]):\n","    for k in range(SMO_max.shape[0]):\n","      if SMO_max[k] >= TT[i]: #which SMO max is just a little bit more than the corresponding time of spike?\n","        TT[i] = SMO_max[k] # gamma align\n","        break\n","  return TT # return a gamma aligned\n","\n","\n","# Test the ISI_Phase Encoder with simple input.\n","\n","# In[15]:\n","\n","\n","ISI_PhaseTestResult = ISI_Phase_Encoder(3)\n","ISI_PhaseTestResult\n","\n","\n","# Visualize the ISI-Phase Encoder.\n","\n","# In[16]:\n","\n","\n","input_x = np.linspace(1.0, isiencoderspikecount, num=10)\n","ISI_Phase_spike_plot = np.zeros((10, isiencoderspikecount))\n","for i in range(10):\n","  ISI_Phase_spike_plot[i] = ISI_Phase_Encoder(input_x[i])\n","#print(ISI_Phase_spike_plot)\n","\n","#Visualizing the ISI-Phase Encoder\n","plt.eventplot(ISI_Phase_spike_plot, colors=['C{}'.format(i) for i in range(ISI_Phase_spike_plot.shape[0])])\n","plt.xlabel('Input', fontsize = 22)\n","plt.ylabel('Output', fontsize = 22)\n","plt.rc('xtick',labelsize=18)\n","plt.rc('ytick',labelsize=18)\n","#plt.show()\n","\n","\n","# In[17]:\n","\n","\n","ISI_Phase_SpikesList = np.zeros((256, isiencoderspikecount)) #!!!! Revert back 6 to 3\n","for i in range(256): #Individually creating time-to-spikes for the lookup table for each possible input current value (i)\n","  ISI_Phase_SpikesList[i] = ISI_Phase_Encoder(i/32) #!!!!removed /32 after i in ISI_Phase_Encoder parantheses\n","\n","\n","# In[18]:\n","\n","\n","\"\"\"get_ipython().run_line_magic('tensorflow_version', '2.x')\n","import tensorflow as tf\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))\"\"\"\n","\n","\n","# Download and process dataset\n","\n","# In[19]:\n","\n","\n","(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","\n","#Start of data augmentation code\n","\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","\n","#print(x_train.shape)\n","labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","fig, axes = plt.subplots(ncols=7, nrows=3, figsize=(17, 8))\n","index = 0\n","\n","#Displaying 21 different labels/nums.? on the screen\n","for i in range(3):\n","    for j in range(7):\n","        axes[i,j].set_title(labels[y_train[index][0]])\n","        axes[i,j].imshow(x_train[index]) #just displays 2d/3d arrays into images (like heatmaps)\n","        axes[i,j].get_xaxis().set_visible(False)\n","        axes[i,j].get_yaxis().set_visible(False)\n","        index += 1\n","#plt.show()\n","\n","#Converting the x training and testing images into grayscale (from BGR)\n","#print(x_train)\n","#print(\"Max {} Min {}\".format(x_train.max(), x_train.min()))\n","x_train = np.array([cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) for image in x_train])\n","x_test = np.array([cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) for image in x_test])\n","#print(\"Max {} Min {}\".format(x_train.max(), x_train.min()))\n","#print(x_train)\n","#print(x_train.shape)\n","\n","fig, axes = plt.subplots(ncols=7, nrows=3, figsize=(17, 8))\n","index = 0\n","#Displaying 21 different images on the screen\n","for i in range(3):\n","    for j in range(7):\n","        axes[i,j].set_title(labels[y_train[index][0]])\n","        axes[i,j].imshow(x_train[index], cmap='gray')\n","        axes[i,j].get_xaxis().set_visible(False) #Basically hides features like tick marks of x-axis and y-axis\n","        axes[i,j].get_yaxis().set_visible(False)\n","        index += 1\n","#plt.show()\n","\n","#One hot encoder basically assigns numerical values to categorical variables\n","one_hot_encoder = OneHotEncoder(sparse_output=False)\n","one_hot_encoder.fit(y_train)\n","y_train = one_hot_encoder.transform(y_train)\n","y_test = one_hot_encoder.transform(y_test)\n","#print(y_train.shape)\n","\n","if encoder_type == \"Rate\":\n","\n","    # Rate Encoding.\n","\n","    # In[ ]:\n","\n","\n","    #RUN ALL CODE ABOVE THIS BLOCK\n","\n","    #Limited number of spikes\n","    #print(x_train)\n","    #print(x_train.max(), x_train.min())\n","\n","    #256/32/8=1 -> Below code just normalizes the x_train element values to between 0 and 1\n","\n","    x_train = np.rint((x_train / 255) * rateencoderspikecount).astype(int)\n","    x_test = np.rint((x_test / 255) * rateencoderspikecount).astype(int)\n","    #print(x_train)\n","    #print(x_train.shape)\n","\n","    #Reshape dataset\n","    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1) #representing grayscale as a dimension of 1 at the end of x_train's shape\n","    x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)\n","    input_shape = (x_train.shape[1], x_train.shape[2], 1)\n","    #print(input_shape)\n","\n","\n","\"\"\"elif encoder_type == \"TTFS\":\n","\n","    # TTFS Encoding\n","\n","    # In[ ]:\n","\n","\n","    #shrink the dataset\n","\n","    #Encode the dataset\n","    x_train = TTFS_Encoder(x_train) #creates 1d array with each element being in a different encoding window\n","    x_test = TTFS_Encoder(x_test)\n","\n","    #Normalize the dataset\n","    x_train = feature_normalize(x_train)\n","    x_test = feature_normalize(x_test)\n","    #print(np.mean(x_train), np.std(x_train))\n","    #print(np.mean(x_test), np.std(x_test))\n","\n","    #Reshape dataset\n","    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n","    x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)\n","    input_shape = (x_train.shape[1], x_train.shape[2], 1)\n","    #print(input_shape)\n","\"\"\"\n","elif encoder_type == \"ISI\":\n","\n","    # ISI Encoding.\n","\n","    # In[20]:\n","\n","\n","    #Encoder the dataset\n","    x_train_temp = np.zeros((50000, 32, 32, isiencoderspikecount))\n","\n","    #print([nonint for nonint in x_train if not isinstance(nonint, int)])\n","    x_train = np.trunc(x_train).astype(int)\n","    x_test = np.trunc(x_test).astype(int)\n","    #print([nonint for nonint in x_train if not isinstance(nonint, int)])\n","    #print(x_train.shape)\n","    x_train_temp = ISI_SpikesList[x_train]\n","\n","    #print(x_train_temp.shape)\n","\n","    x_test_temp = np.zeros((10000, 32, 32, isiencoderspikecount))\n","\n","    x_test_temp = ISI_SpikesList[x_test]\n","\n","    #print(x_test_temp.shape)\n","    x_train = x_train_temp.reshape(50000, 32, 32, isiencoderspikecount)\n","    x_test = x_test_temp.reshape(10000, 32, 32, isiencoderspikecount)\n","\n","    #Normalize the dataset\n","    x_train = feature_normalize(x_train)\n","    x_test = feature_normalize(x_test)\n","    #print(np.mean(x_train), np.std(x_train))\n","    #print(np.mean(x_test), np.std(x_test))\n","\n","    input_shape = (x_train.shape[1], x_train.shape[2], isiencoderspikecount)\n","    #print(input_shape)\n","\n","\"\"\"elif encoder_type == \"TTFS-Phase\":\n","\n","    # TTFS-Phase Encoding\n","\n","    # In[ ]:\n","\n","\n","    #Encoder the dataset\n","    x_train_temp = np.zeros((50000, 32, 32))\n","    x_train = np.trunc(x_train).astype(int)\n","    x_test = np.trunc(x_test).astype(int)\n","\n","    #print(x_train.shape)\n","    #print(x_train)\n","    #print(x_train[0][1][5])\n","\n","    for i in range(50000):\n","      for j in range(32):\n","        for k in range(32):\n","          x_train_temp[i][j][k] = TTFS_Phase_SpikesList[x_train[i][j][k]]\n","    #print(x_train_temp.shape)\n","\n","    x_test_temp = np.zeros((10000, 32, 32))\n","    for i in range(10000):\n","      for j in range(32):\n","        for k in range(32):\n","          x_test_temp[i][j][k] = TTFS_Phase_SpikesList[x_test[i][j][k]]\n","    #print(x_test_temp.shape)\n","    x_train = x_train_temp.reshape(50000, 32, 32, 1)\n","    x_test = x_test_temp.reshape(10000, 32, 32, 1)\n","\n","    #Normalize the dataset\n","    x_train = feature_normalize(x_train)\n","    x_test = feature_normalize(x_test)\n","    #print(np.mean(x_train), np.std(x_train))\n","    #print(np.mean(x_test), np.std(x_test))\n","\n","    input_shape = (x_train.shape[1], x_train.shape[2], 1)\n","    #print(input_shape)\n","\"\"\"\n","elif encoder_type == \"ISI-Phase\":\n","\n","    # ISI-Phase Encoding\n","\n","    # In[ ]:\n","\n","\n","    #Encoder the dataset\n","    x_train_temp = np.zeros((50000, 32, 32, isiencoderspikecount))\n","\n","    x_train = np.trunc(x_train).astype(int)\n","    x_test = np.trunc(x_test).astype(int)\n","\n","    x_train_temp = ISI_Phase_SpikesList[x_train]\n","    #print(x_train_temp.shape)\n","\n","    x_test_temp = np.zeros((10000, 32, 32, isiencoderspikecount))\n","\n","    x_test_temp = ISI_Phase_SpikesList[x_test]\n","    #print(x_test_temp.shape)\n","    x_train = x_train_temp.reshape(50000, 32, 32, isiencoderspikecount)\n","    x_test = x_test_temp.reshape(10000, 32, 32, isiencoderspikecount)\n","\n","    #Normalize the dataset\n","    x_train = feature_normalize(x_train)\n","    x_test = feature_normalize(x_test)\n","    #print(np.mean(x_train), np.std(x_train))\n","    #print(np.mean(x_test), np.std(x_test))\n","\n","    input_shape = (x_train.shape[1], x_train.shape[2], isiencoderspikecount)\n","    #print(input_shape)\n","\n","\n","# #Setting model\n","\n","\n","#Run all blocks including this one and the ones below\n","\n","model = Sequential([\n","    layers.RandomFlip(\"horizontal\"),\n","    # Random zoom (zoom in / out)\n","])\n","\"\"\"/12/2025: the following is for augmentation (optional)\n","make_augmentation_pipeline()\"\"\"\n","\n","model.add(Conv2D(16, (3, 3), activation='relu', strides=(1, 1),\n","    padding='same', input_shape=input_shape)) #16 learned kernels, 3 by 3 in size\n","model.add(Conv2D(32, (3, 3), activation='relu', strides=(1, 1),\n","    padding='same')) #32 newly learned kernels, 3 by 3 in size, applied to the 16 feature maps from the last layer; as an output, there are 32 new feature maps (learned combinations of previous 16 feature map chararacteristics)\n","model.add(Conv2D(64, (3, 3), activation='relu', strides=(1, 1),\n","    padding='same'))\n","model.add(MaxPool2D((2, 2))) #Downsizing by taking the maximum value of many 2 by 2 areas\n","model.add(Conv2D(16, (3, 3), activation='relu', strides=(1, 1),\n","    padding='same'))\n","model.add(Conv2D(32, (3, 3), activation='relu', strides=(1, 1),\n","    padding='same'))\n","model.add(Conv2D(64, (3, 3), activation='relu', strides=(1, 1),\n","    padding='same'))\n","model.add(MaxPool2D((2, 2)))\n","model.add(Flatten())\n","model.add(Dense(256, activation='relu', kernel_regularizer = regularizers.l2(0.001) )) #, kernel_regularizer = regularizers.l2(0.001)\"\"\"\n","model.add(Dropout(0.5)) #originally 0.5 # drop 128 neuron outputs (half) from the previous layer\n","model.add(Dense(128, activation='relu', kernel_regularizer = regularizers.l2(0.001))) #\"\"\", kernel_regularizer = regularizers.l2(0.001)\"\"\"\n","model.add(Dense(64, activation='relu'))\n","model.add(Dense(64, activation='relu'))\n","model.add(Dense(10, activation='softmax'))\n","model.summary()\n","\n","\n","# In[23]:\n","\n","#print(\"Got to this line\")\n","model.compile(loss='categorical_crossentropy',\n","     optimizer='adam',\n","     metrics=['acc'])\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3) #EarlyStopping prevents overfitting by stopping the test runs after validation accuracy plateaus\n","T0 = time.time()\n","history = model.fit(x_train,y_train, epochs=my_epoch, batch_size= 32, validation_data=(x_test, y_test))\n","#, callbacks=[es]\n","T1 = time.time()\n","score = model.evaluate(x_test, y_test, verbose=1)\n","#print(type(score))\n","print('Test Score:', score[0])\n","print('Test Accuracy:', score[1])\n","print('Training Time:', T1-T0)\n","\n","\n","# In[ ]:\n","\n","\n","print(history.history.keys())\n","\n","print(f\"Rate spike ct '{rateencoderspikecount}'\")\n","print(f\"ISI spike ct '{isiencoderspikecount}'\")\n","print(f\"Epochs '{my_epoch}'\")\n","print(f\"Encoder type '{encoder_type}'\")\n","\n","num_xticks = 20 + 1\n","fig, ax = plt.subplots(nrows = 2, ncols = 2, figsize = (16, 10) )\n","train_x = range(1, len(history.history[\"acc\"])+1)\n","train_y = history.history[\"acc\"]\n","train_loss = history.history[\"loss\"]\n","ax[0,0].plot(train_x, train_y)\n","ax[0,0].set_xlabel(\"Epochs\")\n","ax[0,0].set_ylabel(\"Training Accuracy\")\n","ax[0,0].set_xticks(range(1, num_xticks))\n","ax[0,0].tick_params(axis = \"both\", labelsize = 12)\n","ax[0,0].set_yticks(np.linspace(start = min(train_y), stop = max(train_y), num = 10))\n","\n","val_x = range(1, len(history.history[\"val_acc\"])+1)\n","val_y = history.history[\"val_acc\"]\n","ax[0,1].plot(val_x , val_y)\n","ax[0,1].set_xlabel(\"Epochs\")\n","ax[0,1].set_ylabel(\"Validation Accuracy\")\n","ax[0,1].set_xticks(range(1, num_xticks))\n","ax[0,1].set_yticks(np.linspace(start = min(val_y), stop = max(val_y), num = 10))\n","ax[0,1].tick_params(axis = \"both\", labelsize = 12)\n","\n","train_loss = history.history[\"loss\"]\n","ax[1,0].plot(train_x, train_loss)\n","ax[1,0].set_xlabel(\"Epochs\")\n","ax[1,0].set_ylabel(\"Training Loss\")\n","ax[1,0].set_xticks(range(1, num_xticks))\n","ax[1,0].tick_params(axis = \"both\", labelsize = 12)\n","ax[1,0].set_yticks(np.linspace(start = min(train_loss), stop = max(train_loss), num = 10))\n","\n","val_loss = history.history[\"val_loss\"]\n","ax[1,1].plot(train_x, val_loss)\n","ax[1,1].set_xlabel(\"Epochs\")\n","ax[1,1].set_ylabel(\"Validation Loss\")\n","ax[1,1].set_xticks(range(1, num_xticks))\n","ax[1,1].tick_params(axis = \"both\", labelsize = 12)\n","ax[1,1].set_yticks(np.linspace(start = min(val_loss), stop = max(val_loss), num = 10))\n","\n","#plt.show()\n","\n","\n","# In[ ]:\n","\n","\n","\n","\n"]}]}