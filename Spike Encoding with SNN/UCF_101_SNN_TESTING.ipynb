{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "0. Install Package"
      ],
      "metadata": {
        "id": "yuLQC9Q2guyx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMBzNzvpgo0n",
        "outputId": "54b6685b-95e2-4649-dd49-06920618ea13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/125.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.6/125.6 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install snntorch remotezip tqdm opencv-python-headless"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Download UCF-101 and Unzip"
      ],
      "metadata": {
        "id": "iMKNnJG_hwDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random, pathlib, collections\n",
        "from tqdm import tqdm\n",
        "import remotezip as rz\n",
        "\n",
        "# -----------------------------\n",
        "# Download subset (mimic your notebook style)\n",
        "# -----------------------------\n",
        "URL = \"https://storage.googleapis.com/thumos14_files/UCF101_videos.zip\"\n",
        "\n",
        "def get_class(fname: str) -> str:\n",
        "    # notebook logic: v_ApplyEyeMakeup_g01_c01.avi -> ApplyEyeMakeup\n",
        "    return fname.split(\"_\")[-3]\n",
        "\n",
        "def list_video_files(zip_url: str):\n",
        "    files = []\n",
        "    with rz.RemoteZip(zip_url) as z:\n",
        "        for info in z.infolist():\n",
        "            fn = info.filename\n",
        "            # keep only .avi and ignore directory entries\n",
        "            if fn.endswith(\".avi\") and (\"/\" in fn):\n",
        "                files.append(fn)\n",
        "    return files\n",
        "\n",
        "def get_files_per_class(files):\n",
        "    files_for_class = collections.defaultdict(list)\n",
        "    for fn in files:\n",
        "        cls = get_class(pathlib.Path(fn).name)  # use basename (safe)\n",
        "        files_for_class[cls].append(fn)\n",
        "    return files_for_class\n",
        "\n",
        "def split_class_lists(files_for_class, count):\n",
        "    split_files = []\n",
        "    remainder = {}\n",
        "    for cls in files_for_class:\n",
        "        split_files.extend(files_for_class[cls][:count])\n",
        "        remainder[cls] = files_for_class[cls][count:]\n",
        "    return split_files, remainder\n",
        "\n",
        "def download_from_zip(zip_url, to_dir: pathlib.Path, file_names):\n",
        "    to_dir.mkdir(parents=True, exist_ok=True)\n",
        "    with rz.RemoteZip(zip_url) as z:\n",
        "        for fn in tqdm(file_names, desc=f\"extract -> {to_dir.name}\", leave=False):\n",
        "            cls = get_class(pathlib.Path(fn).name)\n",
        "            out_cls_dir = to_dir / cls\n",
        "            out_cls_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "            # extract keeps original internal path; we rename to basename\n",
        "            z.extract(fn, str(out_cls_dir))\n",
        "            extracted = out_cls_dir / fn\n",
        "            extracted = extracted if extracted.exists() else next(out_cls_dir.glob(\"**/*.avi\"))\n",
        "            out_path = out_cls_dir / pathlib.Path(fn).name\n",
        "            if extracted != out_path:\n",
        "                extracted.rename(out_path)\n",
        "\n",
        "def download_ucf101_subset(zip_url, num_classes, splits, download_dir: pathlib.Path, seed=0):\n",
        "    \"\"\"\n",
        "    splits example: {\"train\": 30, \"val\": 10, \"test\": 10}  (per class counts)\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "\n",
        "    files = list_video_files(zip_url)\n",
        "    files_for_class = get_files_per_class(files)\n",
        "\n",
        "    # choose classes (deterministic)\n",
        "    classes = sorted(files_for_class.keys())[:num_classes]\n",
        "    files_for_class = {c: files_for_class[c] for c in classes}\n",
        "\n",
        "    # shuffle per class\n",
        "    for c in classes:\n",
        "        random.shuffle(files_for_class[c])\n",
        "\n",
        "    dirs = {}\n",
        "    for split_name, per_class_count in splits.items():\n",
        "        split_dir = download_dir / split_name\n",
        "        split_files, files_for_class = split_class_lists(files_for_class, per_class_count)\n",
        "        download_from_zip(zip_url, split_dir, split_files)\n",
        "        dirs[split_name] = split_dir\n",
        "\n",
        "    return dirs\n",
        "\n",
        "# --------- run ----------\n",
        "download_dir = pathlib.Path(\"./UCF101_subset\")\n",
        "subset_paths = download_ucf101_subset(\n",
        "    URL,\n",
        "    num_classes=10,\n",
        "    splits={\"train\": 30, \"val\": 10, \"test\": 10},\n",
        "    download_dir=download_dir,\n",
        "    seed=0\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xd1qmWBhh1UA",
        "outputId": "3d2eec14-25b9-4e6d-f2f8-929a34109f0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Rate Encoding"
      ],
      "metadata": {
        "id": "74ByDIr2hLyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import snntorch as snn\n",
        "from snntorch import spikegen\n",
        "import random\n",
        "import pathlib\n",
        "\n",
        "# =========================================================\n",
        "# Config\n",
        "# =========================================================\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "batch_size = 8\n",
        "num_epochs = 80\n",
        "lr = 2e-3\n",
        "\n",
        "T = 16\n",
        "rate_scale = 1.0\n",
        "tau = 2.0\n",
        "beta = torch.exp(torch.tensor(-1.0 / tau)).item()\n",
        "\n",
        "num_classes = 10\n",
        "H, W = 112, 112\n",
        "\n",
        "# =========================================================\n",
        "# Video Utilities\n",
        "# =========================================================\n",
        "def resize_with_pad(frame_bgr, out_hw=(112,112)):\n",
        "    out_h, out_w = out_hw\n",
        "    h, w = frame_bgr.shape[:2]\n",
        "    if h == 0 or w == 0:\n",
        "        return np.zeros((out_h, out_w, 3), dtype=np.uint8)\n",
        "\n",
        "    scale = min(out_w / w, out_h / h)\n",
        "    nw, nh = int(w * scale), int(h * scale)\n",
        "    resized = cv2.resize(frame_bgr, (nw, nh), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    canvas = np.zeros((out_h, out_w, 3), dtype=np.uint8)\n",
        "    top = (out_h - nh) // 2\n",
        "    left = (out_w - nw) // 2\n",
        "    canvas[top:top+nh, left:left+nw] = resized\n",
        "    return canvas\n",
        "\n",
        "def frames_from_video(video_path, n_frames, out_hw=(112,112), frame_step=2, training=True):\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    need_len = 1 + (n_frames - 1) * frame_step\n",
        "    if length <= 0 or need_len > length:\n",
        "        start = 0\n",
        "    else:\n",
        "        max_start = length - need_len\n",
        "        start = random.randint(0, max_start) if training else 0\n",
        "\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
        "\n",
        "    frames = []\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        cap.release()\n",
        "        return np.zeros((n_frames, out_hw[0], out_hw[1], 3), dtype=np.uint8)\n",
        "\n",
        "    frames.append(resize_with_pad(frame, out_hw))\n",
        "\n",
        "    for _ in range(n_frames - 1):\n",
        "        for _ in range(frame_step):\n",
        "            ret, frame = cap.read()\n",
        "        if ret:\n",
        "            frames.append(resize_with_pad(frame, out_hw))\n",
        "        else:\n",
        "            frames.append(np.zeros_like(frames[0]))\n",
        "\n",
        "    cap.release()\n",
        "    frames = np.stack(frames, axis=0)[..., ::-1].copy()  # BGR -> RGB, make contiguous\n",
        "    return frames\n",
        "\n",
        "# =========================================================\n",
        "# Dataset\n",
        "# =========================================================\n",
        "IM_MEAN = torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1)\n",
        "IM_STD  = torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1)\n",
        "\n",
        "class UCFSubsetDataset(Dataset):\n",
        "    def __init__(self, split_dir, n_frames=16, frame_step=2, training=False, class_to_idx=None):\n",
        "        self.split_dir = pathlib.Path(split_dir)\n",
        "        self.n_frames = n_frames\n",
        "        self.frame_step = frame_step\n",
        "        self.training = training\n",
        "\n",
        "        self.video_paths = sorted(self.split_dir.glob(\"*/*.avi\"))\n",
        "        self.class_names = sorted({p.parent.name for p in self.video_paths})\n",
        "\n",
        "        if class_to_idx is None:\n",
        "            self.class_to_idx = {c:i for i,c in enumerate(self.class_names)}\n",
        "        else:\n",
        "            self.class_to_idx = class_to_idx\n",
        "\n",
        "        self.labels = [self.class_to_idx[p.parent.name] for p in self.video_paths]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        vp = self.video_paths[idx]\n",
        "        y = self.labels[idx]\n",
        "\n",
        "        frames = frames_from_video(vp, self.n_frames, (H,W), self.frame_step, self.training)\n",
        "        x = torch.from_numpy(frames).permute(0,3,1,2).float() / 255.0\n",
        "        x = (x - IM_MEAN) / IM_STD\n",
        "        return x, torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "# =========================================================\n",
        "# Rate Encoder\n",
        "# =========================================================\n",
        "def rate_encode_video(video_btchw, rate_scale=1.0):\n",
        "    \"\"\"\n",
        "    video_btchw: [B,T,C,H,W]\n",
        "    return: [T,B,C,H,W]\n",
        "    \"\"\"\n",
        "    B, T_, C, H_, W_ = video_btchw.shape\n",
        "    p = torch.tanh(video_btchw).add(1).mul(0.5)\n",
        "    p = torch.clamp(p * rate_scale, 0.0, 1.0)\n",
        "\n",
        "    spk_list = []\n",
        "    for t in range(T_):\n",
        "        spk_t = spikegen.rate(p[:, t], num_steps=1)\n",
        "        spk_list.append(spk_t[0])\n",
        "    return torch.stack(spk_list, dim=0)\n",
        "\n",
        "# =========================================================\n",
        "# Conv3D SNN\n",
        "# =========================================================\n",
        "class Conv3DSNN(nn.Module):\n",
        "    def __init__(self, beta=0.95, num_classes=10):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv3d(3, 32, 3, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm3d(32)\n",
        "        self.lif1 = snn.Leaky(beta=beta)\n",
        "\n",
        "        self.conv2 = nn.Conv3d(32, 64, 3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm3d(64)\n",
        "        self.lif2 = snn.Leaky(beta=beta)\n",
        "\n",
        "        self.conv3 = nn.Conv3d(64, 128, 3, padding=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm3d(128)\n",
        "        self.lif3 = snn.Leaky(beta=beta)\n",
        "\n",
        "        self.pool = nn.MaxPool3d((1,2,2))\n",
        "        self.gap = nn.AdaptiveAvgPool3d((None,1,1))\n",
        "\n",
        "        self.fc1 = nn.Linear(128, 256)\n",
        "        self.lif4 = snn.Leaky(beta=beta)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "        self.lif5 = snn.Leaky(beta=beta)\n",
        "\n",
        "    def lif_time(self, x, lif, mem):\n",
        "        B,C,T_,H_,W_ = x.shape\n",
        "        spk_list=[]\n",
        "        for t in range(T_):\n",
        "            spk_t, mem = lif(x[:,:,t], mem)\n",
        "            spk_list.append(spk_t)\n",
        "        return torch.stack(spk_list, dim=2), mem\n",
        "\n",
        "    def forward(self, spk_in):\n",
        "        x = spk_in.permute(1,2,0,3,4)  # B,C,T,H,W\n",
        "\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        mem2 = self.lif2.init_leaky()\n",
        "        mem3 = self.lif3.init_leaky()\n",
        "        mem4 = self.lif4.init_leaky()\n",
        "        mem5 = self.lif5.init_leaky()\n",
        "\n",
        "        x = self.conv1(x); x = self.bn1(x); x,mem1=self.lif_time(x,self.lif1,mem1); x=self.pool(x)\n",
        "        x = self.conv2(x); x = self.bn2(x); x,mem2=self.lif_time(x,self.lif2,mem2); x=self.pool(x)\n",
        "        x = self.conv3(x); x = self.bn3(x); x,mem3=self.lif_time(x,self.lif3,mem3); x=self.pool(x)\n",
        "\n",
        "        x = self.gap(x).squeeze(-1).squeeze(-1)  # B,128,T\n",
        "\n",
        "        spk_out=[]\n",
        "        for t in range(x.shape[2]):\n",
        "            h = self.fc1(x[:,:,t])\n",
        "            spk4, mem4 = self.lif4(h, mem4)\n",
        "            o = self.fc2(spk4)\n",
        "            spk5, mem5 = self.lif5(o, mem5)\n",
        "            spk_out.append(spk5)\n",
        "\n",
        "        return torch.stack(spk_out, dim=0)\n",
        "\n",
        "# =========================================================\n",
        "# DataLoaders (subset_paths must exist)\n",
        "# =========================================================\n",
        "train_ds = UCFSubsetDataset(subset_paths[\"train\"], T, training=True)\n",
        "class_to_idx = train_ds.class_to_idx\n",
        "val_ds = UCFSubsetDataset(subset_paths[\"val\"], T, training=False, class_to_idx=class_to_idx)\n",
        "test_ds = UCFSubsetDataset(subset_paths[\"test\"], T, training=False, class_to_idx=class_to_idx)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_ds, batch_size=batch_size)\n",
        "\n",
        "# =========================================================\n",
        "# Train\n",
        "# =========================================================\n",
        "model = Conv3DSNN(beta=beta, num_classes=num_classes).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "    correct,total=0,0\n",
        "    for x,y in loader:\n",
        "        x,y=x.to(device),y.to(device)\n",
        "        spk_in = rate_encode_video(x, rate_scale)\n",
        "        spk_out = model(spk_in)\n",
        "        logits = spk_out.sum(0)\n",
        "        pred = logits.argmax(1)\n",
        "        correct += (pred==y).sum().item()\n",
        "        total += y.numel()\n",
        "    return correct/total\n",
        "\n",
        "for epoch in range(1,num_epochs+1):\n",
        "    model.train()\n",
        "    correct,total,loss_sum=0,0,0\n",
        "    for x,y in train_loader:\n",
        "        x,y=x.to(device),y.to(device)\n",
        "        spk_in = rate_encode_video(x, rate_scale)\n",
        "        spk_out = model(spk_in)\n",
        "        logits = spk_out.sum(0)\n",
        "        loss = F.cross_entropy(logits,y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_sum += loss.item()*y.size(0)\n",
        "        correct += (logits.argmax(1)==y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    scheduler.step()\n",
        "    val_acc = evaluate(val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | \"\n",
        "          f\"train loss {loss_sum/total:.4f} | \"\n",
        "          f\"train acc {correct/total:.4f} | \"\n",
        "          f\"val acc {val_acc:.4f}\")\n",
        "\n",
        "print(\"Test acc:\", evaluate(test_loader))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zA-yGE8RhN1Z",
        "outputId": "cb759e8a-865e-4b69-a238-fb730f96a890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | train loss 2.3263 | train acc 0.1167 | val acc 0.1200\n",
            "Epoch 02 | train loss 2.2885 | train acc 0.1267 | val acc 0.1700\n",
            "Epoch 03 | train loss 2.2695 | train acc 0.2000 | val acc 0.1800\n",
            "Epoch 04 | train loss 2.2058 | train acc 0.1967 | val acc 0.2100\n",
            "Epoch 05 | train loss 2.1404 | train acc 0.2333 | val acc 0.2600\n",
            "Epoch 06 | train loss 2.1736 | train acc 0.2167 | val acc 0.2600\n",
            "Epoch 07 | train loss 2.0945 | train acc 0.2100 | val acc 0.2600\n",
            "Epoch 08 | train loss 2.0617 | train acc 0.2767 | val acc 0.2300\n",
            "Epoch 09 | train loss 2.0118 | train acc 0.2667 | val acc 0.2700\n",
            "Epoch 10 | train loss 2.0755 | train acc 0.2133 | val acc 0.3200\n",
            "Epoch 11 | train loss 1.9930 | train acc 0.2500 | val acc 0.2800\n",
            "Epoch 12 | train loss 1.9515 | train acc 0.2667 | val acc 0.3400\n",
            "Epoch 13 | train loss 1.9433 | train acc 0.2967 | val acc 0.2400\n",
            "Epoch 14 | train loss 1.8943 | train acc 0.2700 | val acc 0.1900\n",
            "Epoch 15 | train loss 1.9251 | train acc 0.3133 | val acc 0.3500\n",
            "Epoch 16 | train loss 1.8417 | train acc 0.3400 | val acc 0.3300\n",
            "Epoch 17 | train loss 1.7697 | train acc 0.3833 | val acc 0.3400\n",
            "Epoch 18 | train loss 1.7136 | train acc 0.3667 | val acc 0.2900\n",
            "Epoch 19 | train loss 1.6266 | train acc 0.4067 | val acc 0.3100\n",
            "Epoch 20 | train loss 1.6784 | train acc 0.3900 | val acc 0.2000\n",
            "Epoch 21 | train loss 1.6493 | train acc 0.3733 | val acc 0.4100\n",
            "Epoch 22 | train loss 1.5801 | train acc 0.4300 | val acc 0.4700\n",
            "Epoch 23 | train loss 1.5598 | train acc 0.3933 | val acc 0.4500\n",
            "Epoch 24 | train loss 1.4139 | train acc 0.4833 | val acc 0.4600\n",
            "Epoch 25 | train loss 1.3596 | train acc 0.4767 | val acc 0.4400\n",
            "Epoch 26 | train loss 1.3155 | train acc 0.4933 | val acc 0.5000\n",
            "Epoch 27 | train loss 1.3689 | train acc 0.5100 | val acc 0.3000\n",
            "Epoch 28 | train loss 1.2215 | train acc 0.5267 | val acc 0.5500\n",
            "Epoch 29 | train loss 1.3282 | train acc 0.5033 | val acc 0.5000\n",
            "Epoch 30 | train loss 1.3623 | train acc 0.4933 | val acc 0.5100\n",
            "Epoch 31 | train loss 1.2117 | train acc 0.5300 | val acc 0.5200\n",
            "Epoch 32 | train loss 1.1606 | train acc 0.5333 | val acc 0.5400\n",
            "Epoch 33 | train loss 1.1513 | train acc 0.5467 | val acc 0.4900\n",
            "Epoch 34 | train loss 1.2112 | train acc 0.5567 | val acc 0.4900\n",
            "Epoch 35 | train loss 1.0181 | train acc 0.6033 | val acc 0.6300\n",
            "Epoch 36 | train loss 1.0327 | train acc 0.5600 | val acc 0.5700\n",
            "Epoch 37 | train loss 1.0666 | train acc 0.6000 | val acc 0.6200\n",
            "Epoch 38 | train loss 1.0717 | train acc 0.6033 | val acc 0.5900\n",
            "Epoch 39 | train loss 1.1312 | train acc 0.5867 | val acc 0.5200\n",
            "Epoch 40 | train loss 0.9001 | train acc 0.6467 | val acc 0.6500\n",
            "Epoch 41 | train loss 1.0418 | train acc 0.6133 | val acc 0.5800\n",
            "Epoch 42 | train loss 0.9206 | train acc 0.6300 | val acc 0.6200\n",
            "Epoch 43 | train loss 0.9454 | train acc 0.6200 | val acc 0.6300\n",
            "Epoch 44 | train loss 0.8905 | train acc 0.6667 | val acc 0.6600\n",
            "Epoch 45 | train loss 0.7383 | train acc 0.7367 | val acc 0.5500\n",
            "Epoch 46 | train loss 0.8903 | train acc 0.6533 | val acc 0.6500\n",
            "Epoch 47 | train loss 0.7184 | train acc 0.7233 | val acc 0.6700\n",
            "Epoch 48 | train loss 0.8206 | train acc 0.6567 | val acc 0.6400\n",
            "Epoch 49 | train loss 0.8339 | train acc 0.7133 | val acc 0.7300\n",
            "Epoch 50 | train loss 0.7312 | train acc 0.7300 | val acc 0.6100\n",
            "Epoch 51 | train loss 0.6786 | train acc 0.7133 | val acc 0.6500\n",
            "Epoch 52 | train loss 0.6558 | train acc 0.7567 | val acc 0.6300\n",
            "Epoch 53 | train loss 0.6770 | train acc 0.7267 | val acc 0.6900\n",
            "Epoch 54 | train loss 0.6440 | train acc 0.7733 | val acc 0.7000\n",
            "Epoch 55 | train loss 0.7172 | train acc 0.7000 | val acc 0.6700\n",
            "Epoch 56 | train loss 0.6841 | train acc 0.7233 | val acc 0.7100\n",
            "Epoch 57 | train loss 0.6692 | train acc 0.7567 | val acc 0.6700\n",
            "Epoch 58 | train loss 0.6350 | train acc 0.7667 | val acc 0.6400\n",
            "Epoch 59 | train loss 0.7339 | train acc 0.7100 | val acc 0.6500\n",
            "Epoch 60 | train loss 0.5448 | train acc 0.7900 | val acc 0.7000\n",
            "Epoch 61 | train loss 0.5516 | train acc 0.7767 | val acc 0.7100\n",
            "Epoch 62 | train loss 0.6045 | train acc 0.7500 | val acc 0.6700\n",
            "Epoch 63 | train loss 0.6342 | train acc 0.7800 | val acc 0.6900\n",
            "Epoch 64 | train loss 0.5734 | train acc 0.7933 | val acc 0.7400\n",
            "Epoch 65 | train loss 0.5699 | train acc 0.8067 | val acc 0.7200\n",
            "Epoch 66 | train loss 0.5262 | train acc 0.8000 | val acc 0.7600\n",
            "Epoch 67 | train loss 0.5680 | train acc 0.7567 | val acc 0.7300\n",
            "Epoch 68 | train loss 0.4821 | train acc 0.8300 | val acc 0.7300\n",
            "Epoch 69 | train loss 0.5059 | train acc 0.7967 | val acc 0.7400\n",
            "Epoch 70 | train loss 0.4893 | train acc 0.8167 | val acc 0.7600\n",
            "Epoch 71 | train loss 0.4808 | train acc 0.8067 | val acc 0.7500\n",
            "Epoch 72 | train loss 0.4703 | train acc 0.8333 | val acc 0.7300\n",
            "Epoch 73 | train loss 0.4519 | train acc 0.8400 | val acc 0.7500\n",
            "Epoch 74 | train loss 0.5038 | train acc 0.8000 | val acc 0.7600\n",
            "Epoch 75 | train loss 0.5044 | train acc 0.7867 | val acc 0.7700\n",
            "Epoch 76 | train loss 0.5314 | train acc 0.7733 | val acc 0.7300\n",
            "Epoch 77 | train loss 0.4507 | train acc 0.8200 | val acc 0.7400\n",
            "Epoch 78 | train loss 0.4912 | train acc 0.8233 | val acc 0.6900\n",
            "Epoch 79 | train loss 0.4594 | train acc 0.8300 | val acc 0.7500\n",
            "Epoch 80 | train loss 0.5153 | train acc 0.7667 | val acc 0.7400\n",
            "Test acc: 0.69\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. TTFS Encoding"
      ],
      "metadata": {
        "id": "NZszAq3TMG9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import snntorch as snn\n",
        "from snntorch import spikegen\n",
        "import random\n",
        "import pathlib\n",
        "\n",
        "# =========================================================\n",
        "# Config\n",
        "# =========================================================\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "batch_size = 8\n",
        "num_epochs = 80\n",
        "lr = 2e-3\n",
        "\n",
        "# video sampling\n",
        "n_frames   = 16     # clip length (also TTFS steps)\n",
        "frame_step = 2      # 1 for more motion info, 2 saves compute\n",
        "H, W       = 112, 112\n",
        "\n",
        "# TTFS/latency params\n",
        "tau_lat = 8.0\n",
        "threshold_lat = 0.001\n",
        "linear_lat = True\n",
        "normalize_lat = True\n",
        "\n",
        "# LIF params\n",
        "tau = 2.0\n",
        "beta = torch.exp(torch.tensor(-1.0 / tau)).item()\n",
        "\n",
        "# CDF build params\n",
        "cdf_bins = 512\n",
        "cdf_max_batches = 200\n",
        "cdf_agg = \"max\"   # \"max\" or \"mean\" to collapse video -> single image for histogram + encoding\n",
        "\n",
        "# checkpoint (optional)\n",
        "ckpt_path = \"./ucf_subset_ttfs_conv3d_snn_ckpt.pth\"\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Video utils (fix negative strides by .copy())\n",
        "# =========================================================\n",
        "def resize_with_pad(frame_bgr, out_hw=(112,112)):\n",
        "    out_h, out_w = out_hw\n",
        "    h, w = frame_bgr.shape[:2]\n",
        "    if h == 0 or w == 0:\n",
        "        return np.zeros((out_h, out_w, 3), dtype=np.uint8)\n",
        "\n",
        "    scale = min(out_w / w, out_h / h)\n",
        "    nw, nh = int(w * scale), int(h * scale)\n",
        "    resized = cv2.resize(frame_bgr, (nw, nh), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    canvas = np.zeros((out_h, out_w, 3), dtype=np.uint8)\n",
        "    top = (out_h - nh) // 2\n",
        "    left = (out_w - nw) // 2\n",
        "    canvas[top:top+nh, left:left+nw] = resized\n",
        "    return canvas\n",
        "\n",
        "def frames_from_video(video_path, n_frames, out_hw=(112,112), frame_step=2, training=True):\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    if not cap.isOpened():\n",
        "        return np.zeros((n_frames, out_hw[0], out_hw[1], 3), dtype=np.uint8)\n",
        "\n",
        "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    need_len = 1 + (n_frames - 1) * frame_step\n",
        "    if length <= 0 or need_len > length:\n",
        "        start = 0\n",
        "    else:\n",
        "        max_start = length - need_len\n",
        "        start = random.randint(0, max_start) if training else 0\n",
        "\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
        "\n",
        "    frames = []\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        cap.release()\n",
        "        return np.zeros((n_frames, out_hw[0], out_hw[1], 3), dtype=np.uint8)\n",
        "\n",
        "    frames.append(resize_with_pad(frame, out_hw))\n",
        "\n",
        "    for _ in range(n_frames - 1):\n",
        "        for _ in range(frame_step):\n",
        "            ret, frame = cap.read()\n",
        "        if ret:\n",
        "            frames.append(resize_with_pad(frame, out_hw))\n",
        "        else:\n",
        "            frames.append(np.zeros_like(frames[0]))\n",
        "\n",
        "    cap.release()\n",
        "    # BGR -> RGB; make contiguous to avoid negative strides\n",
        "    frames = np.stack(frames, axis=0)[..., ::-1].copy()\n",
        "    return frames  # [T,H,W,3] uint8 RGB\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Dataset (subset_paths from your downloader)\n",
        "# x is ImageNet-normalized: [T,C,H,W]\n",
        "# =========================================================\n",
        "IM_MEAN = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
        "IM_STD  = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
        "\n",
        "class UCFSubsetDataset(Dataset):\n",
        "    def __init__(self, split_dir, n_frames=16, frame_step=2, training=False, class_to_idx=None):\n",
        "        self.split_dir = pathlib.Path(split_dir)\n",
        "        self.n_frames = n_frames\n",
        "        self.frame_step = frame_step\n",
        "        self.training = training\n",
        "\n",
        "        self.video_paths = sorted(self.split_dir.glob(\"*/*.avi\"))\n",
        "        self.class_names = sorted({p.parent.name for p in self.video_paths})\n",
        "\n",
        "        if class_to_idx is None:\n",
        "            self.class_to_idx = {c:i for i,c in enumerate(self.class_names)}\n",
        "        else:\n",
        "            self.class_to_idx = class_to_idx\n",
        "\n",
        "        self.labels = [self.class_to_idx[p.parent.name] for p in self.video_paths]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        vp = self.video_paths[idx]\n",
        "        y = self.labels[idx]\n",
        "\n",
        "        frames = frames_from_video(vp, self.n_frames, (H,W), self.frame_step, self.training)  # [T,H,W,3]\n",
        "        x = torch.from_numpy(frames).permute(0,3,1,2).float() / 255.0                          # [T,3,H,W]\n",
        "        x = (x - IM_MEAN) / IM_STD\n",
        "        return x, torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "def collate_btchw(batch):\n",
        "    xs, ys = zip(*batch)\n",
        "    x = torch.stack(xs, dim=0)  # [B,T,3,H,W]\n",
        "    y = torch.stack(ys, dim=0)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# CDF build (mimic your CIFAR CDF builder)\n",
        "# =========================================================\n",
        "@torch.no_grad()\n",
        "def build_cdf_from_trainloader_ucf(train_loader, num_bins=512, max_batches=200, device=\"cuda\", agg=\"max\"):\n",
        "    hist = torch.zeros(num_bins, device=device)\n",
        "    mean = IM_MEAN.to(device)  # [1,3,1,1]\n",
        "    std  = IM_STD.to(device)\n",
        "\n",
        "    for i, (x_norm, _) in enumerate(train_loader):\n",
        "        if max_batches is not None and i >= max_batches:\n",
        "            break\n",
        "\n",
        "        x_norm = x_norm.to(device, non_blocking=True)        # [B,T,3,H,W]\n",
        "        x_raw = (x_norm * std + mean).clamp(0.0, 1.0)        # [B,T,3,H,W]\n",
        "\n",
        "        if agg == \"max\":\n",
        "            x_img = x_raw.max(dim=1).values                  # [B,3,H,W]\n",
        "        elif agg == \"mean\":\n",
        "            x_img = x_raw.mean(dim=1)\n",
        "        else:\n",
        "            raise ValueError(\"agg must be 'max' or 'mean'\")\n",
        "\n",
        "        v = x_img.flatten()\n",
        "        hist += torch.histc(v, bins=num_bins, min=0.0, max=1.0)\n",
        "\n",
        "    hist = hist / (hist.sum() + 1e-12)\n",
        "    cdf = torch.cumsum(hist, dim=0)\n",
        "    bin_edges = torch.linspace(0.0, 1.0, steps=num_bins + 1, device=device)\n",
        "    return bin_edges, cdf\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# TTFS encoder (equalized latency) for UCF subset\n",
        "# Output: [T_steps, B, 3, H, W]\n",
        "# =========================================================\n",
        "@torch.no_grad()\n",
        "def ttfs_encode_ucf_equalized_latency(\n",
        "    x_video_norm, T_steps, bin_edges, cdf,\n",
        "    agg=\"max\",\n",
        "    normalize=True, linear=True, tau=8.0, threshold=0.001\n",
        "):\n",
        "    \"\"\"\n",
        "    x_video_norm: [B,T,3,H,W] normalized\n",
        "    \"\"\"\n",
        "    device = x_video_norm.device\n",
        "    mean = IM_MEAN.to(device)\n",
        "    std  = IM_STD.to(device)\n",
        "\n",
        "    x_raw = (x_video_norm * std + mean).clamp(0.0, 1.0)  # [B,T,3,H,W]\n",
        "\n",
        "    if agg == \"max\":\n",
        "        x_img = x_raw.max(dim=1).values                  # [B,3,H,W]\n",
        "    elif agg == \"mean\":\n",
        "        x_img = x_raw.mean(dim=1)\n",
        "    else:\n",
        "        raise ValueError(\"agg must be 'max' or 'mean'\")\n",
        "\n",
        "    nb = cdf.numel()\n",
        "    idx = torch.bucketize(x_img, bin_edges[1:-1], right=False).clamp(0, nb - 1)\n",
        "    u = cdf[idx].clamp(1e-4, 1.0 - 1e-4)\n",
        "\n",
        "    spk = spikegen.latency(\n",
        "        u,\n",
        "        num_steps=T_steps,\n",
        "        normalize=normalize,\n",
        "        linear=linear,\n",
        "        tau=tau,\n",
        "        threshold=threshold\n",
        "    )\n",
        "    return spk  # [T_steps,B,3,H,W]\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Conv3D SNN (TTFS input)\n",
        "# - last layer outputs analog logits for stability\n",
        "# =========================================================\n",
        "class Conv3DSNN_TTFS(nn.Module):\n",
        "    def __init__(self, beta=0.95, num_classes=10):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv3d(3, 32, 3, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm3d(32)\n",
        "        self.lif1 = snn.Leaky(beta=beta)\n",
        "\n",
        "        self.conv2 = nn.Conv3d(32, 64, 3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm3d(64)\n",
        "        self.lif2 = snn.Leaky(beta=beta)\n",
        "\n",
        "        self.conv3 = nn.Conv3d(64, 128, 3, padding=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm3d(128)\n",
        "        self.lif3 = snn.Leaky(beta=beta)\n",
        "\n",
        "        self.pool = nn.MaxPool3d((1,2,2))\n",
        "        self.gap = nn.AdaptiveAvgPool3d((None,1,1))\n",
        "\n",
        "        self.fc1 = nn.Linear(128, 256)\n",
        "        self.lif4 = snn.Leaky(beta=beta)\n",
        "        self.fc2 = nn.Linear(256, num_classes)  # analog logits\n",
        "\n",
        "    def _lif_over_time_2d(self, x_bcthw, lif, mem):\n",
        "        B,C,T,H,W = x_bcthw.shape\n",
        "        spk_list = []\n",
        "        for t in range(T):\n",
        "            spk_t, mem = lif(x_bcthw[:, :, t], mem)  # [B,C,H,W]\n",
        "            spk_list.append(spk_t)\n",
        "        return torch.stack(spk_list, dim=2), mem\n",
        "\n",
        "    def forward(self, spk_in):\n",
        "        \"\"\"\n",
        "        spk_in: [T,B,3,H,W]\n",
        "        return logits_rec: [T,B,num_classes]\n",
        "        \"\"\"\n",
        "        x = spk_in.permute(1,2,0,3,4).contiguous()  # [B,3,T,H,W]\n",
        "\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        mem2 = self.lif2.init_leaky()\n",
        "        mem3 = self.lif3.init_leaky()\n",
        "        mem4 = self.lif4.init_leaky()\n",
        "\n",
        "        x = self.conv1(x); x = self.bn1(x); x, mem1 = self._lif_over_time_2d(x, self.lif1, mem1); x = self.pool(x)\n",
        "        x = self.conv2(x); x = self.bn2(x); x, mem2 = self._lif_over_time_2d(x, self.lif2, mem2); x = self.pool(x)\n",
        "        x = self.conv3(x); x = self.bn3(x); x, mem3 = self._lif_over_time_2d(x, self.lif3, mem3); x = self.pool(x)\n",
        "\n",
        "        x = self.gap(x).squeeze(-1).squeeze(-1)  # [B,128,T]\n",
        "\n",
        "        logits_list = []\n",
        "        for t in range(x.shape[2]):\n",
        "            h = self.fc1(x[:,:,t])\n",
        "            spk4, mem4 = self.lif4(h, mem4)\n",
        "            logits_t = self.fc2(spk4)\n",
        "            logits_list.append(logits_t)\n",
        "\n",
        "        return torch.stack(logits_list, dim=0)\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Build loaders\n",
        "# =========================================================\n",
        "train_ds = UCFSubsetDataset(subset_paths[\"train\"], n_frames=n_frames, frame_step=frame_step, training=True)\n",
        "class_to_idx = train_ds.class_to_idx\n",
        "val_ds   = UCFSubsetDataset(subset_paths[\"val\"],   n_frames=n_frames, frame_step=frame_step, training=False, class_to_idx=class_to_idx)\n",
        "test_ds  = UCFSubsetDataset(subset_paths[\"test\"],  n_frames=n_frames, frame_step=frame_step, training=False, class_to_idx=class_to_idx)\n",
        "\n",
        "num_classes = len(class_to_idx)\n",
        "T_steps = train_ds.n_frames\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=2, pin_memory=True, collate_fn=collate_btchw)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, drop_last=False, num_workers=2, pin_memory=True, collate_fn=collate_btchw)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, drop_last=False, num_workers=2, pin_memory=True, collate_fn=collate_btchw)\n",
        "\n",
        "print(\"num_classes =\", num_classes, \" | T_steps =\", T_steps, \" | train batches =\", len(train_loader))\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Build CDF once\n",
        "# =========================================================\n",
        "bin_edges, cdf = build_cdf_from_trainloader_ucf(\n",
        "    train_loader, num_bins=cdf_bins, max_batches=cdf_max_batches, device=device, agg=cdf_agg\n",
        ")\n",
        "print(\"CDF built.\")\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Model + Optim\n",
        "# =========================================================\n",
        "model = Conv3DSNN_TTFS(beta=beta, num_classes=num_classes).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Resume (optional)\n",
        "# =========================================================\n",
        "start_epoch = 1\n",
        "if os.path.exists(ckpt_path):\n",
        "    ckpt = torch.load(ckpt_path, map_location=device)\n",
        "    model.load_state_dict(ckpt[\"model_state\"])\n",
        "    optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n",
        "    scheduler.load_state_dict(ckpt[\"scheduler_state\"])\n",
        "    start_epoch = ckpt[\"epoch\"] + 1\n",
        "    print(f\"Resumed from epoch {start_epoch}\")\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Eval\n",
        "# =========================================================\n",
        "@torch.no_grad()\n",
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "    correct, total, loss_sum = 0, 0, 0.0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x = x.to(device, non_blocking=True)  # [B,T,3,H,W]\n",
        "        y = y.to(device, non_blocking=True)\n",
        "\n",
        "        spk_in = ttfs_encode_ucf_equalized_latency(\n",
        "            x, T_steps=T_steps, bin_edges=bin_edges, cdf=cdf,\n",
        "            agg=cdf_agg, normalize=normalize_lat, linear=linear_lat,\n",
        "            tau=tau_lat, threshold=threshold_lat\n",
        "        )  # [T,B,3,H,W]\n",
        "\n",
        "        logits_rec = model(spk_in)       # [T,B,C]\n",
        "        logits = logits_rec.mean(dim=0)  # [B,C]\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "\n",
        "        pred = logits.argmax(dim=1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.numel()\n",
        "        loss_sum += loss.item() * y.size(0)\n",
        "\n",
        "    return loss_sum / max(total, 1), correct / max(total, 1)\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Train\n",
        "# =========================================================\n",
        "for epoch in range(start_epoch, num_epochs + 1):\n",
        "    model.train()\n",
        "    running_loss, running_correct, running_total = 0.0, 0, 0\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        y = y.to(device, non_blocking=True)\n",
        "\n",
        "        spk_in = ttfs_encode_ucf_equalized_latency(\n",
        "            x, T_steps=T_steps, bin_edges=bin_edges, cdf=cdf,\n",
        "            agg=cdf_agg, normalize=normalize_lat, linear=linear_lat,\n",
        "            tau=tau_lat, threshold=threshold_lat\n",
        "        )\n",
        "\n",
        "        logits_rec = model(spk_in)\n",
        "        logits = logits_rec.mean(dim=0)\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * y.size(0)\n",
        "        running_correct += (logits.argmax(dim=1) == y).sum().item()\n",
        "        running_total += y.size(0)\n",
        "\n",
        "    scheduler.step()\n",
        "    val_loss, val_acc = evaluate(val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | \"\n",
        "          f\"train loss {running_loss/running_total:.4f} | \"\n",
        "          f\"train acc {running_correct/running_total:.4f} | \"\n",
        "          f\"val loss {val_loss:.4f} | \"\n",
        "          f\"val acc {val_acc:.4f}\")\n",
        "\n",
        "    torch.save({\n",
        "        \"epoch\": epoch,\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"optimizer_state\": optimizer.state_dict(),\n",
        "        \"scheduler_state\": scheduler.state_dict(),\n",
        "        \"class_to_idx\": class_to_idx,\n",
        "        \"T_steps\": T_steps,\n",
        "        \"cdf_bins\": cdf_bins,\n",
        "        \"cdf_agg\": cdf_agg,\n",
        "        \"tau_lat\": tau_lat,\n",
        "        \"threshold_lat\": threshold_lat,\n",
        "    }, ckpt_path)\n",
        "    print(f\"Saved checkpoint: {ckpt_path}\")\n",
        "\n",
        "test_loss, test_acc = evaluate(test_loader)\n",
        "print(f\"Test loss {test_loss:.4f} | Test acc {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nz5i_v7RMJEm",
        "outputId": "fa1c2f77-ffc8-46e2-b98d-66e133b38b62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_classes = 10  | T_steps = 16  | train batches = 37\n",
            "CDF built.\n",
            "Resumed from epoch 31\n",
            "Epoch 31 | train loss 1.5144 | train acc 0.3986 | val loss 1.4767 | val acc 0.4600\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 32 | train loss 1.5405 | train acc 0.4189 | val loss 1.4818 | val acc 0.4100\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 33 | train loss 1.5395 | train acc 0.4122 | val loss 1.4781 | val acc 0.4500\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 34 | train loss 1.5093 | train acc 0.4527 | val loss 1.4703 | val acc 0.4600\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 35 | train loss 1.5111 | train acc 0.4257 | val loss 1.4746 | val acc 0.4700\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 36 | train loss 1.5327 | train acc 0.4122 | val loss 1.4615 | val acc 0.4800\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 37 | train loss 1.5288 | train acc 0.4527 | val loss 1.4710 | val acc 0.4700\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 38 | train loss 1.5290 | train acc 0.4020 | val loss 1.4739 | val acc 0.4300\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 39 | train loss 1.5166 | train acc 0.4223 | val loss 1.4669 | val acc 0.4700\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 40 | train loss 1.5582 | train acc 0.4189 | val loss 1.7019 | val acc 0.3400\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 41 | train loss 1.5305 | train acc 0.4493 | val loss 1.5091 | val acc 0.3400\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 42 | train loss 1.5408 | train acc 0.4189 | val loss 1.4234 | val acc 0.5000\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 43 | train loss 1.5440 | train acc 0.4189 | val loss 1.4547 | val acc 0.4300\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 44 | train loss 1.5201 | train acc 0.4257 | val loss 1.6000 | val acc 0.3600\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 45 | train loss 1.5580 | train acc 0.3953 | val loss 1.5607 | val acc 0.4100\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 46 | train loss 1.5462 | train acc 0.3851 | val loss 1.5011 | val acc 0.3800\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 47 | train loss 1.5363 | train acc 0.4122 | val loss 1.5817 | val acc 0.4200\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 48 | train loss 1.5674 | train acc 0.3649 | val loss 2.8090 | val acc 0.2000\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 49 | train loss 1.6574 | train acc 0.3750 | val loss 1.7111 | val acc 0.3400\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 50 | train loss 1.5208 | train acc 0.4189 | val loss 2.2612 | val acc 0.2300\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 51 | train loss 1.4850 | train acc 0.4189 | val loss 2.1725 | val acc 0.2900\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 52 | train loss 1.5133 | train acc 0.3885 | val loss 1.4305 | val acc 0.5300\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 53 | train loss 1.4303 | train acc 0.4527 | val loss 1.4553 | val acc 0.4700\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 54 | train loss 1.4699 | train acc 0.4257 | val loss 1.3779 | val acc 0.4900\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 55 | train loss 1.5349 | train acc 0.3919 | val loss 1.5921 | val acc 0.3600\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 56 | train loss 1.4605 | train acc 0.4459 | val loss 1.3836 | val acc 0.5200\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 57 | train loss 1.4658 | train acc 0.4155 | val loss 1.7163 | val acc 0.3800\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 58 | train loss 1.5164 | train acc 0.4426 | val loss 1.4776 | val acc 0.4300\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 59 | train loss 1.5482 | train acc 0.4155 | val loss 1.5239 | val acc 0.4700\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 60 | train loss 1.4445 | train acc 0.4628 | val loss 1.9547 | val acc 0.3500\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 61 | train loss 1.3599 | train acc 0.4865 | val loss 1.6364 | val acc 0.3900\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 62 | train loss 1.3238 | train acc 0.5135 | val loss 1.5101 | val acc 0.3900\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 63 | train loss 1.3313 | train acc 0.5101 | val loss 1.7403 | val acc 0.3800\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 64 | train loss 1.3394 | train acc 0.5203 | val loss 1.3319 | val acc 0.5400\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 65 | train loss 1.3253 | train acc 0.5034 | val loss 1.9596 | val acc 0.3300\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 66 | train loss 1.2852 | train acc 0.5270 | val loss 1.6975 | val acc 0.3900\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 67 | train loss 1.2572 | train acc 0.5203 | val loss 1.7777 | val acc 0.4000\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 68 | train loss 1.1999 | train acc 0.5676 | val loss 1.6855 | val acc 0.3900\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 69 | train loss 1.2265 | train acc 0.5304 | val loss 1.7048 | val acc 0.3800\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 70 | train loss 1.1770 | train acc 0.5608 | val loss 1.1686 | val acc 0.5600\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 71 | train loss 1.1224 | train acc 0.5574 | val loss 1.4887 | val acc 0.4500\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 72 | train loss 1.0980 | train acc 0.6014 | val loss 1.1945 | val acc 0.6100\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 73 | train loss 1.0759 | train acc 0.6047 | val loss 1.1840 | val acc 0.6000\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 74 | train loss 1.0608 | train acc 0.6149 | val loss 1.3982 | val acc 0.5100\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 75 | train loss 1.0338 | train acc 0.6419 | val loss 1.1538 | val acc 0.5700\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 76 | train loss 0.9319 | train acc 0.6791 | val loss 1.1226 | val acc 0.6300\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 77 | train loss 0.8942 | train acc 0.6689 | val loss 1.0417 | val acc 0.6000\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 78 | train loss 0.8841 | train acc 0.7128 | val loss 1.4350 | val acc 0.4900\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 79 | train loss 0.8741 | train acc 0.7162 | val loss 1.0940 | val acc 0.6200\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Epoch 80 | train loss 0.8311 | train acc 0.7297 | val loss 1.0678 | val acc 0.6400\n",
            "Saved checkpoint: ./ucf_subset_ttfs_conv3d_snn_ckpt.pth\n",
            "Test loss 1.2037 | Test acc 0.5800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. ISI Encoding"
      ],
      "metadata": {
        "id": "O9ECCdL_URa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "import pathlib\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import snntorch as snn\n",
        "\n",
        "# =========================================================\n",
        "# Config\n",
        "# =========================================================\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "batch_size = 8\n",
        "num_epochs = 80\n",
        "lr = 2e-3\n",
        "\n",
        "# video sampling\n",
        "T_steps    = 16     # frames_per_clip (also encoder time steps)\n",
        "frame_step = 2\n",
        "H, W       = 112, 112\n",
        "\n",
        "# ISI encoder\n",
        "K_spikes  = 2\n",
        "alpha_max = 2.0\n",
        "eps = 1e-3\n",
        "agg_mode = \"max\"     # \"max\" or \"mean\" to collapse clip -> image before ISI\n",
        "\n",
        "# LIF\n",
        "tau = 2.0\n",
        "beta = torch.exp(torch.tensor(-1.0 / tau)).item()\n",
        "\n",
        "# checkpoint paths\n",
        "last_ckpt_path = \"./ucf_subset_isi_last.pth\"\n",
        "best_ckpt_path = \"./ucf_subset_isi_best.pth\"\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Video utils\n",
        "# =========================================================\n",
        "def resize_with_pad(frame_bgr, out_hw=(112,112)):\n",
        "    out_h, out_w = out_hw\n",
        "    h, w = frame_bgr.shape[:2]\n",
        "    if h == 0 or w == 0:\n",
        "        return np.zeros((out_h, out_w, 3), dtype=np.uint8)\n",
        "\n",
        "    scale = min(out_w / w, out_h / h)\n",
        "    nw, nh = int(w * scale), int(h * scale)\n",
        "    resized = cv2.resize(frame_bgr, (nw, nh), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    canvas = np.zeros((out_h, out_w, 3), dtype=np.uint8)\n",
        "    top = (out_h - nh) // 2\n",
        "    left = (out_w - nw) // 2\n",
        "    canvas[top:top+nh, left:left+nw] = resized\n",
        "    return canvas\n",
        "\n",
        "def frames_from_video(video_path, n_frames, out_hw=(112,112), frame_step=2, training=True):\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    if not cap.isOpened():\n",
        "        return np.zeros((n_frames, out_hw[0], out_hw[1], 3), dtype=np.uint8)\n",
        "\n",
        "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    need_len = 1 + (n_frames - 1) * frame_step\n",
        "    if length <= 0 or need_len > length:\n",
        "        start = 0\n",
        "    else:\n",
        "        max_start = length - need_len\n",
        "        start = random.randint(0, max_start) if training else 0\n",
        "\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
        "\n",
        "    frames = []\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        cap.release()\n",
        "        return np.zeros((n_frames, out_hw[0], out_hw[1], 3), dtype=np.uint8)\n",
        "\n",
        "    frames.append(resize_with_pad(frame, out_hw))\n",
        "\n",
        "    for _ in range(n_frames - 1):\n",
        "        for _ in range(frame_step):\n",
        "            ret, frame = cap.read()\n",
        "        if ret:\n",
        "            frames.append(resize_with_pad(frame, out_hw))\n",
        "        else:\n",
        "            frames.append(np.zeros_like(frames[0]))\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # BGR -> RGB and make contiguous (avoid negative strides)\n",
        "    frames = np.stack(frames, axis=0)[..., ::-1].copy()\n",
        "    return frames  # [T,H,W,3] uint8 RGB\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Dataset\n",
        "# =========================================================\n",
        "IM_MEAN = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
        "IM_STD  = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
        "\n",
        "class UCFSubsetDataset(Dataset):\n",
        "    def __init__(self, split_dir, n_frames=16, frame_step=2, training=False, class_to_idx=None):\n",
        "        self.split_dir = pathlib.Path(split_dir)\n",
        "        self.n_frames = n_frames\n",
        "        self.frame_step = frame_step\n",
        "        self.training = training\n",
        "\n",
        "        self.video_paths = sorted(self.split_dir.glob(\"*/*.avi\"))\n",
        "        self.class_names = sorted({p.parent.name for p in self.video_paths})\n",
        "\n",
        "        if class_to_idx is None:\n",
        "            self.class_to_idx = {c:i for i,c in enumerate(self.class_names)}\n",
        "        else:\n",
        "            self.class_to_idx = class_to_idx\n",
        "\n",
        "        self.labels = [self.class_to_idx[p.parent.name] for p in self.video_paths]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        vp = self.video_paths[idx]\n",
        "        y = self.labels[idx]\n",
        "\n",
        "        frames = frames_from_video(vp, self.n_frames, (H,W), self.frame_step, self.training)\n",
        "        x = torch.from_numpy(frames).permute(0,3,1,2).float() / 255.0   # [T,3,H,W]\n",
        "        x = (x - IM_MEAN) / IM_STD                                       # ImageNet normalize\n",
        "        return x, torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "def collate_btchw(batch):\n",
        "    xs, ys = zip(*batch)\n",
        "    x = torch.stack(xs, dim=0)  # [B,T,3,H,W]\n",
        "    y = torch.stack(ys, dim=0)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# ISI fixed-K strict encoder (same spirit as your snippet)\n",
        "# =========================================================\n",
        "@torch.no_grad()\n",
        "def isi_fixedK_no_endcaps_strict(\n",
        "    x_img_unit: torch.Tensor, T: int, K: int, alpha_max: float = 2.0, eps: float = 1e-3\n",
        ") -> torch.Tensor:\n",
        "    assert T >= 2 and K >= 1\n",
        "    if K > T:\n",
        "        raise ValueError(f\"K={K} must satisfy K<=T={T}.\")\n",
        "\n",
        "    device = x_img_unit.device\n",
        "    B = x_img_unit.size(0)\n",
        "    x = x_img_unit.view(B, -1).clamp(0.0, 1.0)\n",
        "    N = x.size(1)\n",
        "\n",
        "    M = T\n",
        "    j = torch.arange(M, device=device, dtype=torch.float32).view(1, 1, M)\n",
        "    mid = (M - 1) / 2.0\n",
        "\n",
        "    alpha = (x * 2.0 - 1.0) * alpha_max\n",
        "    alpha = alpha.unsqueeze(-1)\n",
        "\n",
        "    w = torch.exp(alpha * (j - mid))\n",
        "    w = w / (w.sum(dim=-1, keepdim=True) + 1e-12)\n",
        "    c = torch.cumsum(w, dim=-1)\n",
        "\n",
        "    q = torch.linspace(eps, 1.0 - eps, steps=K, device=device, dtype=torch.float32)\n",
        "    q = q.view(1, 1, K).expand(B, N, K)\n",
        "\n",
        "    t_idx = torch.searchsorted(c, q).clamp(0, T - 1).long()\n",
        "    t_idx, _ = torch.sort(t_idx, dim=-1)\n",
        "\n",
        "    used = torch.zeros(B, N, T, device=device, dtype=torch.bool)\n",
        "    t_fixed = torch.full_like(t_idx, -1)\n",
        "\n",
        "    for k in range(K):\n",
        "        tk = t_idx[..., k]\n",
        "        free = ~used.gather(dim=2, index=tk.unsqueeze(-1)).squeeze(-1)\n",
        "        t_fixed[..., k] = torch.where(free, tk, torch.full_like(tk, -1))\n",
        "        if free.any():\n",
        "            used[free] |= F.one_hot(tk[free], num_classes=T).bool()\n",
        "\n",
        "    for k in range(K):\n",
        "        need = (t_fixed[..., k] < 0)\n",
        "        if not need.any():\n",
        "            continue\n",
        "\n",
        "        tk = t_idx[..., k].clone()\n",
        "        avail = ~used\n",
        "        ar = torch.arange(T, device=device).view(1, 1, T)\n",
        "\n",
        "        forward_mask = avail & (ar >= tk.unsqueeze(-1))\n",
        "        fwd_pos = forward_mask.float().argmax(dim=-1)\n",
        "        fwd_exists = forward_mask.any(dim=-1)\n",
        "\n",
        "        backward_mask = avail & (ar <= tk.unsqueeze(-1))\n",
        "        rev = torch.flip(backward_mask, dims=[-1])\n",
        "        bwd_pos_rev = rev.float().argmax(dim=-1)\n",
        "        bwd_pos = (T - 1) - bwd_pos_rev\n",
        "        bwd_exists = backward_mask.any(dim=-1)\n",
        "\n",
        "        chosen = torch.where(fwd_exists, fwd_pos, bwd_pos).long()\n",
        "        chosen = torch.where(fwd_exists | bwd_exists, chosen, torch.zeros_like(chosen))\n",
        "\n",
        "        t_fixed[..., k] = torch.where(need, chosen, t_fixed[..., k])\n",
        "        used[need] |= F.one_hot(chosen[need], num_classes=T).bool()\n",
        "\n",
        "    spk_flat = torch.zeros(T, B, N, device=device, dtype=torch.float32)\n",
        "    b_idx = torch.arange(B, device=device).view(B, 1, 1).expand(B, N, K)\n",
        "    n_idx = torch.arange(N, device=device).view(1, N, 1).expand(B, N, K)\n",
        "    spk_flat[t_fixed, b_idx, n_idx] = 1.0\n",
        "\n",
        "    return spk_flat.view(T, B, *x_img_unit.shape[1:])  # [T,B,3,H,W]\n",
        "\n",
        "@torch.no_grad()\n",
        "def isi_encode_ucf_video(\n",
        "    x_video_norm: torch.Tensor,  # [B,T,3,H,W] normalized\n",
        "    T_steps: int,\n",
        "    K: int,\n",
        "    alpha_max: float = 2.0,\n",
        "    eps: float = 1e-3,\n",
        "    agg=\"max\",\n",
        "):\n",
        "    mean = IM_MEAN.to(x_video_norm.device)\n",
        "    std  = IM_STD.to(x_video_norm.device)\n",
        "    x_raw = (x_video_norm * std + mean).clamp(0.0, 1.0)\n",
        "\n",
        "    if agg == \"max\":\n",
        "        x_img = x_raw.max(dim=1).values\n",
        "    elif agg == \"mean\":\n",
        "        x_img = x_raw.mean(dim=1)\n",
        "    else:\n",
        "        raise ValueError(\"agg must be 'max' or 'mean'\")\n",
        "\n",
        "    return isi_fixedK_no_endcaps_strict(x_img, T=T_steps, K=K, alpha_max=alpha_max, eps=eps)\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Conv3D SNN (analog logits head)\n",
        "# =========================================================\n",
        "class Conv3DSNN_ISI(nn.Module):\n",
        "    def __init__(self, beta=0.95, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv3d(3, 32, 3, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm3d(32)\n",
        "        self.lif1 = snn.Leaky(beta=beta)\n",
        "\n",
        "        self.conv2 = nn.Conv3d(32, 64, 3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm3d(64)\n",
        "        self.lif2 = snn.Leaky(beta=beta)\n",
        "\n",
        "        self.conv3 = nn.Conv3d(64, 128, 3, padding=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm3d(128)\n",
        "        self.lif3 = snn.Leaky(beta=beta)\n",
        "\n",
        "        self.pool = nn.MaxPool3d((1,2,2))\n",
        "        self.gap = nn.AdaptiveAvgPool3d((None,1,1))\n",
        "\n",
        "        self.fc1 = nn.Linear(128, 256)\n",
        "        self.lif4 = snn.Leaky(beta=beta)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def _lif_over_time_2d(self, x_bcthw, lif, mem):\n",
        "        B,C,T,H,W = x_bcthw.shape\n",
        "        spk_list=[]\n",
        "        for t in range(T):\n",
        "            spk_t, mem = lif(x_bcthw[:, :, t], mem)\n",
        "            spk_list.append(spk_t)\n",
        "        return torch.stack(spk_list, dim=2), mem\n",
        "\n",
        "    def forward(self, spk_in):\n",
        "        # spk_in: [T,B,3,H,W]\n",
        "        x = spk_in.permute(1,2,0,3,4).contiguous()  # [B,3,T,H,W]\n",
        "\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        mem2 = self.lif2.init_leaky()\n",
        "        mem3 = self.lif3.init_leaky()\n",
        "        mem4 = self.lif4.init_leaky()\n",
        "\n",
        "        x = self.conv1(x); x = self.bn1(x); x, mem1 = self._lif_over_time_2d(x, self.lif1, mem1); x = self.pool(x)\n",
        "        x = self.conv2(x); x = self.bn2(x); x, mem2 = self._lif_over_time_2d(x, self.lif2, mem2); x = self.pool(x)\n",
        "        x = self.conv3(x); x = self.bn3(x); x, mem3 = self._lif_over_time_2d(x, self.lif3, mem3); x = self.pool(x)\n",
        "\n",
        "        x = self.gap(x).squeeze(-1).squeeze(-1)  # [B,128,T]\n",
        "\n",
        "        logits_list=[]\n",
        "        for t in range(x.shape[2]):\n",
        "            h = self.fc1(x[:,:,t])\n",
        "            spk4, mem4 = self.lif4(h, mem4)\n",
        "            logits_t = self.fc2(spk4)\n",
        "            logits_list.append(logits_t)\n",
        "\n",
        "        return torch.stack(logits_list, dim=0)  # [T,B,num_classes]\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Build loaders from subset_paths (must exist)\n",
        "# =========================================================\n",
        "train_ds = UCFSubsetDataset(subset_paths[\"train\"], n_frames=T_steps, frame_step=frame_step, training=True)\n",
        "class_to_idx = train_ds.class_to_idx\n",
        "val_ds   = UCFSubsetDataset(subset_paths[\"val\"],   n_frames=T_steps, frame_step=frame_step, training=False, class_to_idx=class_to_idx)\n",
        "test_ds  = UCFSubsetDataset(subset_paths[\"test\"],  n_frames=T_steps, frame_step=frame_step, training=False, class_to_idx=class_to_idx)\n",
        "\n",
        "num_classes = len(class_to_idx)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True,\n",
        "                          num_workers=2, pin_memory=True, collate_fn=collate_btchw)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, drop_last=False,\n",
        "                          num_workers=2, pin_memory=True, collate_fn=collate_btchw)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, drop_last=False,\n",
        "                          num_workers=2, pin_memory=True, collate_fn=collate_btchw)\n",
        "\n",
        "print(f\"num_classes = {num_classes} | T_steps = {T_steps} | train batches = {len(train_loader)}\")\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Train / Eval + BEST checkpoint\n",
        "# =========================================================\n",
        "model = Conv3DSNN_ISI(beta=beta, num_classes=num_classes).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "start_epoch = 1\n",
        "best_val = -1.0\n",
        "\n",
        "# optional resume from LAST\n",
        "if os.path.exists(last_ckpt_path):\n",
        "    ckpt = torch.load(last_ckpt_path, map_location=device)\n",
        "    model.load_state_dict(ckpt[\"model_state\"])\n",
        "    optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n",
        "    scheduler.load_state_dict(ckpt[\"scheduler_state\"])\n",
        "    start_epoch = ckpt[\"epoch\"] + 1\n",
        "    best_val = ckpt.get(\"best_val\", -1.0)\n",
        "    print(\"Resumed from epoch\", start_epoch, \"best_val\", best_val)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "    correct, total, loss_sum = 0, 0, 0.0\n",
        "    for x, y in loader:\n",
        "        x = x.to(device, non_blocking=True)  # [B,T,3,H,W]\n",
        "        y = y.to(device, non_blocking=True)\n",
        "\n",
        "        spk_in = isi_encode_ucf_video(\n",
        "            x, T_steps=T_steps, K=K_spikes, alpha_max=alpha_max, eps=eps, agg=agg_mode\n",
        "        )  # [T,B,3,H,W]\n",
        "\n",
        "        logits_rec = model(spk_in)           # [T,B,C]\n",
        "        logits = logits_rec.mean(dim=0)      # [B,C]\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "\n",
        "        pred = logits.argmax(dim=1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.numel()\n",
        "        loss_sum += loss.item() * y.size(0)\n",
        "\n",
        "    return loss_sum / max(total, 1), correct / max(total, 1)\n",
        "\n",
        "for epoch in range(start_epoch, num_epochs + 1):\n",
        "    model.train()\n",
        "    running_loss, running_correct, running_total = 0.0, 0, 0\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        y = y.to(device, non_blocking=True)\n",
        "\n",
        "        spk_in = isi_encode_ucf_video(\n",
        "            x, T_steps=T_steps, K=K_spikes, alpha_max=alpha_max, eps=eps, agg=agg_mode\n",
        "        )\n",
        "\n",
        "        logits_rec = model(spk_in)\n",
        "        logits = logits_rec.mean(dim=0)\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * y.size(0)\n",
        "        running_correct += (logits.argmax(dim=1) == y).sum().item()\n",
        "        running_total += y.size(0)\n",
        "\n",
        "    scheduler.step()\n",
        "    val_loss, val_acc = evaluate(val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | \"\n",
        "          f\"train loss {running_loss/running_total:.4f} | \"\n",
        "          f\"train acc {running_correct/running_total:.4f} | \"\n",
        "          f\"val loss {val_loss:.4f} | \"\n",
        "          f\"val acc {val_acc:.4f}\")\n",
        "\n",
        "    # -------- save LAST --------\n",
        "    torch.save({\n",
        "        \"epoch\": epoch,\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"optimizer_state\": optimizer.state_dict(),\n",
        "        \"scheduler_state\": scheduler.state_dict(),\n",
        "        \"best_val\": best_val,\n",
        "        \"class_to_idx\": class_to_idx,\n",
        "        \"T_steps\": T_steps,\n",
        "        \"K_spikes\": K_spikes,\n",
        "        \"alpha_max\": alpha_max,\n",
        "        \"agg_mode\": agg_mode,\n",
        "    }, last_ckpt_path)\n",
        "\n",
        "    # -------- save BEST (by val acc) --------\n",
        "    if val_acc > best_val:\n",
        "        best_val = val_acc\n",
        "        torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"model_state\": model.state_dict(),\n",
        "            \"optimizer_state\": optimizer.state_dict(),\n",
        "            \"scheduler_state\": scheduler.state_dict(),\n",
        "            \"best_val\": best_val,\n",
        "            \"class_to_idx\": class_to_idx,\n",
        "            \"T_steps\": T_steps,\n",
        "            \"K_spikes\": K_spikes,\n",
        "            \"alpha_max\": alpha_max,\n",
        "            \"agg_mode\": agg_mode,\n",
        "        }, best_ckpt_path)\n",
        "        print(f\"Saved BEST: epoch {epoch} | best_val={best_val:.4f} -> {best_ckpt_path}\")\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Test using BEST checkpoint (important)\n",
        "# =========================================================\n",
        "print(\"\\nLoading BEST checkpoint for test...\")\n",
        "best = torch.load(best_ckpt_path, map_location=device)\n",
        "model.load_state_dict(best[\"model_state\"])\n",
        "\n",
        "test_loss, test_acc = evaluate(test_loader)\n",
        "print(f\"BEST epoch {best['epoch']} | best_val {best['best_val']:.4f} | Test loss {test_loss:.4f} | Test acc {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3JHfCxjUWGN",
        "outputId": "92bbfa92-25f5-4387-90b4-eeab304e02b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_classes = 10 | T_steps = 16 | train batches = 37\n",
            "Epoch 01 | train loss 2.2887 | train acc 0.0946 | val loss 2.1836 | val acc 0.2200\n",
            "Saved BEST: epoch 1 | best_val=0.2200 -> ./ucf_subset_isi_best.pth\n",
            "Epoch 02 | train loss 2.1531 | train acc 0.1689 | val loss 2.0812 | val acc 0.1800\n",
            "Epoch 03 | train loss 2.1422 | train acc 0.1824 | val loss 1.9947 | val acc 0.2400\n",
            "Saved BEST: epoch 3 | best_val=0.2400 -> ./ucf_subset_isi_best.pth\n",
            "Epoch 04 | train loss 2.0531 | train acc 0.2061 | val loss 1.9709 | val acc 0.2300\n",
            "Epoch 05 | train loss 2.0429 | train acc 0.2095 | val loss 2.0509 | val acc 0.1700\n",
            "Epoch 06 | train loss 2.0075 | train acc 0.2264 | val loss 1.8715 | val acc 0.2800\n",
            "Saved BEST: epoch 6 | best_val=0.2800 -> ./ucf_subset_isi_best.pth\n",
            "Epoch 07 | train loss 1.9223 | train acc 0.2635 | val loss 1.9380 | val acc 0.2200\n",
            "Epoch 08 | train loss 1.9170 | train acc 0.2703 | val loss 2.3416 | val acc 0.1700\n",
            "Epoch 09 | train loss 1.9060 | train acc 0.2601 | val loss 2.3667 | val acc 0.1800\n",
            "Epoch 10 | train loss 1.9384 | train acc 0.3007 | val loss 1.9281 | val acc 0.2400\n",
            "Epoch 11 | train loss 1.8165 | train acc 0.2872 | val loss 1.8444 | val acc 0.2500\n",
            "Epoch 12 | train loss 1.7795 | train acc 0.3209 | val loss 1.7839 | val acc 0.3700\n",
            "Saved BEST: epoch 12 | best_val=0.3700 -> ./ucf_subset_isi_best.pth\n",
            "Epoch 13 | train loss 1.8003 | train acc 0.3176 | val loss 2.3572 | val acc 0.1900\n",
            "Epoch 14 | train loss 1.7789 | train acc 0.3311 | val loss 2.3672 | val acc 0.1900\n",
            "Epoch 15 | train loss 1.8258 | train acc 0.3345 | val loss 1.8042 | val acc 0.3400\n",
            "Epoch 16 | train loss 1.7418 | train acc 0.3716 | val loss 1.6883 | val acc 0.3500\n",
            "Epoch 17 | train loss 1.7787 | train acc 0.3514 | val loss 1.6292 | val acc 0.3900\n",
            "Saved BEST: epoch 17 | best_val=0.3900 -> ./ucf_subset_isi_best.pth\n",
            "Epoch 18 | train loss 1.6807 | train acc 0.3851 | val loss 1.9824 | val acc 0.3000\n",
            "Epoch 19 | train loss 1.5916 | train acc 0.4155 | val loss 1.5529 | val acc 0.5100\n",
            "Saved BEST: epoch 19 | best_val=0.5100 -> ./ucf_subset_isi_best.pth\n",
            "Epoch 20 | train loss 1.6368 | train acc 0.3986 | val loss 1.6876 | val acc 0.3700\n",
            "Epoch 21 | train loss 1.5685 | train acc 0.4493 | val loss 1.4151 | val acc 0.5100\n",
            "Epoch 22 | train loss 1.5233 | train acc 0.4257 | val loss 2.1745 | val acc 0.2300\n",
            "Epoch 23 | train loss 1.4976 | train acc 0.4561 | val loss 1.6962 | val acc 0.4700\n",
            "Epoch 24 | train loss 1.4742 | train acc 0.4730 | val loss 1.7726 | val acc 0.3300\n",
            "Epoch 25 | train loss 1.3867 | train acc 0.5338 | val loss 1.7134 | val acc 0.3400\n",
            "Epoch 26 | train loss 1.3851 | train acc 0.4628 | val loss 1.2435 | val acc 0.6300\n",
            "Saved BEST: epoch 26 | best_val=0.6300 -> ./ucf_subset_isi_best.pth\n",
            "Epoch 27 | train loss 1.3812 | train acc 0.5034 | val loss 1.2482 | val acc 0.5800\n",
            "Epoch 28 | train loss 1.2911 | train acc 0.5304 | val loss 1.2664 | val acc 0.5500\n",
            "Epoch 29 | train loss 1.3088 | train acc 0.5203 | val loss 1.2984 | val acc 0.6000\n",
            "Epoch 30 | train loss 1.1903 | train acc 0.5777 | val loss 1.3362 | val acc 0.5100\n",
            "Epoch 31 | train loss 1.2722 | train acc 0.5270 | val loss 1.1630 | val acc 0.6300\n",
            "Epoch 32 | train loss 1.1457 | train acc 0.5878 | val loss 2.1499 | val acc 0.3600\n",
            "Epoch 33 | train loss 1.1744 | train acc 0.5642 | val loss 1.3903 | val acc 0.5400\n",
            "Epoch 34 | train loss 1.1228 | train acc 0.5608 | val loss 1.4328 | val acc 0.4800\n",
            "Epoch 35 | train loss 1.1547 | train acc 0.5777 | val loss 1.2866 | val acc 0.4800\n",
            "Epoch 36 | train loss 1.1599 | train acc 0.5777 | val loss 1.3589 | val acc 0.5300\n",
            "Epoch 37 | train loss 1.0859 | train acc 0.6014 | val loss 1.5869 | val acc 0.4300\n",
            "Epoch 38 | train loss 1.2016 | train acc 0.5439 | val loss 2.1616 | val acc 0.2800\n",
            "Epoch 39 | train loss 1.1511 | train acc 0.6115 | val loss 1.4975 | val acc 0.5200\n",
            "Epoch 40 | train loss 1.0529 | train acc 0.6385 | val loss 1.1252 | val acc 0.6400\n",
            "Saved BEST: epoch 40 | best_val=0.6400 -> ./ucf_subset_isi_best.pth\n",
            "Epoch 41 | train loss 1.0370 | train acc 0.6453 | val loss 1.0599 | val acc 0.6300\n",
            "Epoch 42 | train loss 0.9782 | train acc 0.6723 | val loss 1.2380 | val acc 0.5300\n",
            "Epoch 43 | train loss 0.9839 | train acc 0.6419 | val loss 1.2566 | val acc 0.5400\n",
            "Epoch 44 | train loss 0.9819 | train acc 0.6385 | val loss 1.1123 | val acc 0.6300\n",
            "Epoch 45 | train loss 0.9259 | train acc 0.6689 | val loss 1.1794 | val acc 0.5700\n",
            "Epoch 46 | train loss 0.8770 | train acc 0.7027 | val loss 1.1327 | val acc 0.5900\n",
            "Epoch 47 | train loss 0.8448 | train acc 0.6993 | val loss 1.1458 | val acc 0.6200\n",
            "Epoch 48 | train loss 0.9261 | train acc 0.6419 | val loss 1.1170 | val acc 0.6700\n",
            "Saved BEST: epoch 48 | best_val=0.6700 -> ./ucf_subset_isi_best.pth\n",
            "Epoch 49 | train loss 0.8956 | train acc 0.6824 | val loss 1.4101 | val acc 0.5600\n",
            "Epoch 50 | train loss 0.9167 | train acc 0.6824 | val loss 1.1086 | val acc 0.6300\n",
            "Epoch 51 | train loss 0.8220 | train acc 0.6892 | val loss 1.1348 | val acc 0.6000\n",
            "Epoch 52 | train loss 0.8861 | train acc 0.6689 | val loss 1.1270 | val acc 0.6200\n",
            "Epoch 53 | train loss 0.8244 | train acc 0.6892 | val loss 1.0112 | val acc 0.6100\n",
            "Epoch 54 | train loss 0.7478 | train acc 0.7601 | val loss 0.9716 | val acc 0.6800\n",
            "Saved BEST: epoch 54 | best_val=0.6800 -> ./ucf_subset_isi_best.pth\n",
            "Epoch 55 | train loss 0.8359 | train acc 0.7196 | val loss 0.9849 | val acc 0.6200\n",
            "Epoch 56 | train loss 0.7529 | train acc 0.7365 | val loss 1.0270 | val acc 0.6500\n",
            "Epoch 57 | train loss 0.7764 | train acc 0.7264 | val loss 1.0125 | val acc 0.6600\n",
            "Epoch 58 | train loss 0.7716 | train acc 0.7162 | val loss 0.9540 | val acc 0.6500\n",
            "Epoch 59 | train loss 0.7426 | train acc 0.7365 | val loss 0.9133 | val acc 0.6900\n",
            "Saved BEST: epoch 59 | best_val=0.6900 -> ./ucf_subset_isi_best.pth\n",
            "Epoch 60 | train loss 0.7651 | train acc 0.7297 | val loss 1.0708 | val acc 0.6200\n",
            "Epoch 61 | train loss 0.7659 | train acc 0.7500 | val loss 1.0311 | val acc 0.6300\n",
            "Epoch 62 | train loss 0.6921 | train acc 0.7939 | val loss 1.1197 | val acc 0.6100\n",
            "Epoch 63 | train loss 0.7684 | train acc 0.7500 | val loss 0.9438 | val acc 0.6500\n",
            "Epoch 64 | train loss 0.7242 | train acc 0.7162 | val loss 1.0244 | val acc 0.6200\n",
            "Epoch 65 | train loss 0.8070 | train acc 0.7500 | val loss 0.9478 | val acc 0.6600\n",
            "Epoch 66 | train loss 0.7231 | train acc 0.7466 | val loss 0.9416 | val acc 0.6300\n",
            "Epoch 67 | train loss 0.7163 | train acc 0.7804 | val loss 0.9520 | val acc 0.6600\n",
            "Epoch 68 | train loss 0.6344 | train acc 0.8074 | val loss 0.9349 | val acc 0.6400\n",
            "Epoch 69 | train loss 0.7170 | train acc 0.7399 | val loss 0.9631 | val acc 0.6200\n",
            "Epoch 70 | train loss 0.6699 | train acc 0.7770 | val loss 0.9336 | val acc 0.6300\n",
            "Epoch 71 | train loss 0.7141 | train acc 0.7736 | val loss 0.9510 | val acc 0.6300\n",
            "Epoch 72 | train loss 0.6853 | train acc 0.7635 | val loss 0.9386 | val acc 0.6500\n",
            "Epoch 73 | train loss 0.7347 | train acc 0.7804 | val loss 0.9496 | val acc 0.6400\n",
            "Epoch 74 | train loss 0.6488 | train acc 0.7905 | val loss 0.9203 | val acc 0.6600\n",
            "Epoch 75 | train loss 0.6983 | train acc 0.7635 | val loss 0.9252 | val acc 0.6500\n",
            "Epoch 76 | train loss 0.6975 | train acc 0.7365 | val loss 0.9338 | val acc 0.6300\n",
            "Epoch 77 | train loss 0.6538 | train acc 0.8176 | val loss 0.9270 | val acc 0.6400\n",
            "Epoch 78 | train loss 0.6407 | train acc 0.8176 | val loss 0.9541 | val acc 0.6300\n",
            "Epoch 79 | train loss 0.6681 | train acc 0.7770 | val loss 0.9567 | val acc 0.6500\n",
            "Epoch 80 | train loss 0.6462 | train acc 0.7770 | val loss 0.9200 | val acc 0.6300\n",
            "\n",
            "Loading BEST checkpoint for test...\n",
            "BEST epoch 59 | best_val 0.6900 | Test loss 0.8892 | Test acc 0.7000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. TTFS-Phase"
      ],
      "metadata": {
        "id": "mh7uvSMbbNUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "import pathlib\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import snntorch as snn\n",
        "\n",
        "for p in [\"./ucf_subset_ttfsphase_videoK4_last.pth\",\n",
        "          \"./ucf_subset_ttfsphase_videoK4_best.pth\"]:\n",
        "    if os.path.exists(p):\n",
        "        os.remove(p)\n",
        "        print(\"Removed:\", p)\n",
        "# =========================================================\n",
        "# Config\n",
        "# =========================================================\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "batch_size = 8\n",
        "num_epochs = 80\n",
        "lr = 2e-3\n",
        "\n",
        "# clip sampling\n",
        "T_FRAMES   = 16       # frames_per_clip\n",
        "frame_step = 2\n",
        "H, W       = 112, 112\n",
        "\n",
        "# encoder time axis\n",
        "T_STEPS = 60\n",
        "P = 3\n",
        "phi0 = 0\n",
        "assert 0 <= phi0 < P\n",
        "M = int(((T_STEPS - 1 - phi0) // P) + 1)  # maxima bins\n",
        "\n",
        "# \"video-aware\" spike budget\n",
        "K_SPIKES = 4  # <=4 spikes per pixel (one per frame-group)\n",
        "\n",
        "# ranking jitter\n",
        "JITTER = 1e-4   # for stability on many ties\n",
        "\n",
        "# LIF\n",
        "tau = 2.0\n",
        "beta = torch.exp(torch.tensor(-1.0 / tau)).item()\n",
        "\n",
        "# ckpt\n",
        "last_ckpt_path = \"./ucf_subset_ttfsphase_videoK4_last.pth\"\n",
        "best_ckpt_path = \"./ucf_subset_ttfsphase_videoK4_best.pth\"\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Video utils\n",
        "# =========================================================\n",
        "def resize_with_pad(frame_bgr, out_hw=(112,112)):\n",
        "    out_h, out_w = out_hw\n",
        "    h, w = frame_bgr.shape[:2]\n",
        "    if h == 0 or w == 0:\n",
        "        return np.zeros((out_h, out_w, 3), dtype=np.uint8)\n",
        "\n",
        "    scale = min(out_w / w, out_h / h)\n",
        "    nw, nh = int(w * scale), int(h * scale)\n",
        "    resized = cv2.resize(frame_bgr, (nw, nh), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    canvas = np.zeros((out_h, out_w, 3), dtype=np.uint8)\n",
        "    top = (out_h - nh) // 2\n",
        "    left = (out_w - nw) // 2\n",
        "    canvas[top:top+nh, left:left+nw] = resized\n",
        "    return canvas\n",
        "\n",
        "def frames_from_video(video_path, n_frames, out_hw=(112,112), frame_step=2, training=True):\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    if not cap.isOpened():\n",
        "        return np.zeros((n_frames, out_hw[0], out_hw[1], 3), dtype=np.uint8)\n",
        "\n",
        "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    need_len = 1 + (n_frames - 1) * frame_step\n",
        "    if length <= 0 or need_len > length:\n",
        "        start = 0\n",
        "    else:\n",
        "        max_start = length - need_len\n",
        "        start = random.randint(0, max_start) if training else 0\n",
        "\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
        "\n",
        "    frames = []\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        cap.release()\n",
        "        return np.zeros((n_frames, out_hw[0], out_hw[1], 3), dtype=np.uint8)\n",
        "\n",
        "    frames.append(resize_with_pad(frame, out_hw))\n",
        "    for _ in range(n_frames - 1):\n",
        "        for _ in range(frame_step):\n",
        "            ret, frame = cap.read()\n",
        "        if ret:\n",
        "            frames.append(resize_with_pad(frame, out_hw))\n",
        "        else:\n",
        "            frames.append(np.zeros_like(frames[0]))\n",
        "\n",
        "    cap.release()\n",
        "    frames = np.stack(frames, axis=0)[..., ::-1].copy()  # BGR->RGB contiguous\n",
        "    return frames  # [T,H,W,3] uint8\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Dataset\n",
        "# =========================================================\n",
        "IM_MEAN = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
        "IM_STD  = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
        "\n",
        "class UCFSubsetDataset(Dataset):\n",
        "    def __init__(self, split_dir, n_frames=16, frame_step=2, training=False, class_to_idx=None):\n",
        "        self.split_dir = pathlib.Path(split_dir)\n",
        "        self.n_frames = n_frames\n",
        "        self.frame_step = frame_step\n",
        "        self.training = training\n",
        "\n",
        "        self.video_paths = sorted(self.split_dir.glob(\"*/*.avi\"))\n",
        "        self.class_names = sorted({p.parent.name for p in self.video_paths})\n",
        "\n",
        "        if class_to_idx is None:\n",
        "            self.class_to_idx = {c:i for i,c in enumerate(self.class_names)}\n",
        "        else:\n",
        "            self.class_to_idx = class_to_idx\n",
        "\n",
        "        self.labels = [self.class_to_idx[p.parent.name] for p in self.video_paths]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        vp = self.video_paths[idx]\n",
        "        y = self.labels[idx]\n",
        "\n",
        "        frames = frames_from_video(vp, self.n_frames, (H,W), self.frame_step, self.training)\n",
        "        x = torch.from_numpy(frames).permute(0,3,1,2).float() / 255.0  # [Tf,3,H,W]\n",
        "        x = (x - IM_MEAN) / IM_STD\n",
        "        return x, torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "def collate_btchw(batch):\n",
        "    xs, ys = zip(*batch)\n",
        "    x = torch.stack(xs, dim=0)  # [B,Tf,3,H,W]\n",
        "    y = torch.stack(ys, dim=0)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Helper: normalized -> unit [0,1]\n",
        "# =========================================================\n",
        "@torch.no_grad()\n",
        "def video_to_unit_interval(x_video_norm: torch.Tensor) -> torch.Tensor:\n",
        "    mean = IM_MEAN.to(x_video_norm.device)\n",
        "    std  = IM_STD.to(x_video_norm.device)\n",
        "    return (x_video_norm * std + mean).clamp(0.0, 1.0)\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Video-aware TTFS-Phase (Method B), K=4 spikes/pixel EXACT\n",
        "# Group-reduce scores within each frame-group, then rank-balance once per group\n",
        "# =========================================================\n",
        "@torch.no_grad()\n",
        "def ttfs_phase_rank_balance_per_frame_k4(\n",
        "    x_video_norm: torch.Tensor,  # [B,Tf,3,H,W] normalized\n",
        "    T: int,\n",
        "    P: int,\n",
        "    phi0: int,\n",
        "    K: int = 4,\n",
        "    jitter: float = 1e-4,\n",
        "    group_reduce: str = \"mean\",  # \"mean\" or \"max\"\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Video-aware TTFS-Phase (Method B), spike budget K=4:\n",
        "      - split frames into K groups (Tf must be divisible by K)\n",
        "      - within each group, reduce scores across frames (mean/max) -> one score per pixel\n",
        "      - do rank-balance ONCE per group -> 1 phase-locked spike per pixel per group\n",
        "\n",
        "    Guarantees:\n",
        "      - STRICT phase-lock: t = phi0 + k*P\n",
        "      - EXACT K spikes per pixel total (one per group), binary spikes\n",
        "    Returns:\n",
        "      spk: [T, B, 3, H, W]\n",
        "    \"\"\"\n",
        "    assert K == 4, \"this helper is specialized for K=4 (easy to generalize later)\"\n",
        "    assert 0 <= phi0 < P\n",
        "    device = x_video_norm.device\n",
        "    B, Tf, C, H_, W_ = x_video_norm.shape\n",
        "    N = C * H_ * W_\n",
        "\n",
        "    # number of maxima bins\n",
        "    M = int(((T - 1 - phi0) // P) + 1)\n",
        "\n",
        "    # map normalized -> [0,1]\n",
        "    x_unit = video_to_unit_interval(x_video_norm)    # [B,Tf,3,H,W]\n",
        "    x_flat = x_unit.reshape(B, Tf, -1)               # [B,Tf,N]\n",
        "\n",
        "    group_size = Tf // K\n",
        "    assert Tf % K == 0, \"Tf must be divisible by K.\"\n",
        "\n",
        "    spk = torch.zeros(T, B, N, device=device, dtype=torch.float32)\n",
        "\n",
        "    b_idx = torch.arange(B, device=device).view(B, 1).expand(B, N)\n",
        "    n_idx = torch.arange(N, device=device).view(1, N).expand(B, N)\n",
        "\n",
        "    for g in range(K):\n",
        "        f0 = g * group_size\n",
        "        f1 = (g + 1) * group_size\n",
        "\n",
        "        # group score aggregation: [B,N]\n",
        "        if group_reduce == \"mean\":\n",
        "            score = x_flat[:, f0:f1, :].mean(dim=1)\n",
        "        elif group_reduce == \"max\":\n",
        "            score = x_flat[:, f0:f1, :].max(dim=1).values\n",
        "        else:\n",
        "            raise ValueError(\"group_reduce must be 'mean' or 'max'\")\n",
        "\n",
        "        # break ties for stable ranks\n",
        "        if jitter and jitter > 0:\n",
        "            score = score + jitter * torch.randn_like(score)\n",
        "\n",
        "        # rank: 0..N-1 (0=brightest)\n",
        "        order = torch.argsort(score, dim=1, descending=True)  # [B,N]\n",
        "        inv_rank = torch.empty_like(order)\n",
        "        inv_rank.scatter_(1, order, torch.arange(N, device=device).view(1, N).expand(B, N))\n",
        "\n",
        "        # rank -> maxima bin k in [0..M-1] (uniform occupancy)\n",
        "        kbin = torch.floor(inv_rank.float() * M / float(N)).long().clamp(0, M - 1)  # [B,N]\n",
        "\n",
        "        # phase-lock time\n",
        "        t = (phi0 + kbin * P).long()  # [B,N]\n",
        "\n",
        "        # exactly one spike per pixel for this group\n",
        "        spk[t, b_idx, n_idx] = 1.0\n",
        "\n",
        "    return spk.view(T, B, C, H_, W_)\n",
        "\n",
        "# =========================================================\n",
        "# Conv3D SNN (analog logits head)\n",
        "# =========================================================\n",
        "class Conv3DSNN_TTFSPhase(nn.Module):\n",
        "    def __init__(self, beta=0.95, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv3d(3, 32, 3, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm3d(32)\n",
        "        self.lif1 = snn.Leaky(beta=beta)\n",
        "\n",
        "        self.conv2 = nn.Conv3d(32, 64, 3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm3d(64)\n",
        "        self.lif2 = snn.Leaky(beta=beta)\n",
        "\n",
        "        self.conv3 = nn.Conv3d(64, 128, 3, padding=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm3d(128)\n",
        "        self.lif3 = snn.Leaky(beta=beta)\n",
        "\n",
        "        self.pool = nn.MaxPool3d((1,2,2))\n",
        "        self.gap = nn.AdaptiveAvgPool3d((None,1,1))\n",
        "\n",
        "        self.fc1 = nn.Linear(128, 256)\n",
        "        self.lif4 = snn.Leaky(beta=beta)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def _lif_over_time_2d(self, x_bcthw, lif, mem):\n",
        "        B,C,T,H,W = x_bcthw.shape\n",
        "        spk_list=[]\n",
        "        for t in range(T):\n",
        "            spk_t, mem = lif(x_bcthw[:, :, t], mem)\n",
        "            spk_list.append(spk_t)\n",
        "        return torch.stack(spk_list, dim=2), mem\n",
        "\n",
        "    def forward(self, spk_in):\n",
        "        # spk_in: [T,B,3,H,W]\n",
        "        x = spk_in.permute(1,2,0,3,4).contiguous()  # [B,3,T,H,W]\n",
        "\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        mem2 = self.lif2.init_leaky()\n",
        "        mem3 = self.lif3.init_leaky()\n",
        "        mem4 = self.lif4.init_leaky()\n",
        "\n",
        "        x = self.conv1(x); x = self.bn1(x); x, mem1 = self._lif_over_time_2d(x, self.lif1, mem1); x = self.pool(x)\n",
        "        x = self.conv2(x); x = self.bn2(x); x, mem2 = self._lif_over_time_2d(x, self.lif2, mem2); x = self.pool(x)\n",
        "        x = self.conv3(x); x = self.bn3(x); x, mem3 = self._lif_over_time_2d(x, self.lif3, mem3); x = self.pool(x)\n",
        "\n",
        "        x = self.gap(x).squeeze(-1).squeeze(-1)  # [B,128,T]\n",
        "\n",
        "        logits_list=[]\n",
        "        for t in range(x.shape[2]):\n",
        "            h = self.fc1(x[:,:,t])\n",
        "            spk4, mem4 = self.lif4(h, mem4)\n",
        "            logits_t = self.fc2(spk4)\n",
        "            logits_list.append(logits_t)\n",
        "\n",
        "        return torch.stack(logits_list, dim=0)  # [T,B,num_classes]\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Build loaders from subset_paths (must exist)\n",
        "# =========================================================\n",
        "train_ds = UCFSubsetDataset(subset_paths[\"train\"], n_frames=T_FRAMES, frame_step=frame_step, training=True)\n",
        "class_to_idx = train_ds.class_to_idx\n",
        "val_ds   = UCFSubsetDataset(subset_paths[\"val\"],   n_frames=T_FRAMES, frame_step=frame_step, training=False, class_to_idx=class_to_idx)\n",
        "test_ds  = UCFSubsetDataset(subset_paths[\"test\"],  n_frames=T_FRAMES, frame_step=frame_step, training=False, class_to_idx=class_to_idx)\n",
        "\n",
        "num_classes = len(class_to_idx)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=2, pin_memory=True, collate_fn=collate_btchw)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, drop_last=False, num_workers=2, pin_memory=True, collate_fn=collate_btchw)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, drop_last=False, num_workers=2, pin_memory=True, collate_fn=collate_btchw)\n",
        "\n",
        "print(f\"num_classes={num_classes} | clip_frames={T_FRAMES} | encoder_T={T_STEPS} | P={P} | M={M} | K={K_SPIKES} (<=K per pixel)\")\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Model + Optim + Resume + BEST\n",
        "# =========================================================\n",
        "model = Conv3DSNN_TTFSPhase(beta=beta, num_classes=num_classes).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "start_epoch = 1\n",
        "best_val = -1.0\n",
        "\n",
        "if os.path.exists(last_ckpt_path):\n",
        "    ckpt = torch.load(last_ckpt_path, map_location=device)\n",
        "    model.load_state_dict(ckpt[\"model_state\"])\n",
        "    optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n",
        "    scheduler.load_state_dict(ckpt[\"scheduler_state\"])\n",
        "    start_epoch = ckpt[\"epoch\"] + 1\n",
        "    best_val = ckpt.get(\"best_val\", -1.0)\n",
        "    print(\"Resumed from epoch\", start_epoch, \"best_val\", best_val)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "    correct, total, loss_sum = 0, 0, 0.0\n",
        "    for x, y in loader:\n",
        "        x = x.to(device, non_blocking=True)  # [B,Tf,3,H,W]\n",
        "        y = y.to(device, non_blocking=True)\n",
        "\n",
        "        spk_in = ttfs_phase_rank_balance_per_frame_k4(\n",
        "        x, T=T_STEPS, P=P, phi0=phi0, K=K_SPIKES, jitter=JITTER, group_reduce=\"max\"\n",
        "        )  # [T,B,3,H,W]\n",
        "\n",
        "        logits_rec = model(spk_in)      # [T,B,C]\n",
        "        logits = logits_rec.mean(0)     # [B,C]\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "\n",
        "        pred = logits.argmax(1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.numel()\n",
        "        loss_sum += loss.item() * y.size(0)\n",
        "\n",
        "    return loss_sum / max(total, 1), correct / max(total, 1)\n",
        "\n",
        "for epoch in range(start_epoch, num_epochs + 1):\n",
        "    model.train()\n",
        "    running_loss, running_correct, running_total = 0.0, 0, 0\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        y = y.to(device, non_blocking=True)\n",
        "\n",
        "        spk_in = ttfs_phase_rank_balance_per_frame_k4(\n",
        "            x, T=T_STEPS, P=P, phi0=phi0, K=K_SPIKES, jitter=JITTER\n",
        "        )\n",
        "\n",
        "        logits_rec = model(spk_in)\n",
        "        logits = logits_rec.mean(0)\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * y.size(0)\n",
        "        running_correct += (logits.argmax(1) == y).sum().item()\n",
        "        running_total += y.size(0)\n",
        "\n",
        "    scheduler.step()\n",
        "    val_loss, val_acc = evaluate(val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | \"\n",
        "          f\"train loss {running_loss/running_total:.4f} | \"\n",
        "          f\"train acc {running_correct/running_total:.4f} | \"\n",
        "          f\"val loss {val_loss:.4f} | \"\n",
        "          f\"val acc {val_acc:.4f}\")\n",
        "\n",
        "    # save LAST\n",
        "    torch.save({\n",
        "        \"epoch\": epoch,\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"optimizer_state\": optimizer.state_dict(),\n",
        "        \"scheduler_state\": scheduler.state_dict(),\n",
        "        \"best_val\": best_val,\n",
        "        \"class_to_idx\": class_to_idx,\n",
        "        \"T_STEPS\": T_STEPS, \"P\": P, \"phi0\": phi0,\n",
        "        \"K_SPIKES\": K_SPIKES, \"JITTER\": JITTER,\n",
        "    }, last_ckpt_path)\n",
        "\n",
        "    # save BEST\n",
        "    if val_acc > best_val:\n",
        "        best_val = val_acc\n",
        "        torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"model_state\": model.state_dict(),\n",
        "            \"optimizer_state\": optimizer.state_dict(),\n",
        "            \"scheduler_state\": scheduler.state_dict(),\n",
        "            \"best_val\": best_val,\n",
        "            \"class_to_idx\": class_to_idx,\n",
        "            \"T_STEPS\": T_STEPS, \"P\": P, \"phi0\": phi0,\n",
        "            \"K_SPIKES\": K_SPIKES, \"JITTER\": JITTER,\n",
        "        }, best_ckpt_path)\n",
        "        print(f\"Saved BEST: epoch {epoch} | best_val={best_val:.4f} -> {best_ckpt_path}\")\n",
        "\n",
        "print(\"\\nLoading BEST checkpoint for test...\")\n",
        "best = torch.load(best_ckpt_path, map_location=device)\n",
        "model.load_state_dict(best[\"model_state\"])\n",
        "test_loss, test_acc = evaluate(test_loader)\n",
        "print(f\"BEST epoch {best['epoch']} | best_val {best['best_val']:.4f} | Test loss {test_loss:.4f} | Test acc {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3NfvyA8bPD-",
        "outputId": "0321b460-cb2f-400d-8fe1-c7dd5076f4aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed: ./ucf_subset_ttfsphase_videoK4_last.pth\n",
            "Removed: ./ucf_subset_ttfsphase_videoK4_best.pth\n",
            "num_classes=10 | clip_frames=16 | encoder_T=60 | P=3 | M=20 | K=4 (<=K per pixel)\n",
            "Epoch 01 | train loss 2.2898 | train acc 0.1351 | val loss 2.2557 | val acc 0.1500\n",
            "Saved BEST: epoch 1 | best_val=0.1500 -> ./ucf_subset_ttfsphase_videoK4_best.pth\n",
            "Epoch 02 | train loss 2.2010 | train acc 0.1588 | val loss 2.1934 | val acc 0.1200\n",
            "Epoch 03 | train loss 2.0534 | train acc 0.2128 | val loss 2.0252 | val acc 0.2000\n",
            "Saved BEST: epoch 3 | best_val=0.2000 -> ./ucf_subset_ttfsphase_videoK4_best.pth\n",
            "Epoch 04 | train loss 2.0037 | train acc 0.1926 | val loss 2.6319 | val acc 0.1100\n",
            "Epoch 05 | train loss 1.9880 | train acc 0.1993 | val loss 1.9005 | val acc 0.3000\n",
            "Saved BEST: epoch 5 | best_val=0.3000 -> ./ucf_subset_ttfsphase_videoK4_best.pth\n",
            "Epoch 06 | train loss 1.8912 | train acc 0.2365 | val loss 1.8604 | val acc 0.3400\n",
            "Saved BEST: epoch 6 | best_val=0.3400 -> ./ucf_subset_ttfsphase_videoK4_best.pth\n",
            "Epoch 07 | train loss 1.9170 | train acc 0.2128 | val loss 2.0508 | val acc 0.2100\n",
            "Epoch 08 | train loss 1.8253 | train acc 0.2804 | val loss 1.8468 | val acc 0.2800\n",
            "Epoch 09 | train loss 1.7879 | train acc 0.2872 | val loss 1.7642 | val acc 0.3300\n",
            "Epoch 10 | train loss 1.8216 | train acc 0.2703 | val loss 1.7166 | val acc 0.4000\n",
            "Saved BEST: epoch 10 | best_val=0.4000 -> ./ucf_subset_ttfsphase_videoK4_best.pth\n",
            "Epoch 11 | train loss 1.7512 | train acc 0.2534 | val loss 1.9564 | val acc 0.3000\n",
            "Epoch 12 | train loss 1.7736 | train acc 0.2804 | val loss 3.0811 | val acc 0.1100\n",
            "Epoch 13 | train loss 1.7238 | train acc 0.3209 | val loss 3.9482 | val acc 0.1700\n",
            "Epoch 14 | train loss 1.6926 | train acc 0.3446 | val loss 2.1879 | val acc 0.1800\n",
            "Epoch 15 | train loss 1.6869 | train acc 0.3378 | val loss 2.0494 | val acc 0.2000\n",
            "Epoch 16 | train loss 1.6185 | train acc 0.3480 | val loss 3.2257 | val acc 0.1200\n",
            "Epoch 17 | train loss 1.5921 | train acc 0.3919 | val loss 1.6820 | val acc 0.4200\n",
            "Saved BEST: epoch 17 | best_val=0.4200 -> ./ucf_subset_ttfsphase_videoK4_best.pth\n",
            "Epoch 18 | train loss 1.6070 | train acc 0.3716 | val loss 4.2801 | val acc 0.1000\n",
            "Epoch 19 | train loss 1.5360 | train acc 0.3784 | val loss 1.7162 | val acc 0.3500\n",
            "Epoch 20 | train loss 1.5272 | train acc 0.3986 | val loss 2.6879 | val acc 0.2000\n",
            "Epoch 21 | train loss 1.5181 | train acc 0.4088 | val loss 1.8410 | val acc 0.3300\n",
            "Epoch 22 | train loss 1.5065 | train acc 0.4155 | val loss 3.0249 | val acc 0.1100\n",
            "Epoch 23 | train loss 1.5031 | train acc 0.3919 | val loss 3.0745 | val acc 0.1300\n",
            "Epoch 24 | train loss 1.5017 | train acc 0.4020 | val loss 2.6132 | val acc 0.2200\n",
            "Epoch 25 | train loss 1.4865 | train acc 0.4392 | val loss 1.7911 | val acc 0.3500\n",
            "Epoch 26 | train loss 1.4196 | train acc 0.4561 | val loss 1.6788 | val acc 0.4200\n",
            "Epoch 27 | train loss 1.4678 | train acc 0.4223 | val loss 1.8081 | val acc 0.3200\n",
            "Epoch 28 | train loss 1.4614 | train acc 0.4189 | val loss 1.4910 | val acc 0.4700\n",
            "Saved BEST: epoch 28 | best_val=0.4700 -> ./ucf_subset_ttfsphase_videoK4_best.pth\n",
            "Epoch 29 | train loss 1.4112 | train acc 0.4662 | val loss 1.4981 | val acc 0.5200\n",
            "Saved BEST: epoch 29 | best_val=0.5200 -> ./ucf_subset_ttfsphase_videoK4_best.pth\n",
            "Epoch 30 | train loss 1.4169 | train acc 0.4561 | val loss 1.7307 | val acc 0.3600\n",
            "Epoch 31 | train loss 1.4646 | train acc 0.3953 | val loss 1.9653 | val acc 0.3100\n",
            "Epoch 32 | train loss 1.3510 | train acc 0.4662 | val loss 1.5755 | val acc 0.4900\n",
            "Epoch 33 | train loss 1.4113 | train acc 0.4122 | val loss 1.7791 | val acc 0.4100\n",
            "Epoch 34 | train loss 1.3252 | train acc 0.4696 | val loss 1.4855 | val acc 0.5100\n",
            "Epoch 35 | train loss 1.3497 | train acc 0.4662 | val loss 2.6240 | val acc 0.1900\n",
            "Epoch 36 | train loss 1.3707 | train acc 0.4696 | val loss 1.4847 | val acc 0.4700\n",
            "Epoch 37 | train loss 1.2915 | train acc 0.5068 | val loss 1.4936 | val acc 0.5200\n",
            "Epoch 38 | train loss 1.3786 | train acc 0.4662 | val loss 1.7352 | val acc 0.3700\n",
            "Epoch 39 | train loss 1.2841 | train acc 0.5000 | val loss 1.5897 | val acc 0.4000\n",
            "Epoch 40 | train loss 1.3816 | train acc 0.5068 | val loss 2.2452 | val acc 0.2500\n",
            "Epoch 41 | train loss 1.2700 | train acc 0.4797 | val loss 1.4892 | val acc 0.5000\n",
            "Epoch 42 | train loss 1.2260 | train acc 0.5439 | val loss 1.4310 | val acc 0.5200\n",
            "Epoch 43 | train loss 1.2586 | train acc 0.4966 | val loss 2.2587 | val acc 0.2700\n",
            "Epoch 44 | train loss 1.2478 | train acc 0.5135 | val loss 1.4743 | val acc 0.5600\n",
            "Saved BEST: epoch 44 | best_val=0.5600 -> ./ucf_subset_ttfsphase_videoK4_best.pth\n",
            "Epoch 45 | train loss 1.2121 | train acc 0.5473 | val loss 1.4601 | val acc 0.5100\n",
            "Epoch 46 | train loss 1.1832 | train acc 0.5270 | val loss 2.0590 | val acc 0.3100\n",
            "Epoch 47 | train loss 1.2261 | train acc 0.5101 | val loss 1.4382 | val acc 0.5000\n",
            "Epoch 48 | train loss 1.1895 | train acc 0.5372 | val loss 1.6768 | val acc 0.4100\n",
            "Epoch 49 | train loss 1.1443 | train acc 0.5743 | val loss 2.7973 | val acc 0.2100\n",
            "Epoch 50 | train loss 1.2045 | train acc 0.5304 | val loss 1.5415 | val acc 0.4500\n",
            "Epoch 51 | train loss 1.1426 | train acc 0.5574 | val loss 1.5825 | val acc 0.4700\n",
            "Epoch 52 | train loss 1.1583 | train acc 0.5541 | val loss 1.3478 | val acc 0.5500\n",
            "Epoch 53 | train loss 1.1245 | train acc 0.5743 | val loss 1.3896 | val acc 0.5400\n",
            "Epoch 54 | train loss 1.0984 | train acc 0.5743 | val loss 1.3384 | val acc 0.5800\n",
            "Saved BEST: epoch 54 | best_val=0.5800 -> ./ucf_subset_ttfsphase_videoK4_best.pth\n",
            "Epoch 55 | train loss 1.1358 | train acc 0.5405 | val loss 1.3534 | val acc 0.5900\n",
            "Saved BEST: epoch 55 | best_val=0.5900 -> ./ucf_subset_ttfsphase_videoK4_best.pth\n",
            "Epoch 56 | train loss 1.1009 | train acc 0.5473 | val loss 1.7308 | val acc 0.4100\n",
            "Epoch 57 | train loss 1.1084 | train acc 0.5777 | val loss 1.4040 | val acc 0.5000\n",
            "Epoch 58 | train loss 1.0307 | train acc 0.6216 | val loss 1.4891 | val acc 0.4600\n",
            "Epoch 59 | train loss 1.0493 | train acc 0.6149 | val loss 1.3154 | val acc 0.5500\n",
            "Epoch 60 | train loss 1.0267 | train acc 0.6149 | val loss 1.3274 | val acc 0.5800\n",
            "Epoch 61 | train loss 1.0374 | train acc 0.6216 | val loss 1.3217 | val acc 0.5800\n",
            "Epoch 62 | train loss 1.0376 | train acc 0.5912 | val loss 1.6619 | val acc 0.3900\n",
            "Epoch 63 | train loss 1.0330 | train acc 0.6250 | val loss 1.3160 | val acc 0.5800\n",
            "Epoch 64 | train loss 1.0637 | train acc 0.6385 | val loss 1.4374 | val acc 0.5200\n",
            "Epoch 65 | train loss 1.0100 | train acc 0.6115 | val loss 2.0869 | val acc 0.3400\n",
            "Epoch 66 | train loss 0.9892 | train acc 0.6318 | val loss 1.4802 | val acc 0.5100\n",
            "Epoch 67 | train loss 0.9862 | train acc 0.6486 | val loss 1.3896 | val acc 0.4800\n",
            "Epoch 68 | train loss 0.9638 | train acc 0.6385 | val loss 1.2687 | val acc 0.6300\n",
            "Saved BEST: epoch 68 | best_val=0.6300 -> ./ucf_subset_ttfsphase_videoK4_best.pth\n",
            "Epoch 69 | train loss 0.9885 | train acc 0.6588 | val loss 1.3349 | val acc 0.5400\n",
            "Epoch 70 | train loss 0.9501 | train acc 0.6486 | val loss 1.2640 | val acc 0.6100\n",
            "Epoch 71 | train loss 0.9620 | train acc 0.6655 | val loss 1.2690 | val acc 0.6000\n",
            "Epoch 72 | train loss 1.0082 | train acc 0.6318 | val loss 1.2886 | val acc 0.5500\n",
            "Epoch 73 | train loss 0.9660 | train acc 0.6486 | val loss 1.3115 | val acc 0.5200\n",
            "Epoch 74 | train loss 0.9598 | train acc 0.6622 | val loss 1.2712 | val acc 0.5700\n",
            "Epoch 75 | train loss 0.9360 | train acc 0.6284 | val loss 1.3801 | val acc 0.5300\n",
            "Epoch 76 | train loss 0.9561 | train acc 0.6419 | val loss 1.2983 | val acc 0.5600\n",
            "Epoch 77 | train loss 0.9356 | train acc 0.6284 | val loss 1.2664 | val acc 0.5800\n",
            "Epoch 78 | train loss 0.9345 | train acc 0.6689 | val loss 1.2692 | val acc 0.5900\n",
            "Epoch 79 | train loss 0.9228 | train acc 0.6791 | val loss 1.2603 | val acc 0.5800\n",
            "Epoch 80 | train loss 0.9815 | train acc 0.6622 | val loss 1.2640 | val acc 0.5900\n",
            "\n",
            "Loading BEST checkpoint for test...\n",
            "BEST epoch 68 | best_val 0.6300 | Test loss 1.2185 | Test acc 0.6000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. ISI-Phase"
      ],
      "metadata": {
        "id": "6V1acCFO9el-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "import pathlib\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import snntorch as snn\n",
        "\n",
        "# =========================================================\n",
        "# Config\n",
        "# =========================================================\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "batch_size = 8\n",
        "num_epochs = 80\n",
        "lr = 2e-3\n",
        "\n",
        "# video sampling\n",
        "T_steps    = 16     # frames_per_clip (also encoder time steps)\n",
        "frame_step = 2\n",
        "H, W       = 112, 112\n",
        "\n",
        "# ISI-PHASE encoder (strict timegrid)\n",
        "K_spikes  = 4          # spikes per pixel/channel\n",
        "P         = 2          # phase period (grid step)\n",
        "phi0      = 0          # phase offset\n",
        "alpha_max = 2.0\n",
        "eps = 1e-3\n",
        "agg_mode = \"max\"       # \"max\" or \"mean\" collapse clip->image before ISI-Phase\n",
        "\n",
        "# sanity: number of allowed bins on grid\n",
        "M = int(((T_steps - 1 - phi0) // P) + 1)\n",
        "assert K_spikes <= M, f\"K_spikes={K_spikes} must <= M={M}. Try smaller K or smaller P.\"\n",
        "\n",
        "# LIF\n",
        "tau = 2.0\n",
        "beta = torch.exp(torch.tensor(-1.0 / tau)).item()\n",
        "\n",
        "# checkpoint paths\n",
        "last_ckpt_path = \"./ucf_subset_isiphase_last.pth\"\n",
        "best_ckpt_path = \"./ucf_subset_isiphase_best.pth\"\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Video utils\n",
        "# =========================================================\n",
        "def resize_with_pad(frame_bgr, out_hw=(112,112)):\n",
        "    out_h, out_w = out_hw\n",
        "    h, w = frame_bgr.shape[:2]\n",
        "    if h == 0 or w == 0:\n",
        "        return np.zeros((out_h, out_w, 3), dtype=np.uint8)\n",
        "\n",
        "    scale = min(out_w / w, out_h / h)\n",
        "    nw, nh = int(w * scale), int(h * scale)\n",
        "    resized = cv2.resize(frame_bgr, (nw, nh), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    canvas = np.zeros((out_h, out_w, 3), dtype=np.uint8)\n",
        "    top = (out_h - nh) // 2\n",
        "    left = (out_w - nw) // 2\n",
        "    canvas[top:top+nh, left:left+nw] = resized\n",
        "    return canvas\n",
        "\n",
        "def frames_from_video(video_path, n_frames, out_hw=(112,112), frame_step=2, training=True):\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    if not cap.isOpened():\n",
        "        return np.zeros((n_frames, out_hw[0], out_hw[1], 3), dtype=np.uint8)\n",
        "\n",
        "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    need_len = 1 + (n_frames - 1) * frame_step\n",
        "    if length <= 0 or need_len > length:\n",
        "        start = 0\n",
        "    else:\n",
        "        max_start = length - need_len\n",
        "        start = random.randint(0, max_start) if training else 0\n",
        "\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
        "\n",
        "    frames = []\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        cap.release()\n",
        "        return np.zeros((n_frames, out_hw[0], out_hw[1], 3), dtype=np.uint8)\n",
        "\n",
        "    frames.append(resize_with_pad(frame, out_hw))\n",
        "\n",
        "    for _ in range(n_frames - 1):\n",
        "        for _ in range(frame_step):\n",
        "            ret, frame = cap.read()\n",
        "        if ret:\n",
        "            frames.append(resize_with_pad(frame, out_hw))\n",
        "        else:\n",
        "            frames.append(np.zeros_like(frames[0]))\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # BGR -> RGB and make contiguous\n",
        "    frames = np.stack(frames, axis=0)[..., ::-1].copy()\n",
        "    return frames  # [T,H,W,3] uint8 RGB\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Dataset\n",
        "# =========================================================\n",
        "IM_MEAN = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
        "IM_STD  = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
        "\n",
        "class UCFSubsetDataset(Dataset):\n",
        "    def __init__(self, split_dir, n_frames=16, frame_step=2, training=False, class_to_idx=None):\n",
        "        self.split_dir = pathlib.Path(split_dir)\n",
        "        self.n_frames = n_frames\n",
        "        self.frame_step = frame_step\n",
        "        self.training = training\n",
        "\n",
        "        self.video_paths = sorted(self.split_dir.glob(\"*/*.avi\"))\n",
        "        self.class_names = sorted({p.parent.name for p in self.video_paths})\n",
        "\n",
        "        if class_to_idx is None:\n",
        "            self.class_to_idx = {c:i for i,c in enumerate(self.class_names)}\n",
        "        else:\n",
        "            self.class_to_idx = class_to_idx\n",
        "\n",
        "        self.labels = [self.class_to_idx[p.parent.name] for p in self.video_paths]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        vp = self.video_paths[idx]\n",
        "        y = self.labels[idx]\n",
        "\n",
        "        frames = frames_from_video(vp, self.n_frames, (H,W), self.frame_step, self.training)\n",
        "        x = torch.from_numpy(frames).permute(0,3,1,2).float() / 255.0   # [T,3,H,W]\n",
        "        x = (x - IM_MEAN) / IM_STD                                       # ImageNet normalize\n",
        "        return x, torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "def collate_btchw(batch):\n",
        "    xs, ys = zip(*batch)\n",
        "    x = torch.stack(xs, dim=0)  # [B,T,3,H,W]\n",
        "    y = torch.stack(ys, dim=0)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# ISI-Phase Encoder (STRICT phase-lock on time grid)\n",
        "# =========================================================\n",
        "@torch.no_grad()\n",
        "def isi_phase_fixedK_strict_timegrid(\n",
        "    x_img_unit: torch.Tensor,     # [B,3,H,W] in [0,1]\n",
        "    T: int,\n",
        "    K: int,\n",
        "    P: int,\n",
        "    phi0: int = 0,\n",
        "    alpha_max: float = 2.0,\n",
        "    eps: float = 1e-3,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Return spikes [T,B,3,H,W], exactly K spikes per pixel/channel.\n",
        "    All spikes lie on phase-locked grid: t = phi0 + k*P.\n",
        "    \"\"\"\n",
        "    assert 0 <= phi0 < P\n",
        "    device = x_img_unit.device\n",
        "    B, C, H, W = x_img_unit.shape\n",
        "\n",
        "    x = x_img_unit.clamp(0.0, 1.0).view(B, -1)  # [B,N]\n",
        "    N = x.size(1)\n",
        "\n",
        "    M = int(((T - 1 - phi0) // P) + 1)\n",
        "    if K > M:\n",
        "        raise ValueError(f\"K={K} must satisfy K<=M={M} (phase bins).\")\n",
        "\n",
        "    # ISI-like distribution over bins k=0..M-1\n",
        "    k_grid = torch.arange(M, device=device, dtype=torch.float32).view(1, 1, M)\n",
        "    mid = (M - 1) / 2.0\n",
        "\n",
        "    alpha = (x * 2.0 - 1.0) * alpha_max\n",
        "    alpha = alpha.unsqueeze(-1)  # [B,N,1]\n",
        "\n",
        "    w = torch.exp(alpha * (k_grid - mid))                  # [B,N,M]\n",
        "    w = w / (w.sum(dim=-1, keepdim=True) + 1e-12)\n",
        "    cdf = torch.cumsum(w, dim=-1).contiguous()             # [B,N,M]\n",
        "\n",
        "    # quantiles\n",
        "    q = torch.linspace(eps, 1.0 - eps, steps=K, device=device, dtype=torch.float32)\n",
        "    q = q.view(1, 1, K).expand(B, N, K).contiguous()\n",
        "\n",
        "    k_idx = torch.searchsorted(cdf, q).clamp(0, M - 1).long()  # [B,N,K]\n",
        "    k_idx, _ = torch.sort(k_idx, dim=-1)\n",
        "\n",
        "    # strict uniqueness in bins\n",
        "    used = torch.zeros(B, N, M, device=device, dtype=torch.bool)\n",
        "    k_fixed = torch.full_like(k_idx, -1)\n",
        "\n",
        "    for kk in range(K):\n",
        "        k0 = k_idx[..., kk]\n",
        "        free = ~used.gather(dim=2, index=k0.unsqueeze(-1)).squeeze(-1)\n",
        "        k_fixed[..., kk] = torch.where(free, k0, torch.full_like(k0, -1))\n",
        "        if free.any():\n",
        "            used[free] |= F.one_hot(k0[free], num_classes=M).bool()\n",
        "\n",
        "    # resolve collisions: nearest free bin (forward then backward)\n",
        "    for kk in range(K):\n",
        "        need = (k_fixed[..., kk] < 0)\n",
        "        if not need.any():\n",
        "            continue\n",
        "\n",
        "        k0 = k_idx[..., kk].clone()\n",
        "        avail = ~used\n",
        "        ar = torch.arange(M, device=device).view(1, 1, M)\n",
        "\n",
        "        forward_mask = avail & (ar >= k0.unsqueeze(-1))\n",
        "        fwd_pos = forward_mask.float().argmax(dim=-1)\n",
        "        fwd_exists = forward_mask.any(dim=-1)\n",
        "\n",
        "        backward_mask = avail & (ar <= k0.unsqueeze(-1))\n",
        "        rev = torch.flip(backward_mask, dims=[-1])\n",
        "        bwd_pos_rev = rev.float().argmax(dim=-1)\n",
        "        bwd_pos = (M - 1) - bwd_pos_rev\n",
        "        bwd_exists = backward_mask.any(dim=-1)\n",
        "\n",
        "        chosen = torch.where(fwd_exists, fwd_pos, bwd_pos).long()\n",
        "        k_fixed[..., kk] = torch.where(need, chosen, k_fixed[..., kk])\n",
        "        used[need] |= F.one_hot(chosen[need], num_classes=M).bool()\n",
        "\n",
        "    # bins -> time indices\n",
        "    t_idx = (phi0 + k_fixed * P).long()  # [B,N,K], all in [0..T-1]\n",
        "\n",
        "    spk_flat = torch.zeros(T, B, N, device=device, dtype=torch.float32)\n",
        "    b_idx = torch.arange(B, device=device).view(B, 1, 1).expand(B, N, K)\n",
        "    n_idx = torch.arange(N, device=device).view(1, N, 1).expand(B, N, K)\n",
        "    spk_flat[t_idx, b_idx, n_idx] = 1.0\n",
        "\n",
        "    return spk_flat.view(T, B, C, H, W)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def isiphase_encode_ucf_video(\n",
        "    x_video_norm: torch.Tensor,   # [B,T,3,H,W] normalized\n",
        "    T_steps: int,\n",
        "    K: int,\n",
        "    P: int,\n",
        "    phi0: int,\n",
        "    alpha_max: float = 2.0,\n",
        "    eps: float = 1e-3,\n",
        "    agg=\"max\",\n",
        "):\n",
        "    mean = IM_MEAN.to(x_video_norm.device)\n",
        "    std  = IM_STD.to(x_video_norm.device)\n",
        "    x_raw = (x_video_norm * std + mean).clamp(0.0, 1.0)  # [B,T,3,H,W] in [0,1]\n",
        "\n",
        "    if agg == \"max\":\n",
        "        x_img = x_raw.max(dim=1).values   # [B,3,H,W]\n",
        "    elif agg == \"mean\":\n",
        "        x_img = x_raw.mean(dim=1)\n",
        "    else:\n",
        "        raise ValueError(\"agg must be 'max' or 'mean'\")\n",
        "\n",
        "    return isi_phase_fixedK_strict_timegrid(\n",
        "        x_img, T=T_steps, K=K, P=P, phi0=phi0, alpha_max=alpha_max, eps=eps\n",
        "    )  # [T,B,3,H,W]\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Conv3D SNN (analog logits head)\n",
        "# =========================================================\n",
        "class Conv3DSNN_ISIPhase(nn.Module):\n",
        "    def __init__(self, beta=0.95, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv3d(3, 32, 3, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm3d(32)\n",
        "        self.lif1 = snn.Leaky(beta=beta)\n",
        "\n",
        "        self.conv2 = nn.Conv3d(32, 64, 3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm3d(64)\n",
        "        self.lif2 = snn.Leaky(beta=beta)\n",
        "\n",
        "        self.conv3 = nn.Conv3d(64, 128, 3, padding=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm3d(128)\n",
        "        self.lif3 = snn.Leaky(beta=beta)\n",
        "\n",
        "        self.pool = nn.MaxPool3d((1,2,2))\n",
        "        self.gap = nn.AdaptiveAvgPool3d((None,1,1))\n",
        "\n",
        "        self.fc1 = nn.Linear(128, 256)\n",
        "        self.lif4 = snn.Leaky(beta=beta)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def _lif_over_time_2d(self, x_bcthw, lif, mem):\n",
        "        B,C,T,H,W = x_bcthw.shape\n",
        "        spk_list=[]\n",
        "        for t in range(T):\n",
        "            spk_t, mem = lif(x_bcthw[:, :, t], mem)\n",
        "            spk_list.append(spk_t)\n",
        "        return torch.stack(spk_list, dim=2), mem\n",
        "\n",
        "    def forward(self, spk_in):\n",
        "        # spk_in: [T,B,3,H,W]\n",
        "        x = spk_in.permute(1,2,0,3,4).contiguous()  # [B,3,T,H,W]\n",
        "\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        mem2 = self.lif2.init_leaky()\n",
        "        mem3 = self.lif3.init_leaky()\n",
        "        mem4 = self.lif4.init_leaky()\n",
        "\n",
        "        x = self.conv1(x); x = self.bn1(x); x, mem1 = self._lif_over_time_2d(x, self.lif1, mem1); x = self.pool(x)\n",
        "        x = self.conv2(x); x = self.bn2(x); x, mem2 = self._lif_over_time_2d(x, self.lif2, mem2); x = self.pool(x)\n",
        "        x = self.conv3(x); x = self.bn3(x); x, mem3 = self._lif_over_time_2d(x, self.lif3, mem3); x = self.pool(x)\n",
        "\n",
        "        x = self.gap(x).squeeze(-1).squeeze(-1)  # [B,128,T]\n",
        "\n",
        "        logits_list=[]\n",
        "        for t in range(x.shape[2]):\n",
        "            h = self.fc1(x[:,:,t])\n",
        "            spk4, mem4 = self.lif4(h, mem4)\n",
        "            logits_t = self.fc2(spk4)\n",
        "            logits_list.append(logits_t)\n",
        "\n",
        "        return torch.stack(logits_list, dim=0)  # [T,B,num_classes]\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Build loaders from subset_paths (must exist)\n",
        "# =========================================================\n",
        "train_ds = UCFSubsetDataset(subset_paths[\"train\"], n_frames=T_steps, frame_step=frame_step, training=True)\n",
        "class_to_idx = train_ds.class_to_idx\n",
        "val_ds   = UCFSubsetDataset(subset_paths[\"val\"],   n_frames=T_steps, frame_step=frame_step, training=False, class_to_idx=class_to_idx)\n",
        "test_ds  = UCFSubsetDataset(subset_paths[\"test\"],  n_frames=T_steps, frame_step=frame_step, training=False, class_to_idx=class_to_idx)\n",
        "\n",
        "num_classes = len(class_to_idx)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True,\n",
        "                          num_workers=2, pin_memory=True, collate_fn=collate_btchw)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, drop_last=False,\n",
        "                          num_workers=2, pin_memory=True, collate_fn=collate_btchw)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, drop_last=False,\n",
        "                          num_workers=2, pin_memory=True, collate_fn=collate_btchw)\n",
        "\n",
        "print(f\"num_classes = {num_classes} | T_steps = {T_steps} | train batches = {len(train_loader)} | \"\n",
        "      f\"ISI-Phase: K={K_spikes}, P={P}, phi0={phi0}, M={M}, agg={agg_mode}\")\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Train / Eval + BEST checkpoint\n",
        "# =========================================================\n",
        "model = Conv3DSNN_ISIPhase(beta=beta, num_classes=num_classes).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "start_epoch = 1\n",
        "best_val = -1.0\n",
        "\n",
        "# optional resume from LAST\n",
        "if os.path.exists(last_ckpt_path):\n",
        "    ckpt = torch.load(last_ckpt_path, map_location=device)\n",
        "    model.load_state_dict(ckpt[\"model_state\"])\n",
        "    optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n",
        "    scheduler.load_state_dict(ckpt[\"scheduler_state\"])\n",
        "    start_epoch = ckpt[\"epoch\"] + 1\n",
        "    best_val = ckpt.get(\"best_val\", -1.0)\n",
        "    print(\"Resumed from epoch\", start_epoch, \"best_val\", best_val)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "    correct, total, loss_sum = 0, 0, 0.0\n",
        "    for x, y in loader:\n",
        "        x = x.to(device, non_blocking=True)  # [B,T,3,H,W]\n",
        "        y = y.to(device, non_blocking=True)\n",
        "\n",
        "        spk_in = isiphase_encode_ucf_video(\n",
        "            x, T_steps=T_steps, K=K_spikes, P=P, phi0=phi0, alpha_max=alpha_max, eps=eps, agg=agg_mode\n",
        "        )  # [T,B,3,H,W]\n",
        "\n",
        "        logits_rec = model(spk_in)           # [T,B,C]\n",
        "        logits = logits_rec.mean(dim=0)      # [B,C]\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "\n",
        "        pred = logits.argmax(dim=1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.numel()\n",
        "        loss_sum += loss.item() * y.size(0)\n",
        "\n",
        "    return loss_sum / max(total, 1), correct / max(total, 1)\n",
        "\n",
        "for epoch in range(start_epoch, num_epochs + 1):\n",
        "    model.train()\n",
        "    running_loss, running_correct, running_total = 0.0, 0, 0\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        y = y.to(device, non_blocking=True)\n",
        "\n",
        "        spk_in = isiphase_encode_ucf_video(\n",
        "            x, T_steps=T_steps, K=K_spikes, P=P, phi0=phi0, alpha_max=alpha_max, eps=eps, agg=agg_mode\n",
        "        )\n",
        "\n",
        "        logits_rec = model(spk_in)\n",
        "        logits = logits_rec.mean(dim=0)\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * y.size(0)\n",
        "        running_correct += (logits.argmax(dim=1) == y).sum().item()\n",
        "        running_total += y.size(0)\n",
        "\n",
        "    scheduler.step()\n",
        "    val_loss, val_acc = evaluate(val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | \"\n",
        "          f\"train loss {running_loss/running_total:.4f} | \"\n",
        "          f\"train acc {running_correct/running_total:.4f} | \"\n",
        "          f\"val loss {val_loss:.4f} | \"\n",
        "          f\"val acc {val_acc:.4f}\")\n",
        "\n",
        "    # save LAST\n",
        "    torch.save({\n",
        "        \"epoch\": epoch,\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"optimizer_state\": optimizer.state_dict(),\n",
        "        \"scheduler_state\": scheduler.state_dict(),\n",
        "        \"best_val\": best_val,\n",
        "        \"class_to_idx\": class_to_idx,\n",
        "        \"T_steps\": T_steps,\n",
        "        \"K_spikes\": K_spikes,\n",
        "        \"P\": P,\n",
        "        \"phi0\": phi0,\n",
        "        \"alpha_max\": alpha_max,\n",
        "        \"agg_mode\": agg_mode,\n",
        "    }, last_ckpt_path)\n",
        "\n",
        "    # save BEST\n",
        "    if val_acc > best_val:\n",
        "        best_val = val_acc\n",
        "        torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"model_state\": model.state_dict(),\n",
        "            \"optimizer_state\": optimizer.state_dict(),\n",
        "            \"scheduler_state\": scheduler.state_dict(),\n",
        "            \"best_val\": best_val,\n",
        "            \"class_to_idx\": class_to_idx,\n",
        "            \"T_steps\": T_steps,\n",
        "            \"K_spikes\": K_spikes,\n",
        "            \"P\": P,\n",
        "            \"phi0\": phi0,\n",
        "            \"alpha_max\": alpha_max,\n",
        "            \"agg_mode\": agg_mode,\n",
        "        }, best_ckpt_path)\n",
        "        print(f\"Saved BEST: epoch {epoch} | best_val={best_val:.4f} -> {best_ckpt_path}\")\n",
        "\n",
        "print(\"\\nLoading BEST checkpoint for test...\")\n",
        "best = torch.load(best_ckpt_path, map_location=device)\n",
        "model.load_state_dict(best[\"model_state\"])\n",
        "test_loss, test_acc = evaluate(test_loader)\n",
        "print(f\"BEST epoch {best['epoch']} | best_val {best['best_val']:.4f} | Test loss {test_loss:.4f} | Test acc {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81D9Pk7e9kbI",
        "outputId": "ee0c0321-abe5-420c-a9dd-707865962dbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_classes = 10 | T_steps = 16 | train batches = 37 | ISI-Phase: K=4, P=2, phi0=0, M=8, agg=max\n",
            "Epoch 01 | train loss 2.2863 | train acc 0.0946 | val loss 2.2057 | val acc 0.2300\n",
            "Saved BEST: epoch 1 | best_val=0.2300 -> ./ucf_subset_isiphase_best.pth\n",
            "Epoch 02 | train loss 2.1671 | train acc 0.1757 | val loss 2.0058 | val acc 0.2000\n",
            "Epoch 03 | train loss 2.0578 | train acc 0.1959 | val loss 2.0957 | val acc 0.1700\n",
            "Epoch 04 | train loss 2.0612 | train acc 0.2230 | val loss 1.9685 | val acc 0.2800\n",
            "Saved BEST: epoch 4 | best_val=0.2800 -> ./ucf_subset_isiphase_best.pth\n",
            "Epoch 05 | train loss 2.0633 | train acc 0.1993 | val loss 1.9312 | val acc 0.2400\n",
            "Epoch 06 | train loss 1.9822 | train acc 0.2432 | val loss 2.2642 | val acc 0.1900\n",
            "Epoch 07 | train loss 1.9482 | train acc 0.2432 | val loss 2.2099 | val acc 0.1600\n",
            "Epoch 08 | train loss 1.9311 | train acc 0.2703 | val loss 1.8345 | val acc 0.3400\n",
            "Saved BEST: epoch 8 | best_val=0.3400 -> ./ucf_subset_isiphase_best.pth\n",
            "Epoch 09 | train loss 1.8568 | train acc 0.2939 | val loss 1.8404 | val acc 0.2500\n",
            "Epoch 10 | train loss 1.8288 | train acc 0.2770 | val loss 1.8166 | val acc 0.3300\n",
            "Epoch 11 | train loss 1.8455 | train acc 0.3041 | val loss 2.1912 | val acc 0.2500\n",
            "Epoch 12 | train loss 1.8871 | train acc 0.2804 | val loss 2.0253 | val acc 0.3200\n",
            "Epoch 13 | train loss 1.7615 | train acc 0.3480 | val loss 1.7331 | val acc 0.3100\n",
            "Epoch 14 | train loss 1.7574 | train acc 0.3007 | val loss 2.7722 | val acc 0.1900\n",
            "Epoch 15 | train loss 1.6646 | train acc 0.3750 | val loss 1.7682 | val acc 0.4100\n",
            "Saved BEST: epoch 15 | best_val=0.4100 -> ./ucf_subset_isiphase_best.pth\n",
            "Epoch 16 | train loss 1.6219 | train acc 0.3885 | val loss 1.4540 | val acc 0.4900\n",
            "Saved BEST: epoch 16 | best_val=0.4900 -> ./ucf_subset_isiphase_best.pth\n",
            "Epoch 17 | train loss 1.5676 | train acc 0.3919 | val loss 1.4372 | val acc 0.5100\n",
            "Saved BEST: epoch 17 | best_val=0.5100 -> ./ucf_subset_isiphase_best.pth\n",
            "Epoch 18 | train loss 1.4683 | train acc 0.4426 | val loss 1.7970 | val acc 0.3300\n",
            "Epoch 19 | train loss 1.4552 | train acc 0.4358 | val loss 2.1796 | val acc 0.3000\n",
            "Epoch 20 | train loss 1.4157 | train acc 0.4358 | val loss 1.7613 | val acc 0.3400\n",
            "Epoch 21 | train loss 1.4381 | train acc 0.4122 | val loss 1.6770 | val acc 0.3900\n",
            "Epoch 22 | train loss 1.3569 | train acc 0.4932 | val loss 1.7061 | val acc 0.3900\n",
            "Epoch 23 | train loss 1.2872 | train acc 0.4966 | val loss 1.2507 | val acc 0.5500\n",
            "Saved BEST: epoch 23 | best_val=0.5500 -> ./ucf_subset_isiphase_best.pth\n",
            "Epoch 24 | train loss 1.3863 | train acc 0.4696 | val loss 3.4419 | val acc 0.1500\n",
            "Epoch 25 | train loss 1.3375 | train acc 0.4865 | val loss 1.6384 | val acc 0.4200\n",
            "Epoch 26 | train loss 1.2506 | train acc 0.5405 | val loss 1.3063 | val acc 0.5600\n",
            "Saved BEST: epoch 26 | best_val=0.5600 -> ./ucf_subset_isiphase_best.pth\n",
            "Epoch 27 | train loss 1.2909 | train acc 0.5203 | val loss 1.2856 | val acc 0.5400\n",
            "Epoch 28 | train loss 1.3225 | train acc 0.4797 | val loss 1.4026 | val acc 0.4600\n",
            "Epoch 29 | train loss 1.2300 | train acc 0.5304 | val loss 1.3334 | val acc 0.4700\n",
            "Epoch 30 | train loss 1.1945 | train acc 0.6014 | val loss 2.2816 | val acc 0.3400\n",
            "Epoch 31 | train loss 1.2587 | train acc 0.4865 | val loss 1.5063 | val acc 0.4700\n",
            "Epoch 32 | train loss 1.2062 | train acc 0.5507 | val loss 1.4514 | val acc 0.5100\n",
            "Epoch 33 | train loss 1.1140 | train acc 0.5811 | val loss 1.1747 | val acc 0.6200\n",
            "Saved BEST: epoch 33 | best_val=0.6200 -> ./ucf_subset_isiphase_best.pth\n",
            "Epoch 34 | train loss 1.2092 | train acc 0.5878 | val loss 1.3257 | val acc 0.4900\n",
            "Epoch 35 | train loss 1.1781 | train acc 0.5946 | val loss 1.1779 | val acc 0.5400\n",
            "Epoch 36 | train loss 1.1231 | train acc 0.6115 | val loss 1.9518 | val acc 0.3200\n",
            "Epoch 37 | train loss 1.0691 | train acc 0.6182 | val loss 1.4791 | val acc 0.5300\n",
            "Epoch 38 | train loss 1.0294 | train acc 0.6689 | val loss 1.1473 | val acc 0.6000\n",
            "Epoch 39 | train loss 1.0941 | train acc 0.5912 | val loss 1.5603 | val acc 0.4500\n",
            "Epoch 40 | train loss 1.0856 | train acc 0.6250 | val loss 2.3992 | val acc 0.3100\n",
            "Epoch 41 | train loss 1.0061 | train acc 0.6014 | val loss 2.3213 | val acc 0.2100\n",
            "Epoch 42 | train loss 1.0391 | train acc 0.6757 | val loss 1.4016 | val acc 0.5300\n",
            "Epoch 43 | train loss 1.0179 | train acc 0.6486 | val loss 1.2639 | val acc 0.5400\n",
            "Epoch 44 | train loss 0.9815 | train acc 0.6520 | val loss 1.0551 | val acc 0.6500\n",
            "Saved BEST: epoch 44 | best_val=0.6500 -> ./ucf_subset_isiphase_best.pth\n",
            "Epoch 45 | train loss 0.9627 | train acc 0.6689 | val loss 1.8273 | val acc 0.4100\n",
            "Epoch 46 | train loss 0.9739 | train acc 0.6385 | val loss 1.1530 | val acc 0.6100\n",
            "Epoch 47 | train loss 0.8762 | train acc 0.6926 | val loss 1.0853 | val acc 0.6500\n",
            "Epoch 48 | train loss 0.9019 | train acc 0.6824 | val loss 1.4196 | val acc 0.4800\n",
            "Epoch 49 | train loss 0.9153 | train acc 0.6554 | val loss 1.0547 | val acc 0.6500\n",
            "Epoch 50 | train loss 0.8493 | train acc 0.6892 | val loss 1.0695 | val acc 0.6400\n",
            "Epoch 51 | train loss 0.9266 | train acc 0.6892 | val loss 1.0615 | val acc 0.6000\n",
            "Epoch 52 | train loss 0.9014 | train acc 0.6858 | val loss 1.1141 | val acc 0.6300\n",
            "Epoch 53 | train loss 0.9408 | train acc 0.6689 | val loss 1.1174 | val acc 0.6200\n",
            "Epoch 54 | train loss 0.8156 | train acc 0.7128 | val loss 1.1823 | val acc 0.5700\n",
            "Epoch 55 | train loss 0.8035 | train acc 0.7095 | val loss 1.0861 | val acc 0.6200\n",
            "Epoch 56 | train loss 0.7668 | train acc 0.7264 | val loss 1.1928 | val acc 0.6100\n",
            "Epoch 57 | train loss 0.8927 | train acc 0.6959 | val loss 1.0916 | val acc 0.6300\n",
            "Epoch 58 | train loss 0.8268 | train acc 0.7196 | val loss 1.0834 | val acc 0.6200\n",
            "Epoch 59 | train loss 0.7861 | train acc 0.7635 | val loss 1.0833 | val acc 0.6300\n",
            "Epoch 60 | train loss 0.7642 | train acc 0.7500 | val loss 1.0704 | val acc 0.6400\n",
            "Epoch 61 | train loss 0.7685 | train acc 0.7297 | val loss 1.0325 | val acc 0.6600\n",
            "Saved BEST: epoch 61 | best_val=0.6600 -> ./ucf_subset_isiphase_best.pth\n",
            "Epoch 62 | train loss 0.7056 | train acc 0.7635 | val loss 1.0930 | val acc 0.6500\n",
            "Epoch 63 | train loss 0.7749 | train acc 0.7432 | val loss 1.0399 | val acc 0.6500\n",
            "Epoch 64 | train loss 0.7920 | train acc 0.7230 | val loss 0.9967 | val acc 0.6900\n",
            "Saved BEST: epoch 64 | best_val=0.6900 -> ./ucf_subset_isiphase_best.pth\n",
            "Epoch 65 | train loss 0.6722 | train acc 0.7973 | val loss 1.0634 | val acc 0.6700\n",
            "Epoch 66 | train loss 0.7766 | train acc 0.7568 | val loss 1.0216 | val acc 0.6900\n",
            "Epoch 67 | train loss 0.7600 | train acc 0.7466 | val loss 1.0347 | val acc 0.7000\n",
            "Saved BEST: epoch 67 | best_val=0.7000 -> ./ucf_subset_isiphase_best.pth\n",
            "Epoch 68 | train loss 0.7585 | train acc 0.7635 | val loss 1.0221 | val acc 0.6700\n",
            "Epoch 69 | train loss 0.6869 | train acc 0.7804 | val loss 1.0211 | val acc 0.6700\n",
            "Epoch 70 | train loss 0.7173 | train acc 0.7669 | val loss 1.0433 | val acc 0.6700\n",
            "Epoch 71 | train loss 0.6752 | train acc 0.8176 | val loss 1.0350 | val acc 0.6700\n",
            "Epoch 72 | train loss 0.6817 | train acc 0.7669 | val loss 1.0573 | val acc 0.6600\n",
            "Epoch 73 | train loss 0.7049 | train acc 0.7534 | val loss 1.0541 | val acc 0.6600\n",
            "Epoch 74 | train loss 0.7052 | train acc 0.7804 | val loss 1.0523 | val acc 0.6700\n",
            "Epoch 75 | train loss 0.6337 | train acc 0.8007 | val loss 1.0320 | val acc 0.6700\n",
            "Epoch 76 | train loss 0.6968 | train acc 0.7635 | val loss 1.0402 | val acc 0.6700\n",
            "Epoch 77 | train loss 0.6738 | train acc 0.7770 | val loss 1.0122 | val acc 0.6900\n",
            "Epoch 78 | train loss 0.7148 | train acc 0.7601 | val loss 1.0214 | val acc 0.6800\n",
            "Epoch 79 | train loss 0.7209 | train acc 0.7635 | val loss 1.0420 | val acc 0.6600\n",
            "Epoch 80 | train loss 0.6825 | train acc 0.7905 | val loss 1.0246 | val acc 0.6900\n",
            "\n",
            "Loading BEST checkpoint for test...\n",
            "BEST epoch 67 | best_val 0.7000 | Test loss 1.0154 | Test acc 0.6400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. DNN"
      ],
      "metadata": {
        "id": "W5fFOyh3OP8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "import pathlib\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Config\n",
        "# =========================================================\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "batch_size = 8\n",
        "num_epochs = 80\n",
        "lr = 2e-3\n",
        "weight_decay = 1e-4\n",
        "\n",
        "# video sampling\n",
        "T_steps    = 16     # frames_per_clip\n",
        "frame_step = 2\n",
        "H, W       = 112, 112\n",
        "\n",
        "# paths (your subset structure must already exist)\n",
        "# subset_paths = {\"train\": \"...\", \"val\": \"...\", \"test\": \"...\"}  # should exist from your downloader\n",
        "# Example:\n",
        "# subset_paths = {\"train\":\"./UCF101_subset/train\", \"val\":\"./UCF101_subset/val\", \"test\":\"./UCF101_subset/test\"}\n",
        "\n",
        "last_ckpt_path = \"./ucf_subset_3dcnn_last.pth\"\n",
        "best_ckpt_path = \"./ucf_subset_3dcnn_best.pth\"\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Video utils\n",
        "# =========================================================\n",
        "def resize_with_pad(frame_bgr, out_hw=(112,112)):\n",
        "    out_h, out_w = out_hw\n",
        "    h, w = frame_bgr.shape[:2]\n",
        "    if h == 0 or w == 0:\n",
        "        return np.zeros((out_h, out_w, 3), dtype=np.uint8)\n",
        "\n",
        "    scale = min(out_w / w, out_h / h)\n",
        "    nw, nh = int(w * scale), int(h * scale)\n",
        "    resized = cv2.resize(frame_bgr, (nw, nh), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    canvas = np.zeros((out_h, out_w, 3), dtype=np.uint8)\n",
        "    top = (out_h - nh) // 2\n",
        "    left = (out_w - nw) // 2\n",
        "    canvas[top:top+nh, left:left+nw] = resized\n",
        "    return canvas\n",
        "\n",
        "def frames_from_video(video_path, n_frames, out_hw=(112,112), frame_step=2, training=True):\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    if not cap.isOpened():\n",
        "        return np.zeros((n_frames, out_hw[0], out_hw[1], 3), dtype=np.uint8)\n",
        "\n",
        "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    need_len = 1 + (n_frames - 1) * frame_step\n",
        "    if length <= 0 or need_len > length:\n",
        "        start = 0\n",
        "    else:\n",
        "        max_start = length - need_len\n",
        "        start = random.randint(0, max_start) if training else 0\n",
        "\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
        "\n",
        "    frames = []\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        cap.release()\n",
        "        return np.zeros((n_frames, out_hw[0], out_hw[1], 3), dtype=np.uint8)\n",
        "\n",
        "    frames.append(resize_with_pad(frame, out_hw))\n",
        "\n",
        "    for _ in range(n_frames - 1):\n",
        "        for _ in range(frame_step):\n",
        "            ret, frame = cap.read()\n",
        "        if ret:\n",
        "            frames.append(resize_with_pad(frame, out_hw))\n",
        "        else:\n",
        "            frames.append(np.zeros_like(frames[0]))\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # BGR -> RGB, contiguous (avoid negative strides)\n",
        "    frames = np.stack(frames, axis=0)[..., ::-1].copy()\n",
        "    return frames  # [T,H,W,3] uint8 RGB\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Dataset\n",
        "# =========================================================\n",
        "IM_MEAN = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
        "IM_STD  = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
        "\n",
        "class UCFSubsetDataset(Dataset):\n",
        "    def __init__(self, split_dir, n_frames=16, frame_step=2, training=False, class_to_idx=None):\n",
        "        self.split_dir = pathlib.Path(split_dir)\n",
        "        self.n_frames = n_frames\n",
        "        self.frame_step = frame_step\n",
        "        self.training = training\n",
        "\n",
        "        self.video_paths = sorted(self.split_dir.glob(\"*/*.avi\"))\n",
        "        self.class_names = sorted({p.parent.name for p in self.video_paths})\n",
        "\n",
        "        if class_to_idx is None:\n",
        "            self.class_to_idx = {c:i for i,c in enumerate(self.class_names)}\n",
        "        else:\n",
        "            self.class_to_idx = class_to_idx\n",
        "\n",
        "        self.labels = [self.class_to_idx[p.parent.name] for p in self.video_paths]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        vp = self.video_paths[idx]\n",
        "        y = self.labels[idx]\n",
        "\n",
        "        frames = frames_from_video(vp, self.n_frames, (H,W), self.frame_step, self.training)\n",
        "        x = torch.from_numpy(frames).permute(0,3,1,2).float() / 255.0   # [T,3,H,W]\n",
        "        x = (x - IM_MEAN) / IM_STD\n",
        "        return x, torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "def collate_btchw(batch):\n",
        "    xs, ys = zip(*batch)\n",
        "    x = torch.stack(xs, dim=0)  # [B,T,3,H,W]\n",
        "    y = torch.stack(ys, dim=0)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Plain 3D CNN (no encoding, no SNN)\n",
        "# =========================================================\n",
        "class Plain3DCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv3d(3, 32, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm3d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool3d(kernel_size=(1,2,2))  # keep time\n",
        "        )\n",
        "\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv3d(32, 64, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm3d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool3d(kernel_size=(2,2,2))  # downsample time+spatial\n",
        "        )\n",
        "\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv3d(64, 128, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm3d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool3d(kernel_size=(2,2,2))\n",
        "        )\n",
        "\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv3d(128, 256, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm3d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.gap = nn.AdaptiveAvgPool3d((1,1,1))\n",
        "        self.fc  = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x_btchw):\n",
        "        # x: [B,T,3,H,W] -> [B,3,T,H,W]\n",
        "        x = x_btchw.permute(0,2,1,3,4).contiguous()\n",
        "\n",
        "        x = self.stem(x)\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "\n",
        "        x = self.gap(x).flatten(1)   # [B,256]\n",
        "        logits = self.fc(x)          # [B,num_classes]\n",
        "        return logits\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Build loaders\n",
        "# =========================================================\n",
        "# ---- IMPORTANT: subset_paths must exist in your runtime ----\n",
        "# Example if you need:\n",
        "# subset_paths = {\"train\":\"./UCF101_subset/train\", \"val\":\"./UCF101_subset/val\", \"test\":\"./UCF101_subset/test\"}\n",
        "\n",
        "train_ds = UCFSubsetDataset(subset_paths[\"train\"], n_frames=T_steps, frame_step=frame_step, training=True)\n",
        "class_to_idx = train_ds.class_to_idx\n",
        "val_ds   = UCFSubsetDataset(subset_paths[\"val\"],   n_frames=T_steps, frame_step=frame_step, training=False, class_to_idx=class_to_idx)\n",
        "test_ds  = UCFSubsetDataset(subset_paths[\"test\"],  n_frames=T_steps, frame_step=frame_step, training=False, class_to_idx=class_to_idx)\n",
        "\n",
        "num_classes = len(class_to_idx)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True,\n",
        "                          num_workers=2, pin_memory=True, collate_fn=collate_btchw)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, drop_last=False,\n",
        "                          num_workers=2, pin_memory=True, collate_fn=collate_btchw)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, drop_last=False,\n",
        "                          num_workers=2, pin_memory=True, collate_fn=collate_btchw)\n",
        "\n",
        "print(f\"num_classes = {num_classes} | T_steps = {T_steps} | train batches = {len(train_loader)}\")\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Train / Eval + BEST checkpoint\n",
        "# =========================================================\n",
        "model = Plain3DCNN(num_classes=num_classes).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "start_epoch = 1\n",
        "best_val = -1.0\n",
        "\n",
        "# optional resume from LAST\n",
        "if os.path.exists(last_ckpt_path):\n",
        "    ckpt = torch.load(last_ckpt_path, map_location=device)\n",
        "    model.load_state_dict(ckpt[\"model_state\"])\n",
        "    optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n",
        "    scheduler.load_state_dict(ckpt[\"scheduler_state\"])\n",
        "    start_epoch = ckpt[\"epoch\"] + 1\n",
        "    best_val = ckpt.get(\"best_val\", -1.0)\n",
        "    print(\"Resumed from epoch\", start_epoch, \"best_val\", best_val)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "    correct, total, loss_sum = 0, 0, 0.0\n",
        "    for x, y in loader:\n",
        "        x = x.to(device, non_blocking=True)  # [B,T,3,H,W]\n",
        "        y = y.to(device, non_blocking=True)\n",
        "\n",
        "        logits = model(x)                    # [B,C]\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "\n",
        "        pred = logits.argmax(dim=1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.numel()\n",
        "        loss_sum += loss.item() * y.size(0)\n",
        "\n",
        "    return loss_sum / max(total, 1), correct / max(total, 1)\n",
        "\n",
        "for epoch in range(start_epoch, num_epochs + 1):\n",
        "    model.train()\n",
        "    running_loss, running_correct, running_total = 0.0, 0, 0\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        y = y.to(device, non_blocking=True)\n",
        "\n",
        "        logits = model(x)\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * y.size(0)\n",
        "        running_correct += (logits.argmax(dim=1) == y).sum().item()\n",
        "        running_total += y.size(0)\n",
        "\n",
        "    scheduler.step()\n",
        "    val_loss, val_acc = evaluate(val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | \"\n",
        "          f\"train loss {running_loss/running_total:.4f} | \"\n",
        "          f\"train acc {running_correct/running_total:.4f} | \"\n",
        "          f\"val loss {val_loss:.4f} | \"\n",
        "          f\"val acc {val_acc:.4f}\")\n",
        "\n",
        "    # -------- save LAST --------\n",
        "    torch.save({\n",
        "        \"epoch\": epoch,\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"optimizer_state\": optimizer.state_dict(),\n",
        "        \"scheduler_state\": scheduler.state_dict(),\n",
        "        \"best_val\": best_val,\n",
        "        \"class_to_idx\": class_to_idx,\n",
        "        \"T_steps\": T_steps,\n",
        "        \"frame_step\": frame_step,\n",
        "        \"H\": H, \"W\": W,\n",
        "    }, last_ckpt_path)\n",
        "\n",
        "    # -------- save BEST --------\n",
        "    if val_acc > best_val:\n",
        "        best_val = val_acc\n",
        "        torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"model_state\": model.state_dict(),\n",
        "            \"optimizer_state\": optimizer.state_dict(),\n",
        "            \"scheduler_state\": scheduler.state_dict(),\n",
        "            \"best_val\": best_val,\n",
        "            \"class_to_idx\": class_to_idx,\n",
        "            \"T_steps\": T_steps,\n",
        "            \"frame_step\": frame_step,\n",
        "            \"H\": H, \"W\": W,\n",
        "        }, best_ckpt_path)\n",
        "        print(f\"Saved BEST: epoch {epoch} | best_val={best_val:.4f} -> {best_ckpt_path}\")\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# Test using BEST checkpoint\n",
        "# =========================================================\n",
        "print(\"\\nLoading BEST checkpoint for test...\")\n",
        "best = torch.load(best_ckpt_path, map_location=device)\n",
        "model.load_state_dict(best[\"model_state\"])\n",
        "\n",
        "test_loss, test_acc = evaluate(test_loader)\n",
        "print(f\"BEST epoch {best['epoch']} | best_val {best['best_val']:.4f} | Test loss {test_loss:.4f} | Test acc {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVBer7mEOWbh",
        "outputId": "ae84f310-9a31-44e4-861a-15383b031a98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_classes = 10 | T_steps = 16 | train batches = 37\n",
            "Epoch 01 | train loss 2.0690 | train acc 0.2872 | val loss 2.0641 | val acc 0.2500\n",
            "Saved BEST: epoch 1 | best_val=0.2500 -> ./ucf_subset_3dcnn_best.pth\n",
            "Epoch 02 | train loss 1.7658 | train acc 0.3682 | val loss 1.8308 | val acc 0.3300\n",
            "Saved BEST: epoch 2 | best_val=0.3300 -> ./ucf_subset_3dcnn_best.pth\n",
            "Epoch 03 | train loss 1.5896 | train acc 0.4392 | val loss 2.2119 | val acc 0.2200\n",
            "Epoch 04 | train loss 1.3971 | train acc 0.5068 | val loss 3.2156 | val acc 0.2200\n",
            "Epoch 05 | train loss 1.3402 | train acc 0.5236 | val loss 1.8561 | val acc 0.3800\n",
            "Saved BEST: epoch 5 | best_val=0.3800 -> ./ucf_subset_3dcnn_best.pth\n",
            "Epoch 06 | train loss 1.2737 | train acc 0.5101 | val loss 1.6452 | val acc 0.3400\n",
            "Epoch 07 | train loss 1.1396 | train acc 0.6047 | val loss 1.9412 | val acc 0.3100\n",
            "Epoch 08 | train loss 1.0957 | train acc 0.5912 | val loss 1.5799 | val acc 0.4200\n",
            "Saved BEST: epoch 8 | best_val=0.4200 -> ./ucf_subset_3dcnn_best.pth\n",
            "Epoch 09 | train loss 1.0594 | train acc 0.5845 | val loss 1.5391 | val acc 0.4600\n",
            "Saved BEST: epoch 9 | best_val=0.4600 -> ./ucf_subset_3dcnn_best.pth\n",
            "Epoch 10 | train loss 1.0491 | train acc 0.6351 | val loss 1.4969 | val acc 0.4900\n",
            "Saved BEST: epoch 10 | best_val=0.4900 -> ./ucf_subset_3dcnn_best.pth\n",
            "Epoch 11 | train loss 1.1212 | train acc 0.5912 | val loss 1.2082 | val acc 0.5500\n",
            "Saved BEST: epoch 11 | best_val=0.5500 -> ./ucf_subset_3dcnn_best.pth\n",
            "Epoch 12 | train loss 1.0674 | train acc 0.6216 | val loss 0.9595 | val acc 0.6600\n",
            "Saved BEST: epoch 12 | best_val=0.6600 -> ./ucf_subset_3dcnn_best.pth\n",
            "Epoch 13 | train loss 0.9442 | train acc 0.6689 | val loss 1.1125 | val acc 0.6200\n",
            "Epoch 14 | train loss 0.9566 | train acc 0.6318 | val loss 0.9856 | val acc 0.6500\n",
            "Epoch 15 | train loss 0.8443 | train acc 0.6926 | val loss 0.9540 | val acc 0.6800\n",
            "Saved BEST: epoch 15 | best_val=0.6800 -> ./ucf_subset_3dcnn_best.pth\n",
            "Epoch 16 | train loss 0.8410 | train acc 0.7162 | val loss 1.0960 | val acc 0.6800\n",
            "Epoch 17 | train loss 0.8333 | train acc 0.7264 | val loss 2.0240 | val acc 0.4200\n",
            "Epoch 18 | train loss 0.9412 | train acc 0.6791 | val loss 1.1078 | val acc 0.5800\n",
            "Epoch 19 | train loss 0.8268 | train acc 0.6959 | val loss 1.1002 | val acc 0.6800\n",
            "Epoch 20 | train loss 0.8048 | train acc 0.7095 | val loss 0.8096 | val acc 0.6900\n",
            "Saved BEST: epoch 20 | best_val=0.6900 -> ./ucf_subset_3dcnn_best.pth\n",
            "Epoch 21 | train loss 0.7795 | train acc 0.7095 | val loss 1.1483 | val acc 0.5600\n",
            "Epoch 22 | train loss 0.6854 | train acc 0.7534 | val loss 1.1189 | val acc 0.7000\n",
            "Saved BEST: epoch 22 | best_val=0.7000 -> ./ucf_subset_3dcnn_best.pth\n",
            "Epoch 23 | train loss 0.6982 | train acc 0.7804 | val loss 0.8091 | val acc 0.7200\n",
            "Saved BEST: epoch 23 | best_val=0.7200 -> ./ucf_subset_3dcnn_best.pth\n",
            "Epoch 24 | train loss 0.5921 | train acc 0.7939 | val loss 1.2980 | val acc 0.5900\n",
            "Epoch 25 | train loss 0.5879 | train acc 0.8007 | val loss 1.4907 | val acc 0.5100\n",
            "Epoch 26 | train loss 0.6541 | train acc 0.7872 | val loss 1.0330 | val acc 0.6300\n",
            "Epoch 27 | train loss 0.5968 | train acc 0.7905 | val loss 0.9826 | val acc 0.6700\n",
            "Epoch 28 | train loss 0.6435 | train acc 0.7872 | val loss 0.9644 | val acc 0.7100\n",
            "Epoch 29 | train loss 0.5290 | train acc 0.8209 | val loss 1.0439 | val acc 0.7000\n",
            "Epoch 30 | train loss 0.4804 | train acc 0.8412 | val loss 0.9748 | val acc 0.7000\n",
            "Epoch 31 | train loss 0.5971 | train acc 0.7973 | val loss 0.5963 | val acc 0.8100\n",
            "Saved BEST: epoch 31 | best_val=0.8100 -> ./ucf_subset_3dcnn_best.pth\n",
            "Epoch 32 | train loss 0.5182 | train acc 0.8311 | val loss 1.2557 | val acc 0.6100\n",
            "Epoch 33 | train loss 0.4689 | train acc 0.8514 | val loss 0.7412 | val acc 0.7100\n",
            "Epoch 34 | train loss 0.5330 | train acc 0.8108 | val loss 1.0605 | val acc 0.7300\n",
            "Epoch 35 | train loss 0.4164 | train acc 0.8514 | val loss 1.7574 | val acc 0.5700\n",
            "Epoch 36 | train loss 0.4344 | train acc 0.8378 | val loss 0.7396 | val acc 0.7300\n",
            "Epoch 37 | train loss 0.4260 | train acc 0.8547 | val loss 1.3209 | val acc 0.6900\n",
            "Epoch 38 | train loss 0.3371 | train acc 0.8919 | val loss 0.6318 | val acc 0.8300\n",
            "Saved BEST: epoch 38 | best_val=0.8300 -> ./ucf_subset_3dcnn_best.pth\n",
            "Epoch 39 | train loss 0.3392 | train acc 0.8784 | val loss 0.6203 | val acc 0.8000\n",
            "Epoch 40 | train loss 0.3725 | train acc 0.8716 | val loss 0.5609 | val acc 0.8000\n",
            "Epoch 41 | train loss 0.3662 | train acc 0.8885 | val loss 0.6838 | val acc 0.7700\n",
            "Epoch 42 | train loss 0.3091 | train acc 0.8953 | val loss 0.6683 | val acc 0.8300\n",
            "Epoch 43 | train loss 0.2564 | train acc 0.9122 | val loss 0.5888 | val acc 0.7600\n",
            "Epoch 44 | train loss 0.2498 | train acc 0.9324 | val loss 0.5990 | val acc 0.7900\n",
            "Epoch 45 | train loss 0.2910 | train acc 0.9122 | val loss 0.5954 | val acc 0.8200\n",
            "Epoch 46 | train loss 0.2842 | train acc 0.9088 | val loss 0.7607 | val acc 0.7900\n",
            "Epoch 47 | train loss 0.2625 | train acc 0.9020 | val loss 0.5370 | val acc 0.8300\n",
            "Epoch 48 | train loss 0.1882 | train acc 0.9493 | val loss 0.5245 | val acc 0.8400\n",
            "Saved BEST: epoch 48 | best_val=0.8400 -> ./ucf_subset_3dcnn_best.pth\n",
            "Epoch 49 | train loss 0.2246 | train acc 0.9324 | val loss 0.5405 | val acc 0.8300\n",
            "Epoch 50 | train loss 0.2268 | train acc 0.9122 | val loss 0.5289 | val acc 0.8200\n",
            "Epoch 51 | train loss 0.1818 | train acc 0.9426 | val loss 0.5563 | val acc 0.8100\n",
            "Epoch 52 | train loss 0.2060 | train acc 0.9324 | val loss 0.5873 | val acc 0.8000\n",
            "Epoch 53 | train loss 0.1667 | train acc 0.9527 | val loss 0.4721 | val acc 0.8500\n",
            "Saved BEST: epoch 53 | best_val=0.8500 -> ./ucf_subset_3dcnn_best.pth\n",
            "Epoch 54 | train loss 0.1611 | train acc 0.9561 | val loss 0.5876 | val acc 0.8200\n",
            "Epoch 55 | train loss 0.2017 | train acc 0.9392 | val loss 0.5392 | val acc 0.8000\n",
            "Epoch 56 | train loss 0.1426 | train acc 0.9730 | val loss 0.4862 | val acc 0.8200\n",
            "Epoch 57 | train loss 0.1177 | train acc 0.9628 | val loss 0.6483 | val acc 0.7900\n",
            "Epoch 58 | train loss 0.1231 | train acc 0.9595 | val loss 0.4557 | val acc 0.8600\n",
            "Saved BEST: epoch 58 | best_val=0.8600 -> ./ucf_subset_3dcnn_best.pth\n",
            "Epoch 59 | train loss 0.1512 | train acc 0.9426 | val loss 0.5262 | val acc 0.8300\n",
            "Epoch 60 | train loss 0.1297 | train acc 0.9561 | val loss 0.4895 | val acc 0.8500\n",
            "Epoch 61 | train loss 0.0852 | train acc 0.9865 | val loss 0.4478 | val acc 0.8600\n",
            "Epoch 62 | train loss 0.0739 | train acc 0.9899 | val loss 0.4776 | val acc 0.8100\n",
            "Epoch 63 | train loss 0.0954 | train acc 0.9764 | val loss 0.4197 | val acc 0.8300\n",
            "Epoch 64 | train loss 0.0898 | train acc 0.9764 | val loss 0.5330 | val acc 0.8100\n",
            "Epoch 65 | train loss 0.0929 | train acc 0.9764 | val loss 0.4385 | val acc 0.8400\n",
            "Epoch 66 | train loss 0.1311 | train acc 0.9696 | val loss 0.4563 | val acc 0.8400\n",
            "Epoch 67 | train loss 0.0950 | train acc 0.9662 | val loss 0.4997 | val acc 0.8200\n",
            "Epoch 68 | train loss 0.1168 | train acc 0.9730 | val loss 0.4703 | val acc 0.8300\n",
            "Epoch 69 | train loss 0.0725 | train acc 0.9865 | val loss 0.4847 | val acc 0.8300\n",
            "Epoch 70 | train loss 0.0846 | train acc 0.9797 | val loss 0.4997 | val acc 0.8400\n",
            "Epoch 71 | train loss 0.0692 | train acc 0.9899 | val loss 0.4908 | val acc 0.8300\n",
            "Epoch 72 | train loss 0.0756 | train acc 0.9865 | val loss 0.4618 | val acc 0.8500\n",
            "Epoch 73 | train loss 0.0594 | train acc 0.9865 | val loss 0.4724 | val acc 0.8600\n",
            "Epoch 74 | train loss 0.0498 | train acc 0.9932 | val loss 0.4483 | val acc 0.8400\n",
            "Epoch 75 | train loss 0.0524 | train acc 0.9899 | val loss 0.4659 | val acc 0.8300\n",
            "Epoch 76 | train loss 0.0810 | train acc 0.9730 | val loss 0.4687 | val acc 0.8300\n",
            "Epoch 77 | train loss 0.0539 | train acc 0.9932 | val loss 0.4759 | val acc 0.8400\n",
            "Epoch 78 | train loss 0.0699 | train acc 0.9865 | val loss 0.4383 | val acc 0.8300\n",
            "Epoch 79 | train loss 0.0628 | train acc 0.9797 | val loss 0.4597 | val acc 0.8400\n",
            "Epoch 80 | train loss 0.0527 | train acc 0.9932 | val loss 0.4549 | val acc 0.8500\n",
            "\n",
            "Loading BEST checkpoint for test...\n",
            "BEST epoch 58 | best_val 0.8600 | Test loss 0.4155 | Test acc 0.8300\n"
          ]
        }
      ]
    }
  ]
}