{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLTsgS8A224w"
      },
      "source": [
        "0. Install Package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJPpz2F-2jCE",
        "outputId": "f42ffa49-6baa-4301-dfdd-f6ab63085e4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting snntorch\n",
            "  Downloading snntorch-0.9.4-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu128)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: torch==2.9.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.9.0+cu128)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.9.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.9.0->torchvision) (3.0.3)\n",
            "Downloading snntorch-0.9.4-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.6/125.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: snntorch\n",
            "Successfully installed snntorch-0.9.4\n"
          ]
        }
      ],
      "source": [
        "!pip install snntorch torchvision\n",
        "acc_log = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDSujoO_22Hd"
      },
      "source": [
        "1. Rate Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3Er_FJe3BM2",
        "outputId": "249534de-13e9-4d95-f472-8a2ae82bfc17"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:14<00:00, 11.9MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 01 | train loss 2.4983 | train acc 0.2045 | test acc 0.3295\n",
            "Epoch 02 | train loss 1.9103 | train acc 0.3293 | test acc 0.3630\n",
            "Epoch 03 | train loss 1.7461 | train acc 0.3814 | test acc 0.4260\n",
            "Epoch 04 | train loss 1.6328 | train acc 0.4317 | test acc 0.4234\n",
            "Epoch 05 | train loss 1.5236 | train acc 0.4686 | test acc 0.5193\n",
            "Epoch 06 | train loss 1.4446 | train acc 0.4998 | test acc 0.5516\n",
            "Epoch 07 | train loss 1.3614 | train acc 0.5312 | test acc 0.5562\n",
            "Epoch 08 | train loss 1.2745 | train acc 0.5602 | test acc 0.5828\n",
            "Epoch 09 | train loss 1.2033 | train acc 0.5850 | test acc 0.5769\n",
            "Epoch 10 | train loss 1.1669 | train acc 0.6021 | test acc 0.6475\n",
            "Epoch 11 | train loss 1.0970 | train acc 0.6264 | test acc 0.6095\n",
            "Epoch 12 | train loss 1.0506 | train acc 0.6389 | test acc 0.6880\n",
            "Epoch 13 | train loss 1.0007 | train acc 0.6533 | test acc 0.6748\n",
            "Epoch 14 | train loss 0.9650 | train acc 0.6659 | test acc 0.6865\n",
            "Epoch 15 | train loss 0.9165 | train acc 0.6837 | test acc 0.6978\n",
            "Epoch 16 | train loss 0.8940 | train acc 0.6921 | test acc 0.7051\n",
            "Epoch 17 | train loss 0.8538 | train acc 0.7033 | test acc 0.7034\n",
            "Epoch 18 | train loss 0.8163 | train acc 0.7171 | test acc 0.7190\n",
            "Epoch 19 | train loss 0.7845 | train acc 0.7234 | test acc 0.7414\n",
            "Epoch 20 | train loss 0.7588 | train acc 0.7344 | test acc 0.7334\n",
            "Epoch 21 | train loss 0.7314 | train acc 0.7443 | test acc 0.7465\n",
            "Epoch 22 | train loss 0.7025 | train acc 0.7527 | test acc 0.7388\n",
            "Epoch 23 | train loss 0.6714 | train acc 0.7637 | test acc 0.7487\n",
            "Epoch 24 | train loss 0.6572 | train acc 0.7665 | test acc 0.7599\n",
            "Epoch 25 | train loss 0.6344 | train acc 0.7766 | test acc 0.7683\n",
            "Epoch 26 | train loss 0.6200 | train acc 0.7799 | test acc 0.7561\n",
            "Epoch 27 | train loss 0.6042 | train acc 0.7862 | test acc 0.7732\n",
            "Epoch 28 | train loss 0.5952 | train acc 0.7855 | test acc 0.7622\n",
            "Epoch 29 | train loss 0.5910 | train acc 0.7892 | test acc 0.7679\n",
            "Epoch 30 | train loss 0.5825 | train acc 0.7920 | test acc 0.7690\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import snntorch as snn\n",
        "from snntorch import spikegen\n",
        "\n",
        "# -----------------------------\n",
        "# Config\n",
        "# -----------------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "batch_size = 128\n",
        "num_epochs = 30\n",
        "lr = 2e-3\n",
        "\n",
        "T = 20                 # number of time steps (increase to 30-50 if you have GPU time)\n",
        "rate_scale = 1.0       # multiply input intensity before rate sampling (<=1 is safest)\n",
        "tau = 2.0\n",
        "beta = torch.exp(torch.tensor(-1.0 / tau)).item()\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "# -----------------------------\n",
        "# Data (CIFAR-10)\n",
        "# -----------------------------\n",
        "# Common CIFAR-10 normalization (helps a lot for conv nets)\n",
        "cifar_mean = (0.4914, 0.4822, 0.4465)\n",
        "cifar_std  = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(cifar_mean, cifar_std),\n",
        "])\n",
        "\n",
        "test_tf = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(cifar_mean, cifar_std),\n",
        "])\n",
        "\n",
        "train_set = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=train_tf)\n",
        "test_set  = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=test_tf)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_set,  batch_size=batch_size, shuffle=False, drop_last=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# -----------------------------\n",
        "# Rate encoder (Poisson/Bernoulli per timestep)\n",
        "# -----------------------------\n",
        "def rate_encode_cifar(x_img, T, rate_scale=1.0):\n",
        "    \"\"\"\n",
        "    x_img: [B, 3, 32, 32] after normalization.\n",
        "    For rate encoding, we need values in [0,1] as firing probabilities.\n",
        "    So we map normalized image -> [0,1] using a smooth squash.\n",
        "\n",
        "    Returns spk_in: [T, B, 3, 32, 32]\n",
        "    \"\"\"\n",
        "    # Map to [0,1] probability. Tanh squash is stable and avoids hard clipping artifacts.\n",
        "    # (You can also use torch.sigmoid.)\n",
        "    p = torch.tanh(x_img).add(1).mul(0.5)  # roughly [0,1]\n",
        "    p = torch.clamp(p * rate_scale, 0.0, 1.0)\n",
        "\n",
        "    # snnTorch rate encoder expects shape [B, ...] and returns [T, B, ...]\n",
        "    spk = spikegen.rate(p, num_steps=T)  # [T,B,3,32,32]\n",
        "    return spk\n",
        "\n",
        "# -----------------------------\n",
        "# A more complex Conv SNN for CIFAR-10\n",
        "# -----------------------------\n",
        "class ConvSNN_CIFAR10(nn.Module):\n",
        "    def __init__(self, beta=0.95, num_classes=10):\n",
        "        super().__init__()\n",
        "\n",
        "        # Feature extractor (Conv-BN-LIF-Pool) x3\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn1   = nn.BatchNorm2d(64)\n",
        "        self.lif1  = snn.Leaky(beta=beta)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2   = nn.BatchNorm2d(128)\n",
        "        self.lif2  = snn.Leaky(beta=beta)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn3   = nn.BatchNorm2d(256)\n",
        "        self.lif3  = snn.Leaky(beta=beta)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.drop = nn.Dropout(p=0.2)\n",
        "\n",
        "        # Classifier head\n",
        "        # After 3 pools: 32 -> 16 -> 8 -> 4\n",
        "        self.fc1  = nn.Linear(256 * 4 * 4, 512, bias=True)\n",
        "        self.lif4 = snn.Leaky(beta=beta)\n",
        "        self.fc2  = nn.Linear(512, num_classes, bias=True)\n",
        "        self.lif5 = snn.Leaky(beta=beta)\n",
        "\n",
        "    def forward(self, spk_in):\n",
        "        \"\"\"\n",
        "        spk_in: [T, B, 3, 32, 32]\n",
        "        returns spk_out: [T, B, 10]\n",
        "        \"\"\"\n",
        "        Tsteps, B, _, _, _ = spk_in.shape\n",
        "\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        mem2 = self.lif2.init_leaky()\n",
        "        mem3 = self.lif3.init_leaky()\n",
        "        mem4 = self.lif4.init_leaky()\n",
        "        mem5 = self.lif5.init_leaky()\n",
        "\n",
        "        spk_out_rec = []\n",
        "\n",
        "        for t in range(Tsteps):\n",
        "            x = spk_in[t]\n",
        "\n",
        "            x = self.conv1(x)\n",
        "            x = self.bn1(x)\n",
        "            spk1, mem1 = self.lif1(x, mem1)\n",
        "            x = self.pool(spk1)\n",
        "\n",
        "            x = self.conv2(x)\n",
        "            x = self.bn2(x)\n",
        "            spk2, mem2 = self.lif2(x, mem2)\n",
        "            x = self.pool(spk2)\n",
        "\n",
        "            x = self.conv3(x)\n",
        "            x = self.bn3(x)\n",
        "            spk3, mem3 = self.lif3(x, mem3)\n",
        "            x = self.pool(spk3)\n",
        "\n",
        "            x = self.drop(x)\n",
        "            x = x.view(B, -1)\n",
        "\n",
        "            x = self.fc1(x)\n",
        "            spk4, mem4 = self.lif4(x, mem4)\n",
        "\n",
        "            x = self.fc2(spk4)\n",
        "            spk5, mem5 = self.lif5(x, mem5)\n",
        "\n",
        "            spk_out_rec.append(spk5)\n",
        "\n",
        "        return torch.stack(spk_out_rec, dim=0)\n",
        "\n",
        "model = ConvSNN_CIFAR10(beta=beta, num_classes=num_classes).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "# -----------------------------\n",
        "# Train / Eval\n",
        "# -----------------------------\n",
        "@torch.no_grad()\n",
        "def evaluate():\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    for x, y in test_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        spk_in = rate_encode_cifar(x, T=T, rate_scale=rate_scale)\n",
        "\n",
        "        spk_out = model(spk_in)\n",
        "        logits = spk_out.sum(dim=0)  # spike-count readout\n",
        "        pred = logits.argmax(dim=1)\n",
        "\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.numel()\n",
        "    return correct / max(total, 1)\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_correct = 0\n",
        "    running_total = 0\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        spk_in = rate_encode_cifar(x, T=T, rate_scale=rate_scale)\n",
        "        spk_out = model(spk_in)\n",
        "\n",
        "        logits = spk_out.sum(dim=0)  # [B,10]\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * x.size(0)\n",
        "        running_correct += (logits.argmax(dim=1) == y).sum().item()\n",
        "        running_total += x.size(0)\n",
        "\n",
        "    scheduler.step()\n",
        "    test_acc = evaluate()\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | \"\n",
        "          f\"train loss {running_loss/running_total:.4f} | \"\n",
        "          f\"train acc {running_correct/running_total:.4f} | \"\n",
        "          f\"test acc {test_acc:.4f}\")\n",
        "\n",
        "print(\"Done.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGxPiKsw-LKY"
      },
      "source": [
        "2. TTFS Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b72FTML1-NLb",
        "outputId": "74c76526-20c3-46e6-d575-fc56a217f036"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building intensity CDF from training data ...\n",
            "CDF built.\n",
            "TTFS spike-count per input (min,max) = 1.0 1.0\n",
            "TTFS time histogram (first 10 bins): [0.014630635268986225, 0.0406595878303051, 0.0410257987678051, 0.0372314453125, 0.0333506278693676, 0.03489939495921135, 0.03621165081858635, 0.036590576171875, 0.0348917655646801, 0.03798167034983635]\n",
            "TTFS time histogram (last  10 bins): [0.02952066995203495, 0.03033447265625, 0.03035481832921505, 0.02818807028234005, 0.03160349652171135, 0.1603902131319046, 0.0, 0.0, 0.0, 0.0]\n",
            "Epoch 01 | train loss 1.8287 | train acc 0.3562 | test acc 0.1573 | best 0.1573\n",
            "Epoch 02 | train loss 1.6114 | train acc 0.4832 | test acc 0.1928 | best 0.1928\n",
            "Epoch 03 | train loss 1.5061 | train acc 0.5427 | test acc 0.2610 | best 0.2610\n",
            "Epoch 04 | train loss 1.4415 | train acc 0.5779 | test acc 0.2831 | best 0.2831\n",
            "Epoch 05 | train loss 1.3876 | train acc 0.6059 | test acc 0.1777 | best 0.2831\n",
            "Epoch 06 | train loss 1.3381 | train acc 0.6305 | test acc 0.2848 | best 0.2848\n",
            "Epoch 07 | train loss 1.2972 | train acc 0.6513 | test acc 0.2258 | best 0.2848\n",
            "Epoch 08 | train loss 1.2494 | train acc 0.6766 | test acc 0.2847 | best 0.2848\n",
            "Epoch 09 | train loss 1.2168 | train acc 0.6931 | test acc 0.3404 | best 0.3404\n",
            "Epoch 10 | train loss 1.1794 | train acc 0.7114 | test acc 0.4023 | best 0.4023\n",
            "Epoch 11 | train loss 1.1489 | train acc 0.7253 | test acc 0.3446 | best 0.4023\n",
            "Epoch 12 | train loss 1.1153 | train acc 0.7414 | test acc 0.3612 | best 0.4023\n",
            "Epoch 13 | train loss 1.0897 | train acc 0.7561 | test acc 0.3665 | best 0.4023\n",
            "Epoch 14 | train loss 1.0691 | train acc 0.7643 | test acc 0.3708 | best 0.4023\n",
            "Epoch 15 | train loss 1.0480 | train acc 0.7764 | test acc 0.5161 | best 0.5161\n",
            "Epoch 16 | train loss 1.0258 | train acc 0.7863 | test acc 0.4043 | best 0.5161\n",
            "Epoch 17 | train loss 1.0044 | train acc 0.7961 | test acc 0.4339 | best 0.5161\n",
            "Epoch 18 | train loss 0.9847 | train acc 0.8048 | test acc 0.4504 | best 0.5161\n",
            "Epoch 19 | train loss 0.9693 | train acc 0.8137 | test acc 0.4015 | best 0.5161\n",
            "Epoch 20 | train loss 0.9483 | train acc 0.8228 | test acc 0.5279 | best 0.5279\n",
            "Epoch 21 | train loss 0.9305 | train acc 0.8313 | test acc 0.4612 | best 0.5279\n",
            "Epoch 22 | train loss 0.9190 | train acc 0.8377 | test acc 0.3671 | best 0.5279\n",
            "Epoch 23 | train loss 0.9083 | train acc 0.8429 | test acc 0.3773 | best 0.5279\n",
            "Epoch 24 | train loss 0.8946 | train acc 0.8493 | test acc 0.4585 | best 0.5279\n",
            "Epoch 25 | train loss 0.8812 | train acc 0.8556 | test acc 0.4647 | best 0.5279\n",
            "Epoch 26 | train loss 0.8673 | train acc 0.8631 | test acc 0.4158 | best 0.5279\n",
            "Epoch 27 | train loss 0.8518 | train acc 0.8700 | test acc 0.4517 | best 0.5279\n",
            "Epoch 28 | train loss 0.8424 | train acc 0.8741 | test acc 0.5361 | best 0.5361\n",
            "Epoch 29 | train loss 0.8335 | train acc 0.8802 | test acc 0.4493 | best 0.5361\n",
            "Epoch 30 | train loss 0.8193 | train acc 0.8868 | test acc 0.5406 | best 0.5406\n",
            "Epoch 31 | train loss 0.8096 | train acc 0.8920 | test acc 0.4044 | best 0.5406\n",
            "Epoch 32 | train loss 0.8007 | train acc 0.8958 | test acc 0.4833 | best 0.5406\n",
            "Epoch 33 | train loss 0.7915 | train acc 0.8997 | test acc 0.3847 | best 0.5406\n",
            "Epoch 34 | train loss 0.7785 | train acc 0.9068 | test acc 0.4631 | best 0.5406\n",
            "Epoch 35 | train loss 0.7700 | train acc 0.9113 | test acc 0.4996 | best 0.5406\n",
            "Epoch 36 | train loss 0.7632 | train acc 0.9150 | test acc 0.4802 | best 0.5406\n",
            "Epoch 37 | train loss 0.7539 | train acc 0.9203 | test acc 0.4714 | best 0.5406\n",
            "Epoch 38 | train loss 0.7469 | train acc 0.9222 | test acc 0.4749 | best 0.5406\n",
            "Epoch 39 | train loss 0.7370 | train acc 0.9283 | test acc 0.5244 | best 0.5406\n",
            "Epoch 40 | train loss 0.7296 | train acc 0.9315 | test acc 0.4566 | best 0.5406\n",
            "Epoch 41 | train loss 0.7204 | train acc 0.9353 | test acc 0.5202 | best 0.5406\n",
            "Epoch 42 | train loss 0.7146 | train acc 0.9393 | test acc 0.4866 | best 0.5406\n",
            "Epoch 43 | train loss 0.7089 | train acc 0.9429 | test acc 0.5408 | best 0.5408\n",
            "Epoch 44 | train loss 0.7029 | train acc 0.9467 | test acc 0.4858 | best 0.5408\n",
            "Epoch 45 | train loss 0.6968 | train acc 0.9484 | test acc 0.5290 | best 0.5408\n",
            "Epoch 46 | train loss 0.6894 | train acc 0.9521 | test acc 0.5051 | best 0.5408\n",
            "Epoch 47 | train loss 0.6843 | train acc 0.9547 | test acc 0.5297 | best 0.5408\n",
            "Epoch 48 | train loss 0.6793 | train acc 0.9572 | test acc 0.5267 | best 0.5408\n",
            "Epoch 49 | train loss 0.6739 | train acc 0.9599 | test acc 0.5707 | best 0.5707\n",
            "Epoch 50 | train loss 0.6681 | train acc 0.9630 | test acc 0.4865 | best 0.5707\n",
            "Epoch 51 | train loss 0.6647 | train acc 0.9658 | test acc 0.5220 | best 0.5707\n",
            "Epoch 52 | train loss 0.6590 | train acc 0.9676 | test acc 0.5299 | best 0.5707\n",
            "Epoch 53 | train loss 0.6533 | train acc 0.9719 | test acc 0.5553 | best 0.5707\n",
            "Epoch 54 | train loss 0.6502 | train acc 0.9714 | test acc 0.5202 | best 0.5707\n",
            "Epoch 55 | train loss 0.6463 | train acc 0.9734 | test acc 0.5415 | best 0.5707\n",
            "Epoch 56 | train loss 0.6437 | train acc 0.9750 | test acc 0.5153 | best 0.5707\n",
            "Epoch 57 | train loss 0.6397 | train acc 0.9767 | test acc 0.5155 | best 0.5707\n",
            "Epoch 58 | train loss 0.6381 | train acc 0.9774 | test acc 0.5634 | best 0.5707\n",
            "Epoch 59 | train loss 0.6331 | train acc 0.9796 | test acc 0.5283 | best 0.5707\n",
            "Epoch 60 | train loss 0.6305 | train acc 0.9805 | test acc 0.5100 | best 0.5707\n",
            "Epoch 61 | train loss 0.6278 | train acc 0.9819 | test acc 0.5176 | best 0.5707\n",
            "Epoch 62 | train loss 0.6274 | train acc 0.9825 | test acc 0.5492 | best 0.5707\n",
            "Epoch 63 | train loss 0.6243 | train acc 0.9837 | test acc 0.5150 | best 0.5707\n",
            "Epoch 64 | train loss 0.6217 | train acc 0.9845 | test acc 0.4831 | best 0.5707\n",
            "Epoch 65 | train loss 0.6200 | train acc 0.9857 | test acc 0.5463 | best 0.5707\n",
            "Epoch 66 | train loss 0.6184 | train acc 0.9851 | test acc 0.5666 | best 0.5707\n",
            "Epoch 67 | train loss 0.6174 | train acc 0.9862 | test acc 0.5150 | best 0.5707\n",
            "Epoch 68 | train loss 0.6148 | train acc 0.9872 | test acc 0.5591 | best 0.5707\n",
            "Epoch 69 | train loss 0.6136 | train acc 0.9871 | test acc 0.5138 | best 0.5707\n",
            "Epoch 70 | train loss 0.6125 | train acc 0.9879 | test acc 0.5173 | best 0.5707\n",
            "Epoch 71 | train loss 0.6121 | train acc 0.9881 | test acc 0.5517 | best 0.5707\n",
            "Epoch 72 | train loss 0.6109 | train acc 0.9887 | test acc 0.5509 | best 0.5707\n",
            "Epoch 73 | train loss 0.6095 | train acc 0.9896 | test acc 0.5508 | best 0.5707\n",
            "Epoch 74 | train loss 0.6091 | train acc 0.9898 | test acc 0.5364 | best 0.5707\n",
            "Epoch 75 | train loss 0.6092 | train acc 0.9886 | test acc 0.5801 | best 0.5801\n",
            "Epoch 76 | train loss 0.6079 | train acc 0.9892 | test acc 0.5515 | best 0.5801\n",
            "Epoch 77 | train loss 0.6076 | train acc 0.9894 | test acc 0.5443 | best 0.5801\n",
            "Epoch 78 | train loss 0.6073 | train acc 0.9896 | test acc 0.5396 | best 0.5801\n",
            "Epoch 79 | train loss 0.6079 | train acc 0.9898 | test acc 0.5201 | best 0.5801\n",
            "Epoch 80 | train loss 0.6075 | train acc 0.9892 | test acc 0.5401 | best 0.5801\n",
            "Done. Best test acc = 0.5801. Saved to /content/best_ttfs_resnet_snn.pt\n"
          ]
        }
      ],
      "source": [
        "# Stronger SNN for CIFAR-10 with TTFS (latency) encoding (NON-hard)\n",
        "# Changes (ALL added as you requested):\n",
        "# 1) threshold_latency lowered: 0.01 -> 0.001  (reduce tail saturation / late-bin pile-up)\n",
        "# 2) readout_mode = \"mean\" (average membrane logits across time to reduce test variance)\n",
        "# 3) label smoothing in CE loss: label_smoothing=0.1 (reduce overfit, stabilize generalization)\n",
        "# 4) Save best checkpoint by test accuracy (best_ttfs_resnet_snn.pt)\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import snntorch as snn\n",
        "from snntorch import spikegen\n",
        "\n",
        "# -----------------------------\n",
        "# Config\n",
        "# -----------------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "batch_size = 128\n",
        "num_epochs = 80\n",
        "lr = 2e-3\n",
        "weight_decay = 5e-4\n",
        "\n",
        "T = 30  # encoding window\n",
        "\n",
        "# CDF equalization settings\n",
        "num_bins = 512\n",
        "max_cdf_batches = 300\n",
        "\n",
        "# Latency settings (affect time spread)\n",
        "normalize_in_latency = True\n",
        "linear_latency = True\n",
        "tau_latency = 8.0\n",
        "threshold_latency = 0.001  # ✅ CHANGED: was 0.01\n",
        "\n",
        "# Neuron dynamics\n",
        "tau_neuron = 2.0\n",
        "beta = torch.exp(torch.tensor(-1.0 / tau_neuron)).item()\n",
        "\n",
        "# ✅ CHANGED: use mean membrane logits across time\n",
        "readout_mode = \"mean\"  # \"last\" or \"mean\"\n",
        "\n",
        "# ✅ CHANGED: label smoothing to stabilize generalization\n",
        "label_smoothing = 0.1\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "# ✅ NEW: checkpoint\n",
        "ckpt_path = \"best_ttfs_resnet_snn.pt\"\n",
        "\n",
        "# -----------------------------\n",
        "# Data (CIFAR-10)\n",
        "# -----------------------------\n",
        "cifar_mean = (0.4914, 0.4822, 0.4465)\n",
        "cifar_std  = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(cifar_mean, cifar_std),\n",
        "])\n",
        "\n",
        "test_tf = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(cifar_mean, cifar_std),\n",
        "])\n",
        "\n",
        "train_set = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=train_tf)\n",
        "test_set  = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=test_tf)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_set,  batch_size=batch_size, shuffle=False, drop_last=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# -----------------------------\n",
        "# CDF building + Equalized TTFS encoder (still uses spikegen.latency)\n",
        "# -----------------------------\n",
        "_cifar_mean_t = torch.tensor(cifar_mean).view(1, 3, 1, 1)\n",
        "_cifar_std_t  = torch.tensor(cifar_std).view(1, 3, 1, 1)\n",
        "\n",
        "@torch.no_grad()\n",
        "def build_cdf_from_trainloader(train_loader, num_bins=512, max_batches=300, device=\"cuda\"):\n",
        "    hist = torch.zeros(num_bins, device=device)\n",
        "    mean = _cifar_mean_t.to(device)\n",
        "    std  = _cifar_std_t.to(device)\n",
        "\n",
        "    for i, (x_norm, _) in enumerate(train_loader):\n",
        "        if max_batches is not None and i >= max_batches:\n",
        "            break\n",
        "\n",
        "        x_norm = x_norm.to(device, non_blocking=True)\n",
        "        x_raw = (x_norm * std + mean).clamp(0.0, 1.0)\n",
        "\n",
        "        v = x_raw.flatten()\n",
        "        hist += torch.histc(v, bins=num_bins, min=0.0, max=1.0)\n",
        "\n",
        "    hist = hist / (hist.sum() + 1e-12)\n",
        "    cdf = torch.cumsum(hist, dim=0)\n",
        "    bin_edges = torch.linspace(0.0, 1.0, steps=num_bins + 1, device=device)\n",
        "    return bin_edges, cdf\n",
        "\n",
        "@torch.no_grad()\n",
        "def ttfs_encode_cifar_equalized_latency(\n",
        "    x_img_norm, T, bin_edges, cdf,\n",
        "    normalize=True, linear=True, tau=8.0, threshold=0.001\n",
        "):\n",
        "    device = x_img_norm.device\n",
        "    mean = _cifar_mean_t.to(device)\n",
        "    std  = _cifar_std_t.to(device)\n",
        "\n",
        "    x_raw = (x_img_norm * std + mean).clamp(0.0, 1.0)\n",
        "\n",
        "    nb = cdf.numel()\n",
        "    idx = torch.bucketize(x_raw, bin_edges[1:-1], right=False).clamp(0, nb - 1)\n",
        "    u = cdf[idx].clamp(1e-4, 1.0 - 1e-4)\n",
        "\n",
        "    spk = spikegen.latency(\n",
        "        u,\n",
        "        num_steps=T,\n",
        "        normalize=normalize,\n",
        "        linear=linear,\n",
        "        tau=tau,\n",
        "        threshold=threshold\n",
        "    )\n",
        "    return spk  # [T,B,3,32,32]\n",
        "\n",
        "print(\"Building intensity CDF from training data ...\")\n",
        "bin_edges, cdf = build_cdf_from_trainloader(\n",
        "    train_loader, num_bins=num_bins, max_batches=max_cdf_batches, device=device\n",
        ")\n",
        "print(\"CDF built.\")\n",
        "\n",
        "# -----------------------------\n",
        "# Spiking ResNet-style blocks\n",
        "# -----------------------------\n",
        "class SpkBasicBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual block:\n",
        "      x -> Conv-BN -> LIF -> Conv-BN -> (+ shortcut) -> LIF\n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch, out_ch, stride=1, beta=0.95):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1   = nn.BatchNorm2d(out_ch)\n",
        "        self.lif1  = snn.Leaky(beta=beta)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, stride=1, padding=1, bias=False)\n",
        "        self.bn2   = nn.BatchNorm2d(out_ch)\n",
        "        self.lif2  = snn.Leaky(beta=beta)\n",
        "\n",
        "        if stride != 1 or in_ch != out_ch:\n",
        "            self.short = nn.Sequential(\n",
        "                nn.Conv2d(in_ch, out_ch, 1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_ch),\n",
        "            )\n",
        "        else:\n",
        "            self.short = None\n",
        "\n",
        "    def init_state(self):\n",
        "        return self.lif1.init_leaky(), self.lif2.init_leaky()\n",
        "\n",
        "    def forward_step(self, x_spk, mem1, mem2):\n",
        "        out = self.conv1(x_spk)\n",
        "        out = self.bn1(out)\n",
        "        spk1, mem1 = self.lif1(out, mem1)\n",
        "\n",
        "        out = self.conv2(spk1)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        skip = x_spk if self.short is None else self.short(x_spk)\n",
        "        out = out + skip\n",
        "\n",
        "        spk2, mem2 = self.lif2(out, mem2)\n",
        "        return spk2, mem1, mem2\n",
        "\n",
        "class SpikingResNetCIFAR(nn.Module):\n",
        "    \"\"\"\n",
        "    Stem -> [2 blocks @64] -> [2 blocks @128, downsample] -> [2 blocks @256, downsample]\n",
        "    -> global avgpool -> FC -> output LIF -> logits from membrane (last or mean)\n",
        "    \"\"\"\n",
        "    def __init__(self, beta=0.95, num_classes=10):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv0 = nn.Conv2d(3, 64, 3, stride=1, padding=1, bias=False)\n",
        "        self.bn0   = nn.BatchNorm2d(64)\n",
        "        self.lif0  = snn.Leaky(beta=beta)\n",
        "\n",
        "        self.b1_0 = SpkBasicBlock(64, 64, stride=1, beta=beta)\n",
        "        self.b1_1 = SpkBasicBlock(64, 64, stride=1, beta=beta)\n",
        "\n",
        "        self.b2_0 = SpkBasicBlock(64, 128, stride=2, beta=beta)   # 32->16\n",
        "        self.b2_1 = SpkBasicBlock(128, 128, stride=1, beta=beta)\n",
        "\n",
        "        self.b3_0 = SpkBasicBlock(128, 256, stride=2, beta=beta)  # 16->8\n",
        "        self.b3_1 = SpkBasicBlock(256, 256, stride=1, beta=beta)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.drop = nn.Dropout(p=0.3)\n",
        "\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "        self.lif_out = snn.Leaky(beta=beta)\n",
        "\n",
        "    def forward(self, spk_in, readout_mode=\"mean\"):\n",
        "        Tsteps, B, _, _, _ = spk_in.shape\n",
        "\n",
        "        mem0 = self.lif0.init_leaky()\n",
        "\n",
        "        m10, m11 = self.b1_0.init_state()\n",
        "        m12, m13 = self.b1_1.init_state()\n",
        "\n",
        "        m20, m21 = self.b2_0.init_state()\n",
        "        m22, m23 = self.b2_1.init_state()\n",
        "\n",
        "        m30, m31 = self.b3_0.init_state()\n",
        "        m32, m33 = self.b3_1.init_state()\n",
        "\n",
        "        mem_out = self.lif_out.init_leaky()\n",
        "        mem_logits_rec = []\n",
        "\n",
        "        for t in range(Tsteps):\n",
        "            x = spk_in[t]\n",
        "\n",
        "            x = self.conv0(x)\n",
        "            x = self.bn0(x)\n",
        "            x, mem0 = self.lif0(x, mem0)\n",
        "\n",
        "            x, m10, m11 = self.b1_0.forward_step(x, m10, m11)\n",
        "            x, m12, m13 = self.b1_1.forward_step(x, m12, m13)\n",
        "\n",
        "            x, m20, m21 = self.b2_0.forward_step(x, m20, m21)\n",
        "            x, m22, m23 = self.b2_1.forward_step(x, m22, m23)\n",
        "\n",
        "            x, m30, m31 = self.b3_0.forward_step(x, m30, m31)\n",
        "            x, m32, m33 = self.b3_1.forward_step(x, m32, m33)\n",
        "\n",
        "            x = self.avgpool(x).view(B, -1)\n",
        "            x = self.drop(x)\n",
        "            x = self.fc(x)\n",
        "\n",
        "            _, mem_out = self.lif_out(x, mem_out)\n",
        "            mem_logits_rec.append(mem_out)\n",
        "\n",
        "        mem_logits_rec = torch.stack(mem_logits_rec, dim=0)  # [T,B,10]\n",
        "        if readout_mode == \"last\":\n",
        "            return mem_logits_rec[-1]\n",
        "        return mem_logits_rec.mean(dim=0)  # ✅ mean by default\n",
        "\n",
        "# -----------------------------\n",
        "# Model / Optim\n",
        "# -----------------------------\n",
        "model = SpikingResNetCIFAR(beta=beta, num_classes=num_classes).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "# -----------------------------\n",
        "# Sanity check: spike count + time histogram\n",
        "# -----------------------------\n",
        "@torch.no_grad()\n",
        "def sanity_check_ttfs():\n",
        "    x, _ = next(iter(train_loader))\n",
        "    x = x.to(device)\n",
        "\n",
        "    spk = ttfs_encode_cifar_equalized_latency(\n",
        "        x, T=T, bin_edges=bin_edges, cdf=cdf,\n",
        "        normalize=normalize_in_latency,\n",
        "        linear=linear_latency,\n",
        "        tau=tau_latency,\n",
        "        threshold=threshold_latency\n",
        "    )\n",
        "\n",
        "    counts = spk.sum(dim=0)\n",
        "    print(\"TTFS spike-count per input (min,max) =\", counts.min().item(), counts.max().item())\n",
        "\n",
        "    t_spike = spk.argmax(dim=0).flatten()\n",
        "    hist = torch.bincount(t_spike, minlength=T).float()\n",
        "    hist = hist / hist.sum()\n",
        "    print(\"TTFS time histogram (first 10 bins):\", [float(h) for h in hist[:10]])\n",
        "    print(\"TTFS time histogram (last  10 bins):\", [float(h) for h in hist[-10:]])\n",
        "\n",
        "sanity_check_ttfs()\n",
        "\n",
        "# -----------------------------\n",
        "# Train / Eval + Best checkpoint\n",
        "# -----------------------------\n",
        "@torch.no_grad()\n",
        "def evaluate():\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    for x, y in test_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        spk_in = ttfs_encode_cifar_equalized_latency(\n",
        "            x, T=T, bin_edges=bin_edges, cdf=cdf,\n",
        "            normalize=normalize_in_latency,\n",
        "            linear=linear_latency,\n",
        "            tau=tau_latency,\n",
        "            threshold=threshold_latency\n",
        "        )\n",
        "\n",
        "        logits = model(spk_in, readout_mode=readout_mode)\n",
        "        pred = logits.argmax(dim=1)\n",
        "\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.numel()\n",
        "    return correct / max(total, 1)\n",
        "\n",
        "best_acc = -1.0\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    running_loss, running_correct, running_total = 0.0, 0, 0\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        spk_in = ttfs_encode_cifar_equalized_latency(\n",
        "            x, T=T, bin_edges=bin_edges, cdf=cdf,\n",
        "            normalize=normalize_in_latency,\n",
        "            linear=linear_latency,\n",
        "            tau=tau_latency,\n",
        "            threshold=threshold_latency\n",
        "        )\n",
        "\n",
        "        logits = model(spk_in, readout_mode=readout_mode)\n",
        "\n",
        "        # ✅ CHANGED: label smoothing\n",
        "        loss = F.cross_entropy(logits, y, label_smoothing=label_smoothing)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * x.size(0)\n",
        "        running_correct += (logits.argmax(dim=1) == y).sum().item()\n",
        "        running_total += x.size(0)\n",
        "\n",
        "    scheduler.step()\n",
        "    test_acc = evaluate()\n",
        "\n",
        "    # ✅ NEW: save best\n",
        "    if test_acc > best_acc:\n",
        "        best_acc = test_acc\n",
        "        torch.save(\n",
        "            {\n",
        "                \"epoch\": epoch,\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"test_acc\": test_acc,\n",
        "                \"config\": {\n",
        "                    \"T\": T,\n",
        "                    \"tau_latency\": tau_latency,\n",
        "                    \"threshold_latency\": threshold_latency,\n",
        "                    \"normalize_in_latency\": normalize_in_latency,\n",
        "                    \"linear_latency\": linear_latency,\n",
        "                    \"tau_neuron\": tau_neuron,\n",
        "                    \"beta\": beta,\n",
        "                    \"readout_mode\": readout_mode,\n",
        "                    \"label_smoothing\": label_smoothing,\n",
        "                }\n",
        "            },\n",
        "            ckpt_path\n",
        "        )\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | \"\n",
        "          f\"train loss {running_loss/running_total:.4f} | \"\n",
        "          f\"train acc {running_correct/running_total:.4f} | \"\n",
        "          f\"test acc {test_acc:.4f} | \"\n",
        "          f\"best {best_acc:.4f}\")\n",
        "\n",
        "print(f\"Done. Best test acc = {best_acc:.4f}. Saved to {os.path.abspath(ckpt_path)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vln_pZJH5CnU"
      },
      "source": [
        "3. ISI Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8_hLDp15GMp",
        "outputId": "7a964fd5-c9dc-4b58-fbe2-33c5e889cf06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ISI encoder sanity check (T=30, K=4) ...\n",
            "ISI spike-count per input (min,max) = 4.0 4.0\n",
            "ISI time histogram (first 10 bins): [0.16699981689453125, 0.14074642956256866, 0.13408024609088898, 0.051362354308366776, 0.022551218047738075, 0.0182367954403162, 0.015087127685546875, 0.013635635375976562, 0.009203593246638775, 0.0091705322265625]\n",
            "ISI time histogram (last  10 bins): [0.008059819228947163, 0.0074615478515625, 0.0105044050142169, 0.01173273753374815, 0.013326008804142475, 0.017501195892691612, 0.017867406830191612, 0.07789293676614761, 0.08326593786478043, 0.10898780822753906]\n",
            "Epoch 01 | train loss 2.2162 | train acc 0.1514 | test acc 0.1994 | best 0.1994\n",
            "Epoch 02 | train loss 2.0496 | train acc 0.2236 | test acc 0.1994 | best 0.1994\n",
            "Epoch 03 | train loss 2.0144 | train acc 0.2481 | test acc 0.2686 | best 0.2686\n",
            "Epoch 04 | train loss 1.9860 | train acc 0.2704 | test acc 0.2349 | best 0.2686\n",
            "Epoch 05 | train loss 1.9396 | train acc 0.2994 | test acc 0.1969 | best 0.2686\n",
            "Epoch 06 | train loss 1.8814 | train acc 0.3315 | test acc 0.3721 | best 0.3721\n",
            "Epoch 07 | train loss 1.8478 | train acc 0.3510 | test acc 0.3606 | best 0.3721\n",
            "Epoch 08 | train loss 1.8166 | train acc 0.3689 | test acc 0.3838 | best 0.3838\n",
            "Epoch 09 | train loss 1.7817 | train acc 0.3905 | test acc 0.4176 | best 0.4176\n",
            "Epoch 10 | train loss 1.7618 | train acc 0.4019 | test acc 0.3895 | best 0.4176\n",
            "Epoch 11 | train loss 1.7284 | train acc 0.4205 | test acc 0.4540 | best 0.4540\n",
            "Epoch 12 | train loss 1.6938 | train acc 0.4427 | test acc 0.4250 | best 0.4540\n",
            "Epoch 13 | train loss 1.6702 | train acc 0.4529 | test acc 0.4793 | best 0.4793\n",
            "Epoch 14 | train loss 1.6316 | train acc 0.4731 | test acc 0.4372 | best 0.4793\n",
            "Epoch 15 | train loss 1.6136 | train acc 0.4857 | test acc 0.4794 | best 0.4794\n",
            "Epoch 16 | train loss 1.5672 | train acc 0.5083 | test acc 0.5420 | best 0.5420\n",
            "Epoch 17 | train loss 1.5389 | train acc 0.5232 | test acc 0.5511 | best 0.5511\n",
            "Epoch 18 | train loss 1.5132 | train acc 0.5374 | test acc 0.5117 | best 0.5511\n",
            "Epoch 19 | train loss 1.4745 | train acc 0.5553 | test acc 0.5600 | best 0.5600\n",
            "Epoch 20 | train loss 1.4508 | train acc 0.5667 | test acc 0.5749 | best 0.5749\n",
            "Epoch 21 | train loss 1.4215 | train acc 0.5807 | test acc 0.6047 | best 0.6047\n",
            "Epoch 22 | train loss 1.4004 | train acc 0.5929 | test acc 0.5714 | best 0.6047\n",
            "Epoch 23 | train loss 1.3765 | train acc 0.6035 | test acc 0.6121 | best 0.6121\n",
            "Epoch 24 | train loss 1.3526 | train acc 0.6141 | test acc 0.6256 | best 0.6256\n",
            "Epoch 25 | train loss 1.3307 | train acc 0.6241 | test acc 0.6314 | best 0.6314\n",
            "Epoch 26 | train loss 1.3144 | train acc 0.6336 | test acc 0.6189 | best 0.6314\n",
            "Epoch 27 | train loss 1.2948 | train acc 0.6456 | test acc 0.6329 | best 0.6329\n",
            "Epoch 28 | train loss 1.2784 | train acc 0.6536 | test acc 0.6573 | best 0.6573\n",
            "Epoch 29 | train loss 1.2629 | train acc 0.6587 | test acc 0.6500 | best 0.6573\n",
            "Epoch 30 | train loss 1.2483 | train acc 0.6666 | test acc 0.6775 | best 0.6775\n",
            "Epoch 31 | train loss 1.2276 | train acc 0.6770 | test acc 0.6799 | best 0.6799\n",
            "Epoch 32 | train loss 1.2105 | train acc 0.6856 | test acc 0.6833 | best 0.6833\n",
            "Epoch 33 | train loss 1.1887 | train acc 0.6954 | test acc 0.6855 | best 0.6855\n",
            "Epoch 34 | train loss 1.1703 | train acc 0.7026 | test acc 0.7082 | best 0.7082\n",
            "Epoch 35 | train loss 1.1572 | train acc 0.7087 | test acc 0.6844 | best 0.7082\n",
            "Epoch 36 | train loss 1.1390 | train acc 0.7204 | test acc 0.6985 | best 0.7082\n",
            "Epoch 37 | train loss 1.1236 | train acc 0.7272 | test acc 0.7280 | best 0.7280\n",
            "Epoch 38 | train loss 1.1046 | train acc 0.7364 | test acc 0.6833 | best 0.7280\n",
            "Epoch 39 | train loss 1.0944 | train acc 0.7418 | test acc 0.7198 | best 0.7280\n",
            "Epoch 40 | train loss 1.0852 | train acc 0.7460 | test acc 0.7300 | best 0.7300\n",
            "Epoch 41 | train loss 1.0660 | train acc 0.7556 | test acc 0.7362 | best 0.7362\n",
            "Epoch 42 | train loss 1.0565 | train acc 0.7587 | test acc 0.7433 | best 0.7433\n",
            "Epoch 43 | train loss 1.0430 | train acc 0.7654 | test acc 0.7465 | best 0.7465\n",
            "Epoch 44 | train loss 1.0307 | train acc 0.7710 | test acc 0.7450 | best 0.7465\n",
            "Epoch 45 | train loss 1.0133 | train acc 0.7798 | test acc 0.7570 | best 0.7570\n",
            "Epoch 46 | train loss 1.0068 | train acc 0.7816 | test acc 0.7632 | best 0.7632\n",
            "Epoch 47 | train loss 1.0001 | train acc 0.7873 | test acc 0.7487 | best 0.7632\n",
            "Epoch 48 | train loss 0.9874 | train acc 0.7932 | test acc 0.7534 | best 0.7632\n",
            "Epoch 49 | train loss 0.9764 | train acc 0.7974 | test acc 0.7702 | best 0.7702\n",
            "Epoch 50 | train loss 0.9653 | train acc 0.8041 | test acc 0.7748 | best 0.7748\n",
            "Epoch 51 | train loss 0.9579 | train acc 0.8060 | test acc 0.7747 | best 0.7748\n",
            "Epoch 52 | train loss 0.9462 | train acc 0.8113 | test acc 0.7793 | best 0.7793\n",
            "Epoch 53 | train loss 0.9360 | train acc 0.8166 | test acc 0.7818 | best 0.7818\n",
            "Epoch 54 | train loss 0.9282 | train acc 0.8195 | test acc 0.7814 | best 0.7818\n",
            "Epoch 55 | train loss 0.9223 | train acc 0.8225 | test acc 0.7724 | best 0.7818\n",
            "Epoch 56 | train loss 0.9143 | train acc 0.8274 | test acc 0.7779 | best 0.7818\n",
            "Epoch 57 | train loss 0.9068 | train acc 0.8302 | test acc 0.7925 | best 0.7925\n",
            "Epoch 58 | train loss 0.9012 | train acc 0.8334 | test acc 0.7894 | best 0.7925\n",
            "Epoch 59 | train loss 0.8958 | train acc 0.8341 | test acc 0.7922 | best 0.7925\n",
            "Epoch 60 | train loss 0.8839 | train acc 0.8415 | test acc 0.7884 | best 0.7925\n",
            "Epoch 61 | train loss 0.8817 | train acc 0.8434 | test acc 0.7947 | best 0.7947\n",
            "Epoch 62 | train loss 0.8754 | train acc 0.8457 | test acc 0.7896 | best 0.7947\n",
            "Epoch 63 | train loss 0.8696 | train acc 0.8493 | test acc 0.7948 | best 0.7948\n",
            "Epoch 64 | train loss 0.8654 | train acc 0.8502 | test acc 0.7958 | best 0.7958\n",
            "Epoch 65 | train loss 0.8594 | train acc 0.8551 | test acc 0.7973 | best 0.7973\n",
            "Epoch 66 | train loss 0.8576 | train acc 0.8553 | test acc 0.7996 | best 0.7996\n",
            "Epoch 67 | train loss 0.8521 | train acc 0.8550 | test acc 0.8008 | best 0.8008\n",
            "Epoch 68 | train loss 0.8485 | train acc 0.8598 | test acc 0.7967 | best 0.8008\n",
            "Epoch 69 | train loss 0.8437 | train acc 0.8613 | test acc 0.8014 | best 0.8014\n",
            "Epoch 70 | train loss 0.8426 | train acc 0.8616 | test acc 0.7989 | best 0.8014\n",
            "Epoch 71 | train loss 0.8377 | train acc 0.8644 | test acc 0.8021 | best 0.8021\n",
            "Epoch 72 | train loss 0.8370 | train acc 0.8650 | test acc 0.8054 | best 0.8054\n",
            "Epoch 73 | train loss 0.8335 | train acc 0.8661 | test acc 0.8017 | best 0.8054\n",
            "Epoch 74 | train loss 0.8326 | train acc 0.8676 | test acc 0.8028 | best 0.8054\n",
            "Epoch 75 | train loss 0.8304 | train acc 0.8699 | test acc 0.8065 | best 0.8065\n",
            "Epoch 76 | train loss 0.8264 | train acc 0.8705 | test acc 0.8018 | best 0.8065\n",
            "Epoch 77 | train loss 0.8283 | train acc 0.8696 | test acc 0.8050 | best 0.8065\n",
            "Epoch 78 | train loss 0.8288 | train acc 0.8695 | test acc 0.8009 | best 0.8065\n",
            "Epoch 79 | train loss 0.8271 | train acc 0.8700 | test acc 0.8030 | best 0.8065\n",
            "Epoch 80 | train loss 0.8277 | train acc 0.8698 | test acc 0.8038 | best 0.8065\n",
            "Done. Best test acc = 0.8065. Saved to /content/best_isi_resnet_snn_GN_logitsreadout.pt\n"
          ]
        }
      ],
      "source": [
        "# CIFAR-10 Spiking ResNet-style SNN + fixed-K ISI (no-endcaps, strict K)\n",
        "# Stabilized version:\n",
        "#   - BatchNorm -> GroupNorm (much more stable for SNN time-unroll)\n",
        "#   - lr 2e-3 -> 1e-3\n",
        "#   - Output head: remove lif_out; readout uses mean/last of logits directly\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import snntorch as snn\n",
        "\n",
        "# -----------------------------\n",
        "# Config\n",
        "# -----------------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "batch_size = 128\n",
        "num_epochs = 80\n",
        "lr = 1e-3               # ✅ smaller\n",
        "weight_decay = 5e-4\n",
        "\n",
        "T = 30\n",
        "K = 4\n",
        "alpha_max = 2.0\n",
        "eps_q = 1e-3\n",
        "\n",
        "tau_neuron = 2.0\n",
        "beta = torch.exp(torch.tensor(-1.0 / tau_neuron)).item()\n",
        "\n",
        "readout_mode = \"mean\"\n",
        "label_smoothing = 0.1\n",
        "\n",
        "num_classes = 10\n",
        "ckpt_path = \"best_isi_resnet_snn_GN_logitsreadout.pt\"\n",
        "\n",
        "# -----------------------------\n",
        "# Data\n",
        "# -----------------------------\n",
        "cifar_mean = (0.4914, 0.4822, 0.4465)\n",
        "cifar_std  = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(cifar_mean, cifar_std),\n",
        "])\n",
        "\n",
        "test_tf = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(cifar_mean, cifar_std),\n",
        "])\n",
        "\n",
        "train_set = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=train_tf)\n",
        "test_set  = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=test_tf)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_set,  batch_size=batch_size, shuffle=False, drop_last=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "_cifar_mean_t = torch.tensor(cifar_mean).view(1, 3, 1, 1)\n",
        "_cifar_std_t  = torch.tensor(cifar_std).view(1, 3, 1, 1)\n",
        "\n",
        "@torch.no_grad()\n",
        "def cifar_to_unit_interval(x_norm: torch.Tensor) -> torch.Tensor:\n",
        "    mean = _cifar_mean_t.to(x_norm.device)\n",
        "    std  = _cifar_std_t.to(x_norm.device)\n",
        "    return (x_norm * std + mean).clamp(0.0, 1.0)\n",
        "\n",
        "# -----------------------------\n",
        "# Strict fixed-K encoder (from your working version)\n",
        "# -----------------------------\n",
        "@torch.no_grad()\n",
        "def isi_fixedK_no_endcaps_strict(\n",
        "    x_img_unit: torch.Tensor, T: int, K: int, alpha_max: float = 2.0, eps: float = 1e-3\n",
        ") -> torch.Tensor:\n",
        "    assert T >= 2 and K >= 1\n",
        "    if K > T:\n",
        "        raise ValueError(f\"K={K} must satisfy K<=T={T}.\")\n",
        "\n",
        "    device = x_img_unit.device\n",
        "    B = x_img_unit.size(0)\n",
        "    x = x_img_unit.view(B, -1).clamp(0.0, 1.0)\n",
        "    N = x.size(1)\n",
        "\n",
        "    M = T\n",
        "    j = torch.arange(M, device=device, dtype=torch.float32).view(1, 1, M)\n",
        "    mid = (M - 1) / 2.0\n",
        "\n",
        "    alpha = (x * 2.0 - 1.0) * alpha_max\n",
        "    alpha = alpha.unsqueeze(-1)\n",
        "\n",
        "    w = torch.exp(alpha * (j - mid))\n",
        "    w = w / (w.sum(dim=-1, keepdim=True) + 1e-12)\n",
        "    c = torch.cumsum(w, dim=-1)\n",
        "\n",
        "    q = torch.linspace(eps, 1.0 - eps, steps=K, device=device, dtype=torch.float32)\n",
        "    q = q.view(1, 1, K).expand(B, N, K)\n",
        "\n",
        "    t_idx = torch.searchsorted(c, q).clamp(0, T - 1).long()\n",
        "    t_idx, _ = torch.sort(t_idx, dim=-1)\n",
        "\n",
        "    used = torch.zeros(B, N, T, device=device, dtype=torch.bool)\n",
        "    t_fixed = torch.full_like(t_idx, -1)\n",
        "\n",
        "    for k in range(K):\n",
        "        tk = t_idx[..., k]\n",
        "        free = ~used.gather(dim=2, index=tk.unsqueeze(-1)).squeeze(-1)\n",
        "        t_fixed[..., k] = torch.where(free, tk, torch.full_like(tk, -1))\n",
        "        if free.any():\n",
        "            used[free] |= torch.nn.functional.one_hot(tk[free], num_classes=T).bool()\n",
        "\n",
        "    for k in range(K):\n",
        "        need = (t_fixed[..., k] < 0)\n",
        "        if not need.any():\n",
        "            continue\n",
        "\n",
        "        tk = t_idx[..., k].clone()\n",
        "        avail = ~used\n",
        "        ar = torch.arange(T, device=device).view(1, 1, T)\n",
        "\n",
        "        forward_mask = avail & (ar >= tk.unsqueeze(-1))\n",
        "        fwd_pos = forward_mask.float().argmax(dim=-1)\n",
        "        fwd_exists = forward_mask.any(dim=-1)\n",
        "\n",
        "        backward_mask = avail & (ar <= tk.unsqueeze(-1))\n",
        "        rev = torch.flip(backward_mask, dims=[-1])\n",
        "        bwd_pos_rev = rev.float().argmax(dim=-1)\n",
        "        bwd_pos = (T - 1) - bwd_pos_rev\n",
        "        bwd_exists = backward_mask.any(dim=-1)\n",
        "\n",
        "        chosen = torch.where(fwd_exists, fwd_pos, bwd_pos).long()\n",
        "        t_fixed[..., k] = torch.where(need, chosen, t_fixed[..., k])\n",
        "        used[need] |= torch.nn.functional.one_hot(chosen[need], num_classes=T).bool()\n",
        "\n",
        "    spk_flat = torch.zeros(T, B, N, device=device, dtype=torch.float32)\n",
        "    b_idx = torch.arange(B, device=device).view(B, 1, 1).expand(B, N, K)\n",
        "    n_idx = torch.arange(N, device=device).view(1, N, 1).expand(B, N, K)\n",
        "    spk_flat[t_fixed, b_idx, n_idx] = 1.0\n",
        "\n",
        "    return spk_flat.view(T, B, *x_img_unit.shape[1:])\n",
        "\n",
        "# -----------------------------\n",
        "# GN helper\n",
        "# -----------------------------\n",
        "def GN(ch, groups=16):\n",
        "    g = min(groups, ch)\n",
        "    while ch % g != 0 and g > 1:\n",
        "        g -= 1\n",
        "    return nn.GroupNorm(g, ch)\n",
        "\n",
        "# -----------------------------\n",
        "# Spiking ResNet-style blocks (BN -> GN)\n",
        "# -----------------------------\n",
        "class SpkBasicBlockGN(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, stride=1, beta=0.95):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, stride=stride, padding=1, bias=False)\n",
        "        self.gn1   = GN(out_ch)\n",
        "        self.lif1  = snn.Leaky(beta=beta)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, stride=1, padding=1, bias=False)\n",
        "        self.gn2   = GN(out_ch)\n",
        "        self.lif2  = snn.Leaky(beta=beta)\n",
        "\n",
        "        if stride != 1 or in_ch != out_ch:\n",
        "            self.short = nn.Sequential(\n",
        "                nn.Conv2d(in_ch, out_ch, 1, stride=stride, bias=False),\n",
        "                GN(out_ch),\n",
        "            )\n",
        "        else:\n",
        "            self.short = None\n",
        "\n",
        "    def init_state(self):\n",
        "        return self.lif1.init_leaky(), self.lif2.init_leaky()\n",
        "\n",
        "    def forward_step(self, x_spk, mem1, mem2):\n",
        "        out = self.conv1(x_spk)\n",
        "        out = self.gn1(out)\n",
        "        spk1, mem1 = self.lif1(out, mem1)\n",
        "\n",
        "        out = self.conv2(spk1)\n",
        "        out = self.gn2(out)\n",
        "\n",
        "        skip = x_spk if self.short is None else self.short(x_spk)\n",
        "        out = out + skip\n",
        "\n",
        "        spk2, mem2 = self.lif2(out, mem2)\n",
        "        return spk2, mem1, mem2\n",
        "\n",
        "class SpikingResNetCIFAR_GN(nn.Module):\n",
        "    def __init__(self, beta=0.95, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.conv0 = nn.Conv2d(3, 64, 3, stride=1, padding=1, bias=False)\n",
        "        self.gn0   = GN(64)\n",
        "        self.lif0  = snn.Leaky(beta=beta)\n",
        "\n",
        "        self.b1_0 = SpkBasicBlockGN(64, 64, stride=1, beta=beta)\n",
        "        self.b1_1 = SpkBasicBlockGN(64, 64, stride=1, beta=beta)\n",
        "\n",
        "        self.b2_0 = SpkBasicBlockGN(64, 128, stride=2, beta=beta)\n",
        "        self.b2_1 = SpkBasicBlockGN(128, 128, stride=1, beta=beta)\n",
        "\n",
        "        self.b3_0 = SpkBasicBlockGN(128, 256, stride=2, beta=beta)\n",
        "        self.b3_1 = SpkBasicBlockGN(256, 256, stride=1, beta=beta)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.drop = nn.Dropout(p=0.3)\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, spk_in, readout_mode=\"mean\"):\n",
        "        Tsteps, B, _, _, _ = spk_in.shape\n",
        "\n",
        "        mem0 = self.lif0.init_leaky()\n",
        "        m10, m11 = self.b1_0.init_state()\n",
        "        m12, m13 = self.b1_1.init_state()\n",
        "        m20, m21 = self.b2_0.init_state()\n",
        "        m22, m23 = self.b2_1.init_state()\n",
        "        m30, m31 = self.b3_0.init_state()\n",
        "        m32, m33 = self.b3_1.init_state()\n",
        "\n",
        "        logits_rec = []\n",
        "\n",
        "        for t in range(Tsteps):\n",
        "            x = spk_in[t]\n",
        "\n",
        "            x = self.conv0(x); x = self.gn0(x)\n",
        "            x, mem0 = self.lif0(x, mem0)\n",
        "\n",
        "            x, m10, m11 = self.b1_0.forward_step(x, m10, m11)\n",
        "            x, m12, m13 = self.b1_1.forward_step(x, m12, m13)\n",
        "\n",
        "            x, m20, m21 = self.b2_0.forward_step(x, m20, m21)\n",
        "            x, m22, m23 = self.b2_1.forward_step(x, m22, m23)\n",
        "\n",
        "            x, m30, m31 = self.b3_0.forward_step(x, m30, m31)\n",
        "            x, m32, m33 = self.b3_1.forward_step(x, m32, m33)\n",
        "\n",
        "            x = self.avgpool(x).view(B, -1)\n",
        "            x = self.drop(x)\n",
        "            logits = self.fc(x)            # ✅ direct logits (no output LIF)\n",
        "            logits_rec.append(logits)\n",
        "\n",
        "        logits_rec = torch.stack(logits_rec, dim=0)  # [T,B,10]\n",
        "        return logits_rec[-1] if readout_mode == \"last\" else logits_rec.mean(dim=0)\n",
        "\n",
        "# -----------------------------\n",
        "# Sanity check\n",
        "# -----------------------------\n",
        "@torch.no_grad()\n",
        "def sanity_check_isi():\n",
        "    x_norm, _ = next(iter(train_loader))\n",
        "    x_norm = x_norm.to(device)\n",
        "    x_unit = cifar_to_unit_interval(x_norm)\n",
        "\n",
        "    spk = isi_fixedK_no_endcaps_strict(x_unit, T=T, K=K, alpha_max=alpha_max, eps=eps_q)\n",
        "\n",
        "    counts = spk.sum(dim=0)\n",
        "    print(\"ISI spike-count per input (min,max) =\", counts.min().item(), counts.max().item())\n",
        "\n",
        "    t_all = spk.reshape(T, -1).sum(dim=1).float()\n",
        "    t_all = t_all / (t_all.sum() + 1e-12)\n",
        "    print(\"ISI time histogram (first 10 bins):\", [float(v) for v in t_all[:10]])\n",
        "    print(\"ISI time histogram (last  10 bins):\", [float(v) for v in t_all[-10:]])\n",
        "\n",
        "print(f\"ISI encoder sanity check (T={T}, K={K}) ...\")\n",
        "sanity_check_isi()\n",
        "\n",
        "# -----------------------------\n",
        "# Train / Eval\n",
        "# -----------------------------\n",
        "model = SpikingResNetCIFAR_GN(beta=beta, num_classes=num_classes).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate():\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    for x_norm, y in test_loader:\n",
        "        x_norm, y = x_norm.to(device), y.to(device)\n",
        "        x_unit = cifar_to_unit_interval(x_norm)\n",
        "        spk_in = isi_fixedK_no_endcaps_strict(x_unit, T=T, K=K, alpha_max=alpha_max, eps=eps_q)\n",
        "\n",
        "        logits = model(spk_in, readout_mode=readout_mode)\n",
        "        pred = logits.argmax(dim=1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.numel()\n",
        "    return correct / max(total, 1)\n",
        "\n",
        "best_acc = -1.0\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    running_loss, running_correct, running_total = 0.0, 0, 0\n",
        "\n",
        "    for x_norm, y in train_loader:\n",
        "        x_norm, y = x_norm.to(device), y.to(device)\n",
        "\n",
        "        x_unit = cifar_to_unit_interval(x_norm)\n",
        "        spk_in = isi_fixedK_no_endcaps_strict(x_unit, T=T, K=K, alpha_max=alpha_max, eps=eps_q)\n",
        "\n",
        "        logits = model(spk_in, readout_mode=readout_mode)\n",
        "        loss = F.cross_entropy(logits, y, label_smoothing=label_smoothing)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)  # ✅ slightly tighter\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * x_norm.size(0)\n",
        "        running_correct += (logits.argmax(dim=1) == y).sum().item()\n",
        "        running_total += x_norm.size(0)\n",
        "\n",
        "    scheduler.step()\n",
        "    test_acc = evaluate()\n",
        "\n",
        "    if test_acc > best_acc:\n",
        "        best_acc = test_acc\n",
        "        torch.save(\n",
        "            {\n",
        "                \"epoch\": epoch,\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"test_acc\": test_acc,\n",
        "                \"config\": {\n",
        "                    \"T\": T,\n",
        "                    \"K\": K,\n",
        "                    \"alpha_max\": alpha_max,\n",
        "                    \"eps_q\": eps_q,\n",
        "                    \"beta\": beta,\n",
        "                    \"readout_mode\": readout_mode,\n",
        "                    \"label_smoothing\": label_smoothing,\n",
        "                    \"lr\": lr,\n",
        "                }\n",
        "            },\n",
        "            ckpt_path\n",
        "        )\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | \"\n",
        "          f\"train loss {running_loss/running_total:.4f} | \"\n",
        "          f\"train acc {running_correct/running_total:.4f} | \"\n",
        "          f\"test acc {test_acc:.4f} | \"\n",
        "          f\"best {best_acc:.4f}\")\n",
        "\n",
        "print(f\"Done. Best test acc = {best_acc:.4f}. Saved to {os.path.abspath(ckpt_path)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBUxf8AKMJkP"
      },
      "source": [
        "4. TTFS-Phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLzR9yPPML2S",
        "outputId": "c9494ab7-0245-487b-9aa8-74eb1e79b70f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TTFS-Phase rank-balance encoder sanity check (T=60, P=3, phi0=0) ...\n",
            "TTFS-Phase spike-count per pixel (min,max) = 1.0 1.0\n",
            "Phase-lock ratio (should be 1.0): 1.0\n",
            "Maxima-bin occupancy (M=20) first 10: [0.0501302070915699, 0.0501302070915699, 0.0498046875, 0.0501302070915699, 0.0498046875, 0.0501302070915699, 0.0501302070915699, 0.0498046875, 0.0501302070915699, 0.0498046875]\n",
            "Maxima-bin occupancy (M=20) last  10: [0.0501302070915699, 0.0501302070915699, 0.0498046875, 0.0501302070915699, 0.0498046875, 0.0501302070915699, 0.0501302070915699, 0.0498046875, 0.0501302070915699, 0.0498046875]\n",
            "Epoch 01 | train loss 2.1903 | train acc 0.1731 | test acc 0.2051 | best 0.2051\n",
            "Epoch 02 | train loss 2.0934 | train acc 0.2217 | test acc 0.2393 | best 0.2393\n",
            "Epoch 03 | train loss 2.0402 | train acc 0.2489 | test acc 0.2605 | best 0.2605\n",
            "Epoch 04 | train loss 1.9953 | train acc 0.2732 | test acc 0.2919 | best 0.2919\n",
            "Epoch 05 | train loss 1.9685 | train acc 0.2902 | test acc 0.2679 | best 0.2919\n",
            "Epoch 06 | train loss 1.9452 | train acc 0.3118 | test acc 0.3169 | best 0.3169\n",
            "Epoch 07 | train loss 1.9092 | train acc 0.3293 | test acc 0.3445 | best 0.3445\n",
            "Epoch 08 | train loss 1.8917 | train acc 0.3384 | test acc 0.3379 | best 0.3445\n",
            "Epoch 09 | train loss 1.8667 | train acc 0.3537 | test acc 0.3794 | best 0.3794\n",
            "Epoch 10 | train loss 1.8575 | train acc 0.3636 | test acc 0.4060 | best 0.4060\n",
            "Epoch 11 | train loss 1.8221 | train acc 0.3830 | test acc 0.3728 | best 0.4060\n",
            "Epoch 12 | train loss 1.7943 | train acc 0.3998 | test acc 0.4032 | best 0.4060\n",
            "Epoch 13 | train loss 1.7593 | train acc 0.4189 | test acc 0.4399 | best 0.4399\n",
            "Epoch 14 | train loss 1.6982 | train acc 0.4460 | test acc 0.4653 | best 0.4653\n",
            "Epoch 15 | train loss 1.6827 | train acc 0.4559 | test acc 0.4747 | best 0.4747\n",
            "Epoch 16 | train loss 1.6818 | train acc 0.4595 | test acc 0.5061 | best 0.5061\n",
            "Epoch 17 | train loss 1.6452 | train acc 0.4771 | test acc 0.4732 | best 0.5061\n",
            "Epoch 18 | train loss 1.6105 | train acc 0.4943 | test acc 0.4926 | best 0.5061\n",
            "Epoch 19 | train loss 1.5708 | train acc 0.5115 | test acc 0.4748 | best 0.5061\n",
            "Epoch 20 | train loss 1.5546 | train acc 0.5236 | test acc 0.5137 | best 0.5137\n",
            "Epoch 21 | train loss 1.5199 | train acc 0.5373 | test acc 0.5382 | best 0.5382\n",
            "Epoch 22 | train loss 1.4949 | train acc 0.5503 | test acc 0.5341 | best 0.5382\n",
            "Epoch 23 | train loss 1.4655 | train acc 0.5662 | test acc 0.5538 | best 0.5538\n",
            "Epoch 24 | train loss 1.4359 | train acc 0.5769 | test acc 0.5508 | best 0.5538\n",
            "Epoch 25 | train loss 1.4373 | train acc 0.5796 | test acc 0.5853 | best 0.5853\n",
            "Epoch 26 | train loss 1.3901 | train acc 0.6039 | test acc 0.5992 | best 0.5992\n",
            "Epoch 27 | train loss 1.3679 | train acc 0.6135 | test acc 0.6093 | best 0.6093\n",
            "Epoch 28 | train loss 1.3445 | train acc 0.6239 | test acc 0.6101 | best 0.6101\n",
            "Epoch 29 | train loss 1.3265 | train acc 0.6326 | test acc 0.6147 | best 0.6147\n",
            "Epoch 30 | train loss 1.2981 | train acc 0.6482 | test acc 0.5781 | best 0.6147\n",
            "Epoch 31 | train loss 1.2861 | train acc 0.6559 | test acc 0.6426 | best 0.6426\n",
            "Epoch 32 | train loss 1.2628 | train acc 0.6654 | test acc 0.6598 | best 0.6598\n",
            "Epoch 33 | train loss 1.2457 | train acc 0.6719 | test acc 0.6166 | best 0.6598\n",
            "Epoch 34 | train loss 1.2265 | train acc 0.6819 | test acc 0.6542 | best 0.6598\n",
            "Epoch 35 | train loss 1.2054 | train acc 0.6935 | test acc 0.6501 | best 0.6598\n",
            "Epoch 36 | train loss 1.1967 | train acc 0.6964 | test acc 0.6819 | best 0.6819\n",
            "Epoch 37 | train loss 1.1667 | train acc 0.7111 | test acc 0.6508 | best 0.6819\n",
            "Epoch 38 | train loss 1.1606 | train acc 0.7139 | test acc 0.6958 | best 0.6958\n",
            "Epoch 39 | train loss 1.1392 | train acc 0.7249 | test acc 0.6836 | best 0.6958\n",
            "Epoch 40 | train loss 1.1274 | train acc 0.7318 | test acc 0.7103 | best 0.7103\n",
            "Epoch 41 | train loss 1.1110 | train acc 0.7376 | test acc 0.7196 | best 0.7196\n",
            "Epoch 42 | train loss 1.0979 | train acc 0.7431 | test acc 0.7188 | best 0.7196\n",
            "Epoch 43 | train loss 1.0794 | train acc 0.7551 | test acc 0.7188 | best 0.7196\n",
            "Epoch 44 | train loss 1.0620 | train acc 0.7616 | test acc 0.7304 | best 0.7304\n",
            "Epoch 45 | train loss 1.0507 | train acc 0.7668 | test acc 0.7365 | best 0.7365\n",
            "Epoch 46 | train loss 1.0362 | train acc 0.7724 | test acc 0.7376 | best 0.7376\n",
            "Epoch 47 | train loss 1.0340 | train acc 0.7723 | test acc 0.7290 | best 0.7376\n",
            "Epoch 48 | train loss 1.0147 | train acc 0.7841 | test acc 0.7512 | best 0.7512\n",
            "Epoch 49 | train loss 1.0050 | train acc 0.7885 | test acc 0.7363 | best 0.7512\n",
            "Epoch 50 | train loss 0.9938 | train acc 0.7914 | test acc 0.7381 | best 0.7512\n",
            "Epoch 51 | train loss 0.9846 | train acc 0.7957 | test acc 0.7522 | best 0.7522\n",
            "Epoch 52 | train loss 0.9798 | train acc 0.7984 | test acc 0.7462 | best 0.7522\n",
            "Epoch 53 | train loss 0.9644 | train acc 0.8058 | test acc 0.7563 | best 0.7563\n",
            "Epoch 54 | train loss 0.9585 | train acc 0.8107 | test acc 0.7532 | best 0.7563\n",
            "Epoch 55 | train loss 0.9482 | train acc 0.8138 | test acc 0.7600 | best 0.7600\n",
            "Epoch 56 | train loss 0.9393 | train acc 0.8170 | test acc 0.7659 | best 0.7659\n",
            "Epoch 57 | train loss 0.9337 | train acc 0.8189 | test acc 0.7637 | best 0.7659\n",
            "Epoch 58 | train loss 0.9240 | train acc 0.8239 | test acc 0.7665 | best 0.7665\n",
            "Epoch 59 | train loss 0.9177 | train acc 0.8286 | test acc 0.7662 | best 0.7665\n",
            "Epoch 60 | train loss 0.9115 | train acc 0.8298 | test acc 0.7659 | best 0.7665\n",
            "Epoch 61 | train loss 0.9046 | train acc 0.8354 | test acc 0.7705 | best 0.7705\n",
            "Epoch 62 | train loss 0.8981 | train acc 0.8374 | test acc 0.7665 | best 0.7705\n",
            "Epoch 63 | train loss 0.8930 | train acc 0.8394 | test acc 0.7730 | best 0.7730\n",
            "Epoch 64 | train loss 0.8886 | train acc 0.8428 | test acc 0.7735 | best 0.7735\n",
            "Epoch 65 | train loss 0.8809 | train acc 0.8457 | test acc 0.7736 | best 0.7736\n",
            "Epoch 66 | train loss 0.8764 | train acc 0.8481 | test acc 0.7719 | best 0.7736\n",
            "Epoch 67 | train loss 0.8741 | train acc 0.8479 | test acc 0.7733 | best 0.7736\n",
            "Epoch 68 | train loss 0.8688 | train acc 0.8515 | test acc 0.7789 | best 0.7789\n",
            "Epoch 69 | train loss 0.8671 | train acc 0.8529 | test acc 0.7774 | best 0.7789\n",
            "Epoch 70 | train loss 0.8610 | train acc 0.8568 | test acc 0.7752 | best 0.7789\n",
            "Epoch 71 | train loss 0.8606 | train acc 0.8546 | test acc 0.7772 | best 0.7789\n",
            "Epoch 72 | train loss 0.8544 | train acc 0.8601 | test acc 0.7771 | best 0.7789\n",
            "Epoch 73 | train loss 0.8551 | train acc 0.8593 | test acc 0.7816 | best 0.7816\n",
            "Epoch 74 | train loss 0.8517 | train acc 0.8615 | test acc 0.7801 | best 0.7816\n",
            "Epoch 75 | train loss 0.8506 | train acc 0.8599 | test acc 0.7815 | best 0.7816\n",
            "Epoch 76 | train loss 0.8490 | train acc 0.8637 | test acc 0.7763 | best 0.7816\n",
            "Epoch 77 | train loss 0.8503 | train acc 0.8617 | test acc 0.7793 | best 0.7816\n",
            "Epoch 78 | train loss 0.8486 | train acc 0.8637 | test acc 0.7806 | best 0.7816\n",
            "Epoch 79 | train loss 0.8460 | train acc 0.8639 | test acc 0.7786 | best 0.7816\n",
            "Epoch 80 | train loss 0.8460 | train acc 0.8641 | test acc 0.7800 | best 0.7816\n",
            "Done. Best test acc = 0.7816. Saved to /content/best_ttfs_phase_rankbalance_resnet_snn_GN_beta095.pt\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# CIFAR-10 Spiking ResNet-style SNN (GroupNorm) +\n",
        "# TTFS-Phase Encoder (STRICT SMO maxima-only, 1 spike per pixel)\n",
        "# Method B: per-image rank-balanced binning (nearly uniform over M maxima bins)\n",
        "#\n",
        "# Key properties:\n",
        "# - One spike per pixel (no increase in spike count)\n",
        "# - Strict phase-lock: t ∈ {phi0 + k*P}\n",
        "# - Per image, spike times are distributed ~uniformly across M maxima bins\n",
        "# - Stable beta=0.95, GN ResNet-style SNN, logits readout mean/last\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import snntorch as snn\n",
        "\n",
        "# -----------------------------\n",
        "# Config\n",
        "# -----------------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "batch_size = 128\n",
        "num_epochs = 80\n",
        "lr = 1e-3\n",
        "weight_decay = 5e-4\n",
        "\n",
        "T = 60\n",
        "P = 3\n",
        "phi0 = 0\n",
        "\n",
        "beta = 0.95\n",
        "\n",
        "readout_mode = \"mean\"     # \"mean\" or \"last\"\n",
        "label_smoothing = 0.1\n",
        "num_classes = 10\n",
        "ckpt_path = \"best_ttfs_phase_rankbalance_resnet_snn_GN_beta095.pt\"\n",
        "\n",
        "# encoder settings\n",
        "USE_CDF_U = False   # keep False first; rank-balance already equalizes within each image\n",
        "JITTER = 1e-6       # break ties for identical pixel values\n",
        "\n",
        "# CDF settings (only used if USE_CDF_U=True)\n",
        "CDF_BINS = 4096\n",
        "CDF_MAX_BATCHES = 400\n",
        "\n",
        "# -----------------------------\n",
        "# Data\n",
        "# -----------------------------\n",
        "cifar_mean = (0.4914, 0.4822, 0.4465)\n",
        "cifar_std  = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(cifar_mean, cifar_std),\n",
        "])\n",
        "\n",
        "test_tf = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(cifar_mean, cifar_std),\n",
        "])\n",
        "\n",
        "train_set = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=train_tf)\n",
        "test_set  = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=test_tf)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_set,  batch_size=batch_size, shuffle=False, drop_last=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "_cifar_mean_t = torch.tensor(cifar_mean).view(1, 3, 1, 1)\n",
        "_cifar_std_t  = torch.tensor(cifar_std).view(1, 3, 1, 1)\n",
        "\n",
        "@torch.no_grad()\n",
        "def cifar_to_unit_interval(x_norm: torch.Tensor) -> torch.Tensor:\n",
        "    mean = _cifar_mean_t.to(x_norm.device)\n",
        "    std  = _cifar_std_t.to(x_norm.device)\n",
        "    return (x_norm * std + mean).clamp(0.0, 1.0)\n",
        "\n",
        "# -----------------------------\n",
        "# Optional: build CDF (only if USE_CDF_U=True)\n",
        "# -----------------------------\n",
        "@torch.no_grad()\n",
        "def build_cdf_from_cifar_loader(loader, num_bins=4096, device=\"cpu\", max_batches=400):\n",
        "    hist = torch.zeros(num_bins, dtype=torch.float64, device=device)\n",
        "    for bi, (x_norm, _) in enumerate(loader):\n",
        "        if (max_batches is not None) and (bi >= max_batches):\n",
        "            break\n",
        "        x_norm = x_norm.to(device, non_blocking=True)\n",
        "        x_unit = cifar_to_unit_interval(x_norm)\n",
        "        x_flat = x_unit.flatten()\n",
        "        idx = torch.clamp((x_flat * (num_bins - 1)).long(), 0, num_bins - 1)\n",
        "        hist.scatter_add_(0, idx, torch.ones_like(idx, dtype=torch.float64))\n",
        "    if hist.sum().item() <= 0:\n",
        "        raise RuntimeError(\"CDF build failed: empty histogram.\")\n",
        "    pdf = hist / hist.sum()\n",
        "    cdf = torch.cumsum(pdf, dim=0)\n",
        "    cdf = cdf / cdf[-1].clamp_min(1e-12)\n",
        "    return cdf.to(dtype=torch.float32, device=\"cpu\")\n",
        "\n",
        "cdf_cpu = None\n",
        "if USE_CDF_U:\n",
        "    print(f\"Building CIFAR intensity CDF (bins={CDF_BINS}, max_batches={CDF_MAX_BATCHES}) ...\")\n",
        "    cdf_cpu = build_cdf_from_cifar_loader(train_loader, num_bins=CDF_BINS, device=\"cpu\", max_batches=CDF_MAX_BATCHES)\n",
        "    print(\"CDF built.\")\n",
        "\n",
        "# ============================================================\n",
        "# TTFS-Phase Encoder (Method B): per-image rank-balanced binning\n",
        "# ============================================================\n",
        "@torch.no_grad()\n",
        "def ttfs_phase_rank_balance_cifar(\n",
        "    x_norm: torch.Tensor,          # [B,3,32,32] normalized\n",
        "    T: int,\n",
        "    P: int,\n",
        "    phi0: int,\n",
        "    use_cdf_u: bool = False,\n",
        "    cdf_cpu: torch.Tensor | None = None,\n",
        "    num_bins: int = 4096,\n",
        "    jitter: float = 1e-6,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    One-spike-per-pixel, STRICT maxima-only (t = phi0 + k*P).\n",
        "    Per image, assigns bins by rank so that occupancy across M maxima bins is ~uniform.\n",
        "\n",
        "    Larger intensity -> earlier bins (smaller k).\n",
        "    \"\"\"\n",
        "    assert 0 <= phi0 < P\n",
        "    device = x_norm.device\n",
        "    B = x_norm.size(0)\n",
        "\n",
        "    # undo normalize -> [0,1]\n",
        "    x_unit = cifar_to_unit_interval(x_norm)          # [B,3,32,32]\n",
        "    x_flat = x_unit.view(B, -1)                      # [B,N]\n",
        "    N = x_flat.size(1)\n",
        "\n",
        "    # choose score used for ranking\n",
        "    if use_cdf_u:\n",
        "        assert cdf_cpu is not None\n",
        "        idx = torch.clamp((x_unit * (num_bins - 1)).long(), 0, num_bins - 1)\n",
        "        score = cdf_cpu.to(device)[idx].view(B, -1).clamp(0.0, 1.0)\n",
        "    else:\n",
        "        score = x_flat\n",
        "\n",
        "    # break ties (important for many identical pixels after clamp)\n",
        "    if jitter > 0:\n",
        "        score = score + jitter * torch.randn_like(score)\n",
        "\n",
        "    # rank pixels: 0..N-1 (0 = brightest)\n",
        "    order = torch.argsort(score, dim=1, descending=True)  # [B,N]\n",
        "    inv_rank = torch.empty_like(order)\n",
        "    inv_rank.scatter_(1, order, torch.arange(N, device=device).view(1, N).expand(B, N))\n",
        "\n",
        "    # number of maxima bins\n",
        "    M = int(((T - 1 - phi0) // P) + 1)  # e.g., T=60,P=3 => 20\n",
        "    # map rank -> bin k so each bin gets ~N/M pixels\n",
        "    k = torch.floor(inv_rank.float() * M / float(N)).long().clamp(0, M - 1)  # [B,N]\n",
        "\n",
        "    # time index: t = phi0 + k*P\n",
        "    t = (phi0 + k * P).long()  # [B,N]\n",
        "\n",
        "    spk = torch.zeros(T, B, N, device=device, dtype=torch.float32)\n",
        "    b_idx = torch.arange(B, device=device).view(B, 1).expand(B, N)\n",
        "    n_idx = torch.arange(N, device=device).view(1, N).expand(B, N)\n",
        "    spk[t, b_idx, n_idx] = 1.0\n",
        "\n",
        "    return spk.view(T, B, *x_norm.shape[1:])  # [T,B,3,32,32]\n",
        "\n",
        "# -----------------------------\n",
        "# Sanity check utilities\n",
        "# -----------------------------\n",
        "@torch.no_grad()\n",
        "def phase_lock_ratio(spk_in: torch.Tensor, P: int, phi0: int) -> float:\n",
        "    Tsteps = spk_in.size(0)\n",
        "    spk_flat = spk_in.view(Tsteps, -1)\n",
        "    total = spk_flat.sum()\n",
        "    if total.item() == 0:\n",
        "        return 0.0\n",
        "    t = torch.arange(Tsteps, device=spk_in.device)\n",
        "    on_grid = ((t - phi0) % P == 0).float()\n",
        "    on = (spk_flat * on_grid[:, None]).sum()\n",
        "    return (on / total).item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def maxima_bin_hist(spk_in: torch.Tensor, T: int, P: int, phi0: int):\n",
        "    \"\"\"\n",
        "    returns occupancy over maxima bins k=0..M-1 (normalized).\n",
        "    \"\"\"\n",
        "    Tsteps = spk_in.size(0)\n",
        "    assert Tsteps == T\n",
        "    M = int(((T - 1 - phi0) // P) + 1)\n",
        "\n",
        "    # count spikes at each t\n",
        "    t_count = spk_in.view(T, -1).sum(dim=1)  # [T]\n",
        "    total = t_count.sum().clamp_min(1.0)\n",
        "\n",
        "    # collect maxima times\n",
        "    ks = torch.arange(M, device=spk_in.device)\n",
        "    t_max = (phi0 + ks * P).long()\n",
        "    k_count = t_count[t_max] / total\n",
        "    return k_count.detach().cpu()\n",
        "\n",
        "@torch.no_grad()\n",
        "def sanity_check_encoder():\n",
        "    x_norm, _ = next(iter(train_loader))\n",
        "    x_norm = x_norm.to(device)\n",
        "\n",
        "    spk_in = ttfs_phase_rank_balance_cifar(\n",
        "        x_norm, T=T, P=P, phi0=phi0,\n",
        "        use_cdf_u=USE_CDF_U, cdf_cpu=cdf_cpu, num_bins=CDF_BINS, jitter=JITTER\n",
        "    )\n",
        "\n",
        "    counts = spk_in.sum(dim=0)\n",
        "    print(\"TTFS-Phase spike-count per pixel (min,max) =\", counts.min().item(), counts.max().item())\n",
        "\n",
        "    print(\"Phase-lock ratio (should be 1.0):\", phase_lock_ratio(spk_in, P=P, phi0=phi0))\n",
        "\n",
        "    k_hist = maxima_bin_hist(spk_in, T=T, P=P, phi0=phi0)  # length M\n",
        "    print(f\"Maxima-bin occupancy (M={len(k_hist)}) first 10:\", [float(v) for v in k_hist[:10]])\n",
        "    print(f\"Maxima-bin occupancy (M={len(k_hist)}) last  10:\", [float(v) for v in k_hist[-10:]])\n",
        "\n",
        "print(f\"TTFS-Phase rank-balance encoder sanity check (T={T}, P={P}, phi0={phi0}) ...\")\n",
        "sanity_check_encoder()\n",
        "\n",
        "# -----------------------------\n",
        "# GN helper\n",
        "# -----------------------------\n",
        "def GN(ch, groups=16):\n",
        "    g = min(groups, ch)\n",
        "    while ch % g != 0 and g > 1:\n",
        "        g -= 1\n",
        "    return nn.GroupNorm(g, ch)\n",
        "\n",
        "# -----------------------------\n",
        "# Spiking ResNet-style blocks (GN)\n",
        "# -----------------------------\n",
        "class SpkBasicBlockGN(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, stride=1, beta=0.95):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, stride=stride, padding=1, bias=False)\n",
        "        self.gn1   = GN(out_ch)\n",
        "        self.lif1  = snn.Leaky(beta=beta)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, stride=1, padding=1, bias=False)\n",
        "        self.gn2   = GN(out_ch)\n",
        "        self.lif2  = snn.Leaky(beta=beta)\n",
        "\n",
        "        if stride != 1 or in_ch != out_ch:\n",
        "            self.short = nn.Sequential(\n",
        "                nn.Conv2d(in_ch, out_ch, 1, stride=stride, bias=False),\n",
        "                GN(out_ch),\n",
        "            )\n",
        "        else:\n",
        "            self.short = None\n",
        "\n",
        "    def init_state(self):\n",
        "        return self.lif1.init_leaky(), self.lif2.init_leaky()\n",
        "\n",
        "    def forward_step(self, x_spk, mem1, mem2):\n",
        "        out = self.conv1(x_spk)\n",
        "        out = self.gn1(out)\n",
        "        spk1, mem1 = self.lif1(out, mem1)\n",
        "\n",
        "        out = self.conv2(spk1)\n",
        "        out = self.gn2(out)\n",
        "\n",
        "        skip = x_spk if self.short is None else self.short(x_spk)\n",
        "        out = out + skip\n",
        "\n",
        "        spk2, mem2 = self.lif2(out, mem2)\n",
        "        return spk2, mem1, mem2\n",
        "\n",
        "class SpikingResNetCIFAR_GN(nn.Module):\n",
        "    def __init__(self, beta=0.95, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.conv0 = nn.Conv2d(3, 64, 3, stride=1, padding=1, bias=False)\n",
        "        self.gn0   = GN(64)\n",
        "        self.lif0  = snn.Leaky(beta=beta)\n",
        "\n",
        "        self.b1_0 = SpkBasicBlockGN(64, 64,  stride=1, beta=beta)\n",
        "        self.b1_1 = SpkBasicBlockGN(64, 64,  stride=1, beta=beta)\n",
        "\n",
        "        self.b2_0 = SpkBasicBlockGN(64, 128, stride=2, beta=beta)\n",
        "        self.b2_1 = SpkBasicBlockGN(128,128, stride=1, beta=beta)\n",
        "\n",
        "        self.b3_0 = SpkBasicBlockGN(128,256, stride=2, beta=beta)\n",
        "        self.b3_1 = SpkBasicBlockGN(256,256, stride=1, beta=beta)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.drop    = nn.Dropout(p=0.3)\n",
        "        self.fc      = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, spk_in, readout_mode=\"mean\"):\n",
        "        Tsteps, B, _, _, _ = spk_in.shape\n",
        "\n",
        "        mem0 = self.lif0.init_leaky()\n",
        "        m10, m11 = self.b1_0.init_state()\n",
        "        m12, m13 = self.b1_1.init_state()\n",
        "        m20, m21 = self.b2_0.init_state()\n",
        "        m22, m23 = self.b2_1.init_state()\n",
        "        m30, m31 = self.b3_0.init_state()\n",
        "        m32, m33 = self.b3_1.init_state()\n",
        "\n",
        "        logits_rec = []\n",
        "        for t in range(Tsteps):\n",
        "            x = spk_in[t]\n",
        "\n",
        "            x = self.conv0(x); x = self.gn0(x)\n",
        "            x, mem0 = self.lif0(x, mem0)\n",
        "\n",
        "            x, m10, m11 = self.b1_0.forward_step(x, m10, m11)\n",
        "            x, m12, m13 = self.b1_1.forward_step(x, m12, m13)\n",
        "\n",
        "            x, m20, m21 = self.b2_0.forward_step(x, m20, m21)\n",
        "            x, m22, m23 = self.b2_1.forward_step(x, m22, m23)\n",
        "\n",
        "            x, m30, m31 = self.b3_0.forward_step(x, m30, m31)\n",
        "            x, m32, m33 = self.b3_1.forward_step(x, m32, m33)\n",
        "\n",
        "            x = self.avgpool(x).view(B, -1)\n",
        "            x = self.drop(x)\n",
        "            logits = self.fc(x)  # direct logits\n",
        "            logits_rec.append(logits)\n",
        "\n",
        "        logits_rec = torch.stack(logits_rec, dim=0)  # [T,B,10]\n",
        "        return logits_rec[-1] if readout_mode == \"last\" else logits_rec.mean(dim=0)\n",
        "\n",
        "# -----------------------------\n",
        "# Train / Eval\n",
        "# -----------------------------\n",
        "model = SpikingResNetCIFAR_GN(beta=beta, num_classes=num_classes).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate():\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    for x_norm, y in test_loader:\n",
        "        x_norm, y = x_norm.to(device), y.to(device)\n",
        "\n",
        "        spk_in = ttfs_phase_rank_balance_cifar(\n",
        "            x_norm, T=T, P=P, phi0=phi0,\n",
        "            use_cdf_u=USE_CDF_U, cdf_cpu=cdf_cpu, num_bins=CDF_BINS, jitter=JITTER\n",
        "        )\n",
        "\n",
        "        logits = model(spk_in, readout_mode=readout_mode)\n",
        "        pred = logits.argmax(dim=1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.numel()\n",
        "    return correct / max(total, 1)\n",
        "\n",
        "best_acc = -1.0\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    running_loss, running_correct, running_total = 0.0, 0, 0\n",
        "\n",
        "    for x_norm, y in train_loader:\n",
        "        x_norm, y = x_norm.to(device), y.to(device)\n",
        "\n",
        "        spk_in = ttfs_phase_rank_balance_cifar(\n",
        "            x_norm, T=T, P=P, phi0=phi0,\n",
        "            use_cdf_u=USE_CDF_U, cdf_cpu=cdf_cpu, num_bins=CDF_BINS, jitter=JITTER\n",
        "        )\n",
        "\n",
        "        logits = model(spk_in, readout_mode=readout_mode)\n",
        "        loss = F.cross_entropy(logits, y, label_smoothing=label_smoothing)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * x_norm.size(0)\n",
        "        running_correct += (logits.argmax(dim=1) == y).sum().item()\n",
        "        running_total += x_norm.size(0)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    test_acc = evaluate()\n",
        "    if test_acc > best_acc:\n",
        "        best_acc = test_acc\n",
        "        torch.save(\n",
        "            {\n",
        "                \"epoch\": epoch,\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"test_acc\": test_acc,\n",
        "                \"config\": {\n",
        "                    \"T\": T, \"P\": P, \"phi0\": phi0,\n",
        "                    \"beta\": beta,\n",
        "                    \"readout_mode\": readout_mode,\n",
        "                    \"label_smoothing\": label_smoothing,\n",
        "                    \"lr\": lr,\n",
        "                    \"encoder\": \"ttfs_phase_rank_balance\",\n",
        "                    \"use_cdf_u\": USE_CDF_U,\n",
        "                    \"jitter\": JITTER,\n",
        "                },\n",
        "            },\n",
        "            ckpt_path,\n",
        "        )\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch:02d} | \"\n",
        "        f\"train loss {running_loss/running_total:.4f} | \"\n",
        "        f\"train acc {running_correct/running_total:.4f} | \"\n",
        "        f\"test acc {test_acc:.4f} | \"\n",
        "        f\"best {best_acc:.4f}\"\n",
        "    )\n",
        "\n",
        "print(f\"Done. Best test acc = {best_acc:.4f}. Saved to {os.path.abspath(ckpt_path)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yp3lj79myt3m"
      },
      "source": [
        "5. ISI-Phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3gcV4i-yys-",
        "outputId": "1a5e501b-f91d-4db4-9047-10a5a9c27654"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:05<00:00, 29.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ISI-Phase(timegrid) sanity check (T=60, K=4, P=3, phi0=0) ...\n",
            "ISI-Phase spike-count per pixel (min,max) = 4.0 4.0\n",
            "Phase-lock ratio (should be 1.0): 1.0\n",
            "Maxima-bin occupancy (M=20) first 10: [0.17098236083984375, 0.14102935791015625, 0.1348133087158203, 0.0566864013671875, 0.024326324462890625, 0.02075704000890255, 0.01550420094281435, 0.0155398054048419, 0.011348724365234375, 0.012067794799804688]\n",
            "Maxima-bin occupancy (M=20) last  10: [0.011987686157226562, 0.01106135081499815, 0.013695399276912212, 0.014286041259765625, 0.017553329467773438, 0.018564224243164062, 0.021310806274414062, 0.08214187622070312, 0.08818117529153824, 0.11816278845071793]\n",
            "Epoch 01 | train loss 2.2075 | train acc 0.1681 | test acc 0.2309 | best 0.2309\n",
            "Epoch 02 | train loss 2.1110 | train acc 0.2184 | test acc 0.2342 | best 0.2342\n",
            "Epoch 03 | train loss 2.0508 | train acc 0.2373 | test acc 0.2519 | best 0.2519\n",
            "Epoch 04 | train loss 2.0108 | train acc 0.2492 | test acc 0.2609 | best 0.2609\n",
            "Epoch 05 | train loss 1.9960 | train acc 0.2559 | test acc 0.2648 | best 0.2648\n",
            "Epoch 06 | train loss 1.9726 | train acc 0.2676 | test acc 0.2981 | best 0.2981\n",
            "Epoch 07 | train loss 1.9278 | train acc 0.2982 | test acc 0.3016 | best 0.3016\n",
            "Epoch 08 | train loss 1.8858 | train acc 0.3275 | test acc 0.3499 | best 0.3499\n",
            "Epoch 09 | train loss 1.8358 | train acc 0.3615 | test acc 0.3801 | best 0.3801\n",
            "Epoch 10 | train loss 1.7788 | train acc 0.3936 | test acc 0.4256 | best 0.4256\n",
            "Epoch 11 | train loss 1.7156 | train acc 0.4301 | test acc 0.4494 | best 0.4494\n",
            "Epoch 12 | train loss 1.6588 | train acc 0.4603 | test acc 0.4926 | best 0.4926\n",
            "Epoch 13 | train loss 1.6218 | train acc 0.4808 | test acc 0.4613 | best 0.4926\n",
            "Epoch 14 | train loss 1.5776 | train acc 0.5056 | test acc 0.4994 | best 0.4994\n",
            "Epoch 15 | train loss 1.5316 | train acc 0.5287 | test acc 0.5094 | best 0.5094\n",
            "Epoch 16 | train loss 1.4937 | train acc 0.5484 | test acc 0.5778 | best 0.5778\n",
            "Epoch 17 | train loss 1.4685 | train acc 0.5602 | test acc 0.5744 | best 0.5778\n",
            "Epoch 18 | train loss 1.4409 | train acc 0.5725 | test acc 0.5842 | best 0.5842\n",
            "Epoch 19 | train loss 1.4213 | train acc 0.5818 | test acc 0.5866 | best 0.5866\n",
            "Epoch 20 | train loss 1.3888 | train acc 0.5981 | test acc 0.5358 | best 0.5866\n",
            "Epoch 21 | train loss 1.3671 | train acc 0.6095 | test acc 0.6198 | best 0.6198\n",
            "Epoch 22 | train loss 1.3299 | train acc 0.6276 | test acc 0.6055 | best 0.6198\n",
            "Epoch 23 | train loss 1.3143 | train acc 0.6353 | test acc 0.6151 | best 0.6198\n",
            "Epoch 24 | train loss 1.2935 | train acc 0.6435 | test acc 0.6247 | best 0.6247\n",
            "Epoch 25 | train loss 1.2688 | train acc 0.6563 | test acc 0.6228 | best 0.6247\n",
            "Epoch 26 | train loss 1.2464 | train acc 0.6664 | test acc 0.6664 | best 0.6664\n",
            "Epoch 27 | train loss 1.2287 | train acc 0.6747 | test acc 0.6523 | best 0.6664\n",
            "Epoch 28 | train loss 1.2179 | train acc 0.6809 | test acc 0.6795 | best 0.6795\n",
            "Epoch 29 | train loss 1.1953 | train acc 0.6896 | test acc 0.6618 | best 0.6795\n",
            "Epoch 30 | train loss 1.1819 | train acc 0.6983 | test acc 0.6762 | best 0.6795\n",
            "Epoch 31 | train loss 1.1696 | train acc 0.7023 | test acc 0.6817 | best 0.6817\n",
            "Epoch 32 | train loss 1.1513 | train acc 0.7106 | test acc 0.6641 | best 0.6817\n",
            "Epoch 33 | train loss 1.1301 | train acc 0.7218 | test acc 0.6891 | best 0.6891\n",
            "Epoch 34 | train loss 1.1102 | train acc 0.7321 | test acc 0.7136 | best 0.7136\n",
            "Epoch 35 | train loss 1.1039 | train acc 0.7346 | test acc 0.7154 | best 0.7154\n",
            "Epoch 36 | train loss 1.0946 | train acc 0.7400 | test acc 0.7199 | best 0.7199\n",
            "Epoch 37 | train loss 1.0858 | train acc 0.7413 | test acc 0.6865 | best 0.7199\n",
            "Epoch 38 | train loss 1.0649 | train acc 0.7543 | test acc 0.7264 | best 0.7264\n",
            "Epoch 39 | train loss 1.0506 | train acc 0.7570 | test acc 0.7394 | best 0.7394\n",
            "Epoch 40 | train loss 1.0434 | train acc 0.7631 | test acc 0.7173 | best 0.7394\n",
            "Epoch 41 | train loss 1.0294 | train acc 0.7683 | test acc 0.7176 | best 0.7394\n",
            "Epoch 42 | train loss 1.0175 | train acc 0.7748 | test acc 0.7401 | best 0.7401\n",
            "Epoch 43 | train loss 1.0084 | train acc 0.7807 | test acc 0.7520 | best 0.7520\n",
            "Epoch 44 | train loss 0.9960 | train acc 0.7841 | test acc 0.7426 | best 0.7520\n",
            "Epoch 45 | train loss 0.9919 | train acc 0.7875 | test acc 0.7439 | best 0.7520\n",
            "Epoch 46 | train loss 0.9724 | train acc 0.7964 | test acc 0.7541 | best 0.7541\n",
            "Epoch 47 | train loss 0.9627 | train acc 0.7997 | test acc 0.7647 | best 0.7647\n",
            "Epoch 48 | train loss 0.9545 | train acc 0.8028 | test acc 0.7538 | best 0.7647\n",
            "Epoch 49 | train loss 0.9471 | train acc 0.8056 | test acc 0.7594 | best 0.7647\n",
            "Epoch 50 | train loss 0.9383 | train acc 0.8106 | test acc 0.7494 | best 0.7647\n",
            "Epoch 51 | train loss 0.9245 | train acc 0.8165 | test acc 0.7667 | best 0.7667\n",
            "Epoch 52 | train loss 0.9212 | train acc 0.8182 | test acc 0.7722 | best 0.7722\n",
            "Epoch 53 | train loss 0.9128 | train acc 0.8228 | test acc 0.7694 | best 0.7722\n",
            "Epoch 54 | train loss 0.9050 | train acc 0.8278 | test acc 0.7743 | best 0.7743\n",
            "Epoch 55 | train loss 0.8993 | train acc 0.8293 | test acc 0.7700 | best 0.7743\n",
            "Epoch 56 | train loss 0.8908 | train acc 0.8339 | test acc 0.7694 | best 0.7743\n",
            "Epoch 57 | train loss 0.8793 | train acc 0.8397 | test acc 0.7759 | best 0.7759\n",
            "Epoch 58 | train loss 0.8748 | train acc 0.8413 | test acc 0.7863 | best 0.7863\n",
            "Epoch 59 | train loss 0.8657 | train acc 0.8443 | test acc 0.7818 | best 0.7863\n",
            "Epoch 60 | train loss 0.8596 | train acc 0.8467 | test acc 0.7801 | best 0.7863\n",
            "Epoch 61 | train loss 0.8527 | train acc 0.8519 | test acc 0.7781 | best 0.7863\n",
            "Epoch 62 | train loss 0.8467 | train acc 0.8547 | test acc 0.7856 | best 0.7863\n",
            "Epoch 63 | train loss 0.8444 | train acc 0.8545 | test acc 0.7787 | best 0.7863\n",
            "Epoch 64 | train loss 0.8384 | train acc 0.8584 | test acc 0.7816 | best 0.7863\n",
            "Epoch 65 | train loss 0.8317 | train acc 0.8607 | test acc 0.7818 | best 0.7863\n",
            "Epoch 66 | train loss 0.8270 | train acc 0.8645 | test acc 0.7879 | best 0.7879\n",
            "Epoch 67 | train loss 0.8243 | train acc 0.8649 | test acc 0.7891 | best 0.7891\n",
            "Epoch 68 | train loss 0.8172 | train acc 0.8670 | test acc 0.7907 | best 0.7907\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# CIFAR-10 Spiking ResNet-style SNN (GroupNorm) +\n",
        "# ISI-Phase Encoder (STRICT phase-locked time grid, fixed-K, no spike-count increase)\n",
        "#\n",
        "# Key properties:\n",
        "# - K spikes per pixel/channel (no increase in spike count)\n",
        "# - Strict phase-lock: t ∈ {phi0 + k*P}\n",
        "# - ISI-like distribution over maxima bins (M bins), then mapped to time grid\n",
        "# - Stable beta=0.95, GN ResNet-style SNN, logits readout mean/last\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import snntorch as snn\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Config\n",
        "# -----------------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "batch_size = 128\n",
        "num_epochs = 80\n",
        "lr = 1e-3\n",
        "weight_decay = 5e-4\n",
        "\n",
        "T = 60              # total unroll steps\n",
        "P = 3               # phase period\n",
        "phi0 = 0            # phase offset, must satisfy 0 <= phi0 < P\n",
        "\n",
        "K = 4               # K spikes per pixel/channel (must satisfy K <= M)\n",
        "alpha_max = 2.0\n",
        "eps_q = 1e-3\n",
        "\n",
        "beta = 0.95\n",
        "\n",
        "readout_mode = \"mean\"     # \"mean\" or \"last\"\n",
        "label_smoothing = 0.1\n",
        "num_classes = 10\n",
        "ckpt_path = f\"best_isiphase_timegrid_resnet_snn_GN_beta{beta}_T{T}_P{P}_phi{phi0}_K{K}.pt\"\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Data\n",
        "# -----------------------------\n",
        "cifar_mean = (0.4914, 0.4822, 0.4465)\n",
        "cifar_std  = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(cifar_mean, cifar_std),\n",
        "])\n",
        "\n",
        "test_tf = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(cifar_mean, cifar_std),\n",
        "])\n",
        "\n",
        "train_set = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=train_tf)\n",
        "test_set  = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=test_tf)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_set,  batch_size=batch_size, shuffle=False, drop_last=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "_cifar_mean_t = torch.tensor(cifar_mean).view(1, 3, 1, 1)\n",
        "_cifar_std_t  = torch.tensor(cifar_std).view(1, 3, 1, 1)\n",
        "\n",
        "@torch.no_grad()\n",
        "def cifar_to_unit_interval(x_norm: torch.Tensor) -> torch.Tensor:\n",
        "    mean = _cifar_mean_t.to(x_norm.device)\n",
        "    std  = _cifar_std_t.to(x_norm.device)\n",
        "    return (x_norm * std + mean).clamp(0.0, 1.0)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# ISI-Phase Encoder (STRICT phase-lock on time grid)\n",
        "# ============================================================\n",
        "@torch.no_grad()\n",
        "def isi_phase_fixedK_strict_timegrid(\n",
        "    x_img_unit: torch.Tensor,     # [B,3,H,W] in [0,1]\n",
        "    T: int,\n",
        "    K: int,\n",
        "    P: int,\n",
        "    phi0: int = 0,\n",
        "    alpha_max: float = 2.0,\n",
        "    eps: float = 1e-3,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Return spikes [T,B,3,H,W], with exactly K spikes per pixel/channel.\n",
        "    All spikes lie on phase-locked grid: t = phi0 + k*P.\n",
        "\n",
        "    Build ISI-style CDF over M maxima bins (k=0..M-1), then map to time indices.\n",
        "\n",
        "    Requirements:\n",
        "      - 0 <= phi0 < P\n",
        "      - M = floor((T-1-phi0)/P)+1\n",
        "      - K <= M\n",
        "    \"\"\"\n",
        "    assert 0 <= phi0 < P\n",
        "    assert T >= 2 and K >= 1 and P >= 1\n",
        "\n",
        "    device = x_img_unit.device\n",
        "    B, C, H, W = x_img_unit.shape\n",
        "\n",
        "    x = x_img_unit.clamp(0.0, 1.0).view(B, -1)  # [B,N]\n",
        "    N = x.size(1)\n",
        "\n",
        "    # number of allowed phase-locked bins\n",
        "    M = int(((T - 1 - phi0) // P) + 1)\n",
        "    if K > M:\n",
        "        raise ValueError(\n",
        "            f\"K={K} must satisfy K<=M={M}. (M = floor((T-1-phi0)/P)+1)\"\n",
        "        )\n",
        "\n",
        "    # Build ISI-like distribution over bins k=0..M-1\n",
        "    k_grid = torch.arange(M, device=device, dtype=torch.float32).view(1, 1, M)\n",
        "    mid = (M - 1) / 2.0\n",
        "\n",
        "    alpha = (x * 2.0 - 1.0) * alpha_max           # [B,N]\n",
        "    alpha = alpha.unsqueeze(-1)                    # [B,N,1]\n",
        "\n",
        "    w = torch.exp(alpha * (k_grid - mid))          # [B,N,M]\n",
        "    w = w / (w.sum(dim=-1, keepdim=True) + 1e-12)\n",
        "    cdf = torch.cumsum(w, dim=-1)                  # [B,N,M]\n",
        "\n",
        "    # Quantiles -> pick K bins\n",
        "    q = torch.linspace(eps, 1.0 - eps, steps=K, device=device, dtype=torch.float32)\n",
        "    q = q.view(1, 1, K).expand(B, N, K)\n",
        "\n",
        "    # Avoid non-contiguous warning + slightly faster\n",
        "    cdf_c = cdf.contiguous()\n",
        "    q_c   = q.contiguous()\n",
        "\n",
        "    k_idx = torch.searchsorted(cdf_c, q_c).clamp(0, M - 1).long()  # [B,N,K]\n",
        "    k_idx, _ = torch.sort(k_idx, dim=-1)\n",
        "\n",
        "    # Strict uniqueness in bins\n",
        "    used = torch.zeros(B, N, M, device=device, dtype=torch.bool)\n",
        "    k_fixed = torch.full_like(k_idx, -1)\n",
        "\n",
        "    for kk in range(K):\n",
        "        k0 = k_idx[..., kk]\n",
        "        free = ~used.gather(dim=2, index=k0.unsqueeze(-1)).squeeze(-1)\n",
        "        k_fixed[..., kk] = torch.where(free, k0, torch.full_like(k0, -1))\n",
        "        if free.any():\n",
        "            used[free] |= F.one_hot(k0[free], num_classes=M).bool()\n",
        "\n",
        "    # Fill collisions with nearest free bin (forward then backward)\n",
        "    for kk in range(K):\n",
        "        need = (k_fixed[..., kk] < 0)\n",
        "        if not need.any():\n",
        "            continue\n",
        "\n",
        "        k0 = k_idx[..., kk].clone()\n",
        "        avail = ~used\n",
        "        ar = torch.arange(M, device=device).view(1, 1, M)\n",
        "\n",
        "        forward_mask = avail & (ar >= k0.unsqueeze(-1))\n",
        "        fwd_pos = forward_mask.float().argmax(dim=-1)\n",
        "        fwd_exists = forward_mask.any(dim=-1)\n",
        "\n",
        "        backward_mask = avail & (ar <= k0.unsqueeze(-1))\n",
        "        rev = torch.flip(backward_mask, dims=[-1])\n",
        "        bwd_pos_rev = rev.float().argmax(dim=-1)\n",
        "        bwd_pos = (M - 1) - bwd_pos_rev\n",
        "        bwd_exists = backward_mask.any(dim=-1)\n",
        "\n",
        "        chosen = torch.where(fwd_exists, fwd_pos, bwd_pos).long()\n",
        "        k_fixed[..., kk] = torch.where(need, chosen, k_fixed[..., kk])\n",
        "        used[need] |= F.one_hot(chosen[need], num_classes=M).bool()\n",
        "\n",
        "    # Map bins -> time indices (phase-locked)\n",
        "    t_idx = (phi0 + k_fixed * P).long()  # [B,N,K] in [0..T-1]\n",
        "\n",
        "    # Build spike tensor\n",
        "    spk_flat = torch.zeros(T, B, N, device=device, dtype=torch.float32)\n",
        "    b_idx = torch.arange(B, device=device).view(B, 1, 1).expand(B, N, K)\n",
        "    n_idx = torch.arange(N, device=device).view(1, N, 1).expand(B, N, K)\n",
        "    spk_flat[t_idx, b_idx, n_idx] = 1.0\n",
        "\n",
        "    return spk_flat.view(T, B, C, H, W)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Sanity check utilities\n",
        "# -----------------------------\n",
        "@torch.no_grad()\n",
        "def phase_lock_ratio(spk_in: torch.Tensor, P: int, phi0: int) -> float:\n",
        "    Tsteps = spk_in.size(0)\n",
        "    spk_flat = spk_in.view(Tsteps, -1)\n",
        "    total = spk_flat.sum().clamp_min(1.0)\n",
        "    t = torch.arange(Tsteps, device=spk_in.device)\n",
        "    on_grid = ((t - phi0) % P == 0).float()\n",
        "    on = (spk_flat * on_grid[:, None]).sum()\n",
        "    return (on / total).item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def maxima_bin_hist(spk_in: torch.Tensor, T: int, P: int, phi0: int):\n",
        "    \"\"\"\n",
        "    returns occupancy over maxima bins k=0..M-1 (normalized).\n",
        "    \"\"\"\n",
        "    Tsteps = spk_in.size(0)\n",
        "    assert Tsteps == T\n",
        "    M = int(((T - 1 - phi0) // P) + 1)\n",
        "\n",
        "    t_count = spk_in.view(T, -1).sum(dim=1)  # [T]\n",
        "    total = t_count.sum().clamp_min(1.0)\n",
        "\n",
        "    ks = torch.arange(M, device=spk_in.device)\n",
        "    t_max = (phi0 + ks * P).long()\n",
        "    k_count = t_count[t_max] / total\n",
        "    return k_count.detach().cpu()\n",
        "\n",
        "@torch.no_grad()\n",
        "def sanity_check_encoder():\n",
        "    x_norm, _ = next(iter(train_loader))\n",
        "    x_norm = x_norm.to(device)\n",
        "    x_unit = cifar_to_unit_interval(x_norm)\n",
        "\n",
        "    spk_in = isi_phase_fixedK_strict_timegrid(\n",
        "        x_unit, T=T, K=K, P=P, phi0=phi0, alpha_max=alpha_max, eps=eps_q\n",
        "    )\n",
        "\n",
        "    counts = spk_in.sum(dim=0)  # [B,3,H,W]\n",
        "    print(\"ISI-Phase spike-count per pixel (min,max) =\", counts.min().item(), counts.max().item())\n",
        "    print(\"Phase-lock ratio (should be 1.0):\", phase_lock_ratio(spk_in, P=P, phi0=phi0))\n",
        "\n",
        "    k_hist = maxima_bin_hist(spk_in, T=T, P=P, phi0=phi0)\n",
        "    print(f\"Maxima-bin occupancy (M={len(k_hist)}) first 10:\", [float(v) for v in k_hist[:10]])\n",
        "    print(f\"Maxima-bin occupancy (M={len(k_hist)}) last  10:\", [float(v) for v in k_hist[-10:]])\n",
        "\n",
        "print(f\"ISI-Phase(timegrid) sanity check (T={T}, K={K}, P={P}, phi0={phi0}) ...\")\n",
        "sanity_check_encoder()\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# GN helper\n",
        "# -----------------------------\n",
        "def GN(ch, groups=16):\n",
        "    g = min(groups, ch)\n",
        "    while ch % g != 0 and g > 1:\n",
        "        g -= 1\n",
        "    return nn.GroupNorm(g, ch)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Spiking ResNet-style blocks (GN)\n",
        "# -----------------------------\n",
        "class SpkBasicBlockGN(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, stride=1, beta=0.95):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, stride=stride, padding=1, bias=False)\n",
        "        self.gn1   = GN(out_ch)\n",
        "        self.lif1  = snn.Leaky(beta=beta)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, stride=1, padding=1, bias=False)\n",
        "        self.gn2   = GN(out_ch)\n",
        "        self.lif2  = snn.Leaky(beta=beta)\n",
        "\n",
        "        if stride != 1 or in_ch != out_ch:\n",
        "            self.short = nn.Sequential(\n",
        "                nn.Conv2d(in_ch, out_ch, 1, stride=stride, bias=False),\n",
        "                GN(out_ch),\n",
        "            )\n",
        "        else:\n",
        "            self.short = None\n",
        "\n",
        "    def init_state(self):\n",
        "        return self.lif1.init_leaky(), self.lif2.init_leaky()\n",
        "\n",
        "    def forward_step(self, x_spk, mem1, mem2):\n",
        "        out = self.conv1(x_spk)\n",
        "        out = self.gn1(out)\n",
        "        spk1, mem1 = self.lif1(out, mem1)\n",
        "\n",
        "        out = self.conv2(spk1)\n",
        "        out = self.gn2(out)\n",
        "\n",
        "        skip = x_spk if self.short is None else self.short(x_spk)\n",
        "        out = out + skip\n",
        "\n",
        "        spk2, mem2 = self.lif2(out, mem2)\n",
        "        return spk2, mem1, mem2\n",
        "\n",
        "\n",
        "class SpikingResNetCIFAR_GN(nn.Module):\n",
        "    def __init__(self, beta=0.95, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.conv0 = nn.Conv2d(3, 64, 3, stride=1, padding=1, bias=False)\n",
        "        self.gn0   = GN(64)\n",
        "        self.lif0  = snn.Leaky(beta=beta)\n",
        "\n",
        "        self.b1_0 = SpkBasicBlockGN(64, 64,  stride=1, beta=beta)\n",
        "        self.b1_1 = SpkBasicBlockGN(64, 64,  stride=1, beta=beta)\n",
        "\n",
        "        self.b2_0 = SpkBasicBlockGN(64, 128, stride=2, beta=beta)\n",
        "        self.b2_1 = SpkBasicBlockGN(128,128, stride=1, beta=beta)\n",
        "\n",
        "        self.b3_0 = SpkBasicBlockGN(128,256, stride=2, beta=beta)\n",
        "        self.b3_1 = SpkBasicBlockGN(256,256, stride=1, beta=beta)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.drop    = nn.Dropout(p=0.3)\n",
        "        self.fc      = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, spk_in, readout_mode=\"mean\"):\n",
        "        Tsteps, B, _, _, _ = spk_in.shape\n",
        "\n",
        "        mem0 = self.lif0.init_leaky()\n",
        "        m10, m11 = self.b1_0.init_state()\n",
        "        m12, m13 = self.b1_1.init_state()\n",
        "        m20, m21 = self.b2_0.init_state()\n",
        "        m22, m23 = self.b2_1.init_state()\n",
        "        m30, m31 = self.b3_0.init_state()\n",
        "        m32, m33 = self.b3_1.init_state()\n",
        "\n",
        "        logits_rec = []\n",
        "        for t in range(Tsteps):\n",
        "            x = spk_in[t]\n",
        "\n",
        "            x = self.conv0(x); x = self.gn0(x)\n",
        "            x, mem0 = self.lif0(x, mem0)\n",
        "\n",
        "            x, m10, m11 = self.b1_0.forward_step(x, m10, m11)\n",
        "            x, m12, m13 = self.b1_1.forward_step(x, m12, m13)\n",
        "\n",
        "            x, m20, m21 = self.b2_0.forward_step(x, m20, m21)\n",
        "            x, m22, m23 = self.b2_1.forward_step(x, m22, m23)\n",
        "\n",
        "            x, m30, m31 = self.b3_0.forward_step(x, m30, m31)\n",
        "            x, m32, m33 = self.b3_1.forward_step(x, m32, m33)\n",
        "\n",
        "            x = self.avgpool(x).view(B, -1)\n",
        "            x = self.drop(x)\n",
        "            logits = self.fc(x)\n",
        "            logits_rec.append(logits)\n",
        "\n",
        "        logits_rec = torch.stack(logits_rec, dim=0)  # [T,B,10]\n",
        "        return logits_rec[-1] if readout_mode == \"last\" else logits_rec.mean(dim=0)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Train / Eval\n",
        "# -----------------------------\n",
        "model = SpikingResNetCIFAR_GN(beta=beta, num_classes=num_classes).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate():\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    for x_norm, y in test_loader:\n",
        "        x_norm, y = x_norm.to(device), y.to(device)\n",
        "\n",
        "        x_unit = cifar_to_unit_interval(x_norm)\n",
        "        spk_in = isi_phase_fixedK_strict_timegrid(\n",
        "            x_unit, T=T, K=K, P=P, phi0=phi0, alpha_max=alpha_max, eps=eps_q\n",
        "        )\n",
        "\n",
        "        logits = model(spk_in, readout_mode=readout_mode)\n",
        "        pred = logits.argmax(dim=1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.numel()\n",
        "    return correct / max(total, 1)\n",
        "\n",
        "best_acc = -1.0\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    running_loss, running_correct, running_total = 0.0, 0, 0\n",
        "\n",
        "    for x_norm, y in train_loader:\n",
        "        x_norm, y = x_norm.to(device), y.to(device)\n",
        "\n",
        "        x_unit = cifar_to_unit_interval(x_norm)\n",
        "        spk_in = isi_phase_fixedK_strict_timegrid(\n",
        "            x_unit, T=T, K=K, P=P, phi0=phi0, alpha_max=alpha_max, eps=eps_q\n",
        "        )\n",
        "\n",
        "        logits = model(spk_in, readout_mode=readout_mode)\n",
        "        loss = F.cross_entropy(logits, y, label_smoothing=label_smoothing)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * x_norm.size(0)\n",
        "        running_correct += (logits.argmax(dim=1) == y).sum().item()\n",
        "        running_total += x_norm.size(0)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    test_acc = evaluate()\n",
        "    if test_acc > best_acc:\n",
        "        best_acc = test_acc\n",
        "        torch.save(\n",
        "            {\n",
        "                \"epoch\": epoch,\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"test_acc\": test_acc,\n",
        "                \"config\": {\n",
        "                    \"T\": T, \"P\": P, \"phi0\": phi0,\n",
        "                    \"K\": K, \"alpha_max\": alpha_max, \"eps_q\": eps_q,\n",
        "                    \"beta\": beta,\n",
        "                    \"readout_mode\": readout_mode,\n",
        "                    \"label_smoothing\": label_smoothing,\n",
        "                    \"lr\": lr,\n",
        "                    \"encoder\": \"isi_phase_fixedK_strict_timegrid\",\n",
        "                },\n",
        "            },\n",
        "            ckpt_path,\n",
        "        )\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch:02d} | \"\n",
        "        f\"train loss {running_loss/running_total:.4f} | \"\n",
        "        f\"train acc {running_correct/running_total:.4f} | \"\n",
        "        f\"test acc {test_acc:.4f} | \"\n",
        "        f\"best {best_acc:.4f}\"\n",
        "    )\n",
        "\n",
        "print(f\"Done. Best test acc = {best_acc:.4f}. Saved to {os.path.abspath(ckpt_path)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# -----------------------------\n",
        "# Config\n",
        "# -----------------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "batch_size = 128\n",
        "epochs = 20\n",
        "lr = 1e-3\n",
        "weight_decay = 5e-4\n",
        "\n",
        "# -----------------------------\n",
        "# Data\n",
        "# -----------------------------\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                         (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                         (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "train_ds = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=True, download=True, transform=transform_train\n",
        ")\n",
        "test_ds = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=False, download=True, transform=transform_test\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size,\n",
        "                          shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=batch_size,\n",
        "                         shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# -----------------------------\n",
        "# Model\n",
        "# -----------------------------\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Convolutional feature extractor\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),        # 32x32 -> 16x16\n",
        "\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),        # 16x16 -> 8x8\n",
        "        )\n",
        "\n",
        "        # MLP classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64 * 8 * 8, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model = SimpleCNN().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                       lr=lr,\n",
        "                       weight_decay=weight_decay)\n",
        "\n",
        "# -----------------------------\n",
        "# Evaluation function\n",
        "# -----------------------------\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    loss_sum = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "\n",
        "            loss_sum += loss.item() * x.size(0)\n",
        "            pred = logits.argmax(dim=1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "    return loss_sum / total, correct / total\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Training loop\n",
        "# -----------------------------\n",
        "for epoch in range(1, epochs + 1):\n",
        "    model.train()\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    train_loss, train_acc = evaluate(model, train_loader)\n",
        "    test_loss, test_acc = evaluate(model, test_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | \"\n",
        "          f\"Train Loss {train_loss:.4f} Acc {train_acc*100:.2f}% | \"\n",
        "          f\"Test Loss {test_loss:.4f} Acc {test_acc*100:.2f}%\")\n",
        "\n",
        "print(\"Training finished.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcWumNBTogja",
        "outputId": "a26cc334-4d0a-49d6-e63c-401325e96d77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:13<00:00, 12.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | Train Loss 1.3424 Acc 51.08% | Test Loss 1.2543 Acc 54.45%\n",
            "Epoch 02 | Train Loss 1.2559 Acc 54.43% | Test Loss 1.1932 Acc 57.10%\n",
            "Epoch 03 | Train Loss 0.9955 Acc 64.32% | Test Loss 0.9434 Acc 65.46%\n",
            "Epoch 04 | Train Loss 0.9080 Acc 68.02% | Test Loss 0.8504 Acc 69.56%\n",
            "Epoch 05 | Train Loss 0.8134 Acc 72.00% | Test Loss 0.7704 Acc 73.47%\n",
            "Epoch 06 | Train Loss 0.7496 Acc 73.44% | Test Loss 0.7105 Acc 74.87%\n",
            "Epoch 07 | Train Loss 0.7348 Acc 74.09% | Test Loss 0.7015 Acc 75.64%\n",
            "Epoch 08 | Train Loss 0.6965 Acc 75.32% | Test Loss 0.6842 Acc 75.84%\n",
            "Epoch 09 | Train Loss 0.6916 Acc 75.62% | Test Loss 0.6639 Acc 76.70%\n",
            "Epoch 10 | Train Loss 0.6759 Acc 76.24% | Test Loss 0.6539 Acc 77.03%\n",
            "Epoch 11 | Train Loss 0.6245 Acc 78.29% | Test Loss 0.6306 Acc 78.11%\n",
            "Epoch 12 | Train Loss 0.6250 Acc 78.15% | Test Loss 0.6166 Acc 78.99%\n",
            "Epoch 13 | Train Loss 0.6047 Acc 78.73% | Test Loss 0.5997 Acc 79.55%\n",
            "Epoch 14 | Train Loss 0.5895 Acc 79.47% | Test Loss 0.5977 Acc 79.56%\n",
            "Epoch 15 | Train Loss 0.5918 Acc 79.40% | Test Loss 0.5913 Acc 79.22%\n",
            "Epoch 16 | Train Loss 0.5624 Acc 80.49% | Test Loss 0.5603 Acc 80.98%\n",
            "Epoch 17 | Train Loss 0.6025 Acc 79.28% | Test Loss 0.6015 Acc 78.89%\n",
            "Epoch 18 | Train Loss 0.5824 Acc 79.79% | Test Loss 0.6030 Acc 79.28%\n",
            "Epoch 19 | Train Loss 0.5425 Acc 81.09% | Test Loss 0.5561 Acc 80.79%\n",
            "Epoch 20 | Train Loss 0.5399 Acc 81.30% | Test Loss 0.5534 Acc 81.50%\n",
            "Training finished.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}